{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9d8132e10>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    normalized_x = x / 255.0\n",
    "    return normalized_x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # IDK why the method is called 2 times in the test\n",
    "    # 1 with a list, other with a nd-array\n",
    "    \n",
    "    qtd = 168\n",
    "    if type(x) == np.ndarray:\n",
    "        qtd=x.shape[0]\n",
    "    else:\n",
    "        qtd=len(x)\n",
    "    \n",
    "    # The actual function is here\n",
    "    hot_x = np.zeros((qtd, 10))\n",
    "    for i, label in enumerate(x):\n",
    "        x_label = np.zeros(10)\n",
    "        x_label[label] = 1\n",
    "        hot_x[i] = x_label\n",
    "    return hot_x\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, \n",
    "                          shape=[None, image_shape[0], image_shape[1], image_shape[2]], \n",
    "                          name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    x_shape = [x for x in filter(lambda x: x != None, x_tensor.get_shape().as_list())]\n",
    "    shape = list(conv_ksize) + [x_shape[2], conv_num_outputs]\n",
    "    \n",
    "    c_strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    p_strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    p_ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    \n",
    "    W_conv = weight_variable(shape)\n",
    "    b_conv = bias_variable([conv_num_outputs])\n",
    "    conv_op = tf.nn.conv2d(x_tensor, W_conv, strides=c_strides, padding='SAME')\n",
    "    # Add bias\n",
    "    conv_op = tf.nn.bias_add(conv_op, b_conv)\n",
    "    h_conv = tf.nn.relu(conv_op)\n",
    "    \n",
    "    return tf.nn.max_pool(h_conv, ksize=p_ksize, strides=p_strides, padding='SAME')\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # Seems this solution is not working\n",
    "    #flat_shape = np.prod(np.array(x_tensor.shape[1:]))\n",
    "    #dim = tf.reduce_prod(tf.shape(x_tensor)[1:])\n",
    "    #flat = tf.reshape(x_tensor, [-1, dim])\n",
    "    #return flat\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 32, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    #tf.nn.dropout(conv1, keep_prob)\n",
    "    conv2 = conv2d_maxpool(conv1, 32, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    \n",
    "    conv3 = conv2d_maxpool(conv2, 64, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    conv4 = conv2d_maxpool(conv3, 64, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    conv4 = tf.nn.dropout(conv4, keep_prob)\n",
    "    \n",
    "    \n",
    "    flattened = flatten(conv4)\n",
    "    \n",
    "    fc1 = fully_conn(flattened, 512)\n",
    "    tf.nn.dropout(fc1, keep_prob)\n",
    "    fc2 = fully_conn(fc1, 512)\n",
    "    tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    y = output(fc2, 10)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    return session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # c = session.run(cost, feed_dict={x: feature_batch, y: label_batch })\n",
    "    c = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0 })\n",
    "    acc = session.run(accuracy, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0 })\n",
    "    \n",
    "    # c1 = session.run(cost, feed_dict={x: valid_features, y: valid_labels })\n",
    "    c1 = session.run(cost, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0 })\n",
    "    acc1 = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0 })\n",
    "    \n",
    "    # Course Review\n",
    "    # Use the global variables valid_features and valid_labels to calculate validation accuracy.\n",
    "    print('training cost %g' % c, \" validation cost %g\" % c1)\n",
    "    print('training accuracy %g' % acc, \" validation accuracy %g\" % acc1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  training cost 2.15473  validation cost 1.97067\n",
      "training accuracy 0.3  validation accuracy 0.3078\n",
      "Epoch  2, CIFAR-10 Batch 1:  training cost 1.96114  validation cost 1.75381\n",
      "training accuracy 0.35  validation accuracy 0.3554\n",
      "Epoch  3, CIFAR-10 Batch 1:  training cost 1.84276  validation cost 1.67889\n",
      "training accuracy 0.35  validation accuracy 0.3804\n",
      "Epoch  4, CIFAR-10 Batch 1:  training cost 1.83251  validation cost 1.59128\n",
      "training accuracy 0.4  validation accuracy 0.4182\n",
      "Epoch  5, CIFAR-10 Batch 1:  training cost 1.70715  validation cost 1.5281\n",
      "training accuracy 0.4  validation accuracy 0.4394\n",
      "Epoch  6, CIFAR-10 Batch 1:  training cost 1.66083  validation cost 1.4963\n",
      "training accuracy 0.425  validation accuracy 0.45\n",
      "Epoch  7, CIFAR-10 Batch 1:  training cost 1.56378  validation cost 1.47446\n",
      "training accuracy 0.475  validation accuracy 0.459\n",
      "Epoch  8, CIFAR-10 Batch 1:  training cost 1.51334  validation cost 1.4624\n",
      "training accuracy 0.475  validation accuracy 0.4644\n",
      "Epoch  9, CIFAR-10 Batch 1:  training cost 1.42112  validation cost 1.40701\n",
      "training accuracy 0.5  validation accuracy 0.491\n",
      "Epoch 10, CIFAR-10 Batch 1:  training cost 1.37273  validation cost 1.40741\n",
      "training accuracy 0.525  validation accuracy 0.4976\n",
      "Epoch 11, CIFAR-10 Batch 1:  training cost 1.35108  validation cost 1.37456\n",
      "training accuracy 0.55  validation accuracy 0.4982\n",
      "Epoch 12, CIFAR-10 Batch 1:  training cost 1.27234  validation cost 1.37347\n",
      "training accuracy 0.575  validation accuracy 0.4994\n",
      "Epoch 13, CIFAR-10 Batch 1:  training cost 1.27218  validation cost 1.38621\n",
      "training accuracy 0.6  validation accuracy 0.4912\n",
      "Epoch 14, CIFAR-10 Batch 1:  training cost 1.18772  validation cost 1.33411\n",
      "training accuracy 0.6  validation accuracy 0.5174\n",
      "Epoch 15, CIFAR-10 Batch 1:  training cost 1.14562  validation cost 1.3271\n",
      "training accuracy 0.6  validation accuracy 0.5226\n",
      "Epoch 16, CIFAR-10 Batch 1:  training cost 1.11766  validation cost 1.31523\n",
      "training accuracy 0.625  validation accuracy 0.5232\n",
      "Epoch 17, CIFAR-10 Batch 1:  training cost 1.03881  validation cost 1.28842\n",
      "training accuracy 0.65  validation accuracy 0.5362\n",
      "Epoch 18, CIFAR-10 Batch 1:  training cost 1.05919  validation cost 1.32819\n",
      "training accuracy 0.7  validation accuracy 0.5244\n",
      "Epoch 19, CIFAR-10 Batch 1:  training cost 0.977063  validation cost 1.34256\n",
      "training accuracy 0.725  validation accuracy 0.5256\n",
      "Epoch 20, CIFAR-10 Batch 1:  training cost 0.917712  validation cost 1.28043\n",
      "training accuracy 0.7  validation accuracy 0.5376\n",
      "Epoch 21, CIFAR-10 Batch 1:  training cost 0.902923  validation cost 1.27716\n",
      "training accuracy 0.775  validation accuracy 0.5372\n",
      "Epoch 22, CIFAR-10 Batch 1:  training cost 0.8866  validation cost 1.30662\n",
      "training accuracy 0.7  validation accuracy 0.5258\n",
      "Epoch 23, CIFAR-10 Batch 1:  training cost 0.822186  validation cost 1.26475\n",
      "training accuracy 0.75  validation accuracy 0.5432\n",
      "Epoch 24, CIFAR-10 Batch 1:  training cost 0.783068  validation cost 1.26698\n",
      "training accuracy 0.75  validation accuracy 0.5482\n",
      "Epoch 25, CIFAR-10 Batch 1:  training cost 0.755304  validation cost 1.24419\n",
      "training accuracy 0.75  validation accuracy 0.5534\n",
      "Epoch 26, CIFAR-10 Batch 1:  training cost 0.753192  validation cost 1.21691\n",
      "training accuracy 0.75  validation accuracy 0.5644\n",
      "Epoch 27, CIFAR-10 Batch 1:  training cost 0.704317  validation cost 1.23062\n",
      "training accuracy 0.8  validation accuracy 0.5582\n",
      "Epoch 28, CIFAR-10 Batch 1:  training cost 0.656003  validation cost 1.20435\n",
      "training accuracy 0.825  validation accuracy 0.5668\n",
      "Epoch 29, CIFAR-10 Batch 1:  training cost 0.643404  validation cost 1.20474\n",
      "training accuracy 0.85  validation accuracy 0.5662\n",
      "Epoch 30, CIFAR-10 Batch 1:  training cost 0.586248  validation cost 1.20731\n",
      "training accuracy 0.85  validation accuracy 0.5696\n",
      "Epoch 31, CIFAR-10 Batch 1:  training cost 0.583413  validation cost 1.18917\n",
      "training accuracy 0.825  validation accuracy 0.5778\n",
      "Epoch 32, CIFAR-10 Batch 1:  training cost 0.568558  validation cost 1.18895\n",
      "training accuracy 0.85  validation accuracy 0.5772\n",
      "Epoch 33, CIFAR-10 Batch 1:  training cost 0.530956  validation cost 1.17535\n",
      "training accuracy 0.875  validation accuracy 0.5768\n",
      "Epoch 34, CIFAR-10 Batch 1:  training cost 0.464459  validation cost 1.1701\n",
      "training accuracy 0.9  validation accuracy 0.5874\n",
      "Epoch 35, CIFAR-10 Batch 1:  training cost 0.52297  validation cost 1.17795\n",
      "training accuracy 0.9  validation accuracy 0.5798\n",
      "Epoch 36, CIFAR-10 Batch 1:  training cost 0.444991  validation cost 1.16691\n",
      "training accuracy 0.875  validation accuracy 0.5866\n",
      "Epoch 37, CIFAR-10 Batch 1:  training cost 0.468476  validation cost 1.17488\n",
      "training accuracy 0.875  validation accuracy 0.5816\n",
      "Epoch 38, CIFAR-10 Batch 1:  training cost 0.429537  validation cost 1.15619\n",
      "training accuracy 0.9  validation accuracy 0.5904\n",
      "Epoch 39, CIFAR-10 Batch 1:  training cost 0.390001  validation cost 1.13056\n",
      "training accuracy 0.9  validation accuracy 0.591\n",
      "Epoch 40, CIFAR-10 Batch 1:  training cost 0.400601  validation cost 1.16573\n",
      "training accuracy 0.925  validation accuracy 0.5856\n",
      "Epoch 41, CIFAR-10 Batch 1:  training cost 0.351505  validation cost 1.14276\n",
      "training accuracy 0.925  validation accuracy 0.5924\n",
      "Epoch 42, CIFAR-10 Batch 1:  training cost 0.300159  validation cost 1.15685\n",
      "training accuracy 0.9  validation accuracy 0.5904\n",
      "Epoch 43, CIFAR-10 Batch 1:  training cost 0.37152  validation cost 1.17534\n",
      "training accuracy 0.95  validation accuracy 0.5934\n",
      "Epoch 44, CIFAR-10 Batch 1:  training cost 0.300367  validation cost 1.15362\n",
      "training accuracy 0.925  validation accuracy 0.596\n",
      "Epoch 45, CIFAR-10 Batch 1:  training cost 0.269368  validation cost 1.1407\n",
      "training accuracy 0.925  validation accuracy 0.5988\n",
      "Epoch 46, CIFAR-10 Batch 1:  training cost 0.272747  validation cost 1.13562\n",
      "training accuracy 0.95  validation accuracy 0.6032\n",
      "Epoch 47, CIFAR-10 Batch 1:  training cost 0.235712  validation cost 1.14478\n",
      "training accuracy 1  validation accuracy 0.599\n",
      "Epoch 48, CIFAR-10 Batch 1:  training cost 0.25277  validation cost 1.13146\n",
      "training accuracy 0.95  validation accuracy 0.5952\n",
      "Epoch 49, CIFAR-10 Batch 1:  training cost 0.21556  validation cost 1.16491\n",
      "training accuracy 1  validation accuracy 0.5934\n",
      "Epoch 50, CIFAR-10 Batch 1:  training cost 0.208466  validation cost 1.1441\n",
      "training accuracy 0.975  validation accuracy 0.6014\n",
      "Epoch 51, CIFAR-10 Batch 1:  training cost 0.187437  validation cost 1.14603\n",
      "training accuracy 1  validation accuracy 0.6076\n",
      "Epoch 52, CIFAR-10 Batch 1:  training cost 0.16289  validation cost 1.14752\n",
      "training accuracy 1  validation accuracy 0.6104\n",
      "Epoch 53, CIFAR-10 Batch 1:  training cost 0.174559  validation cost 1.16911\n",
      "training accuracy 1  validation accuracy 0.5996\n",
      "Epoch 54, CIFAR-10 Batch 1:  training cost 0.160188  validation cost 1.16825\n",
      "training accuracy 1  validation accuracy 0.5974\n",
      "Epoch 55, CIFAR-10 Batch 1:  training cost 0.137328  validation cost 1.20202\n",
      "training accuracy 1  validation accuracy 0.595\n",
      "Epoch 56, CIFAR-10 Batch 1:  training cost 0.122474  validation cost 1.17313\n",
      "training accuracy 1  validation accuracy 0.596\n",
      "Epoch 57, CIFAR-10 Batch 1:  training cost 0.129095  validation cost 1.1474\n",
      "training accuracy 1  validation accuracy 0.6118\n",
      "Epoch 58, CIFAR-10 Batch 1:  training cost 0.13652  validation cost 1.16167\n",
      "training accuracy 1  validation accuracy 0.6066\n",
      "Epoch 59, CIFAR-10 Batch 1:  training cost 0.145681  validation cost 1.1839\n",
      "training accuracy 0.975  validation accuracy 0.5954\n",
      "Epoch 60, CIFAR-10 Batch 1:  training cost 0.0889693  validation cost 1.20248\n",
      "training accuracy 1  validation accuracy 0.6074\n",
      "Epoch 61, CIFAR-10 Batch 1:  training cost 0.117658  validation cost 1.19689\n",
      "training accuracy 1  validation accuracy 0.5996\n",
      "Epoch 62, CIFAR-10 Batch 1:  training cost 0.0940595  validation cost 1.16748\n",
      "training accuracy 1  validation accuracy 0.6048\n",
      "Epoch 63, CIFAR-10 Batch 1:  training cost 0.0890585  validation cost 1.18176\n",
      "training accuracy 1  validation accuracy 0.6028\n",
      "Epoch 64, CIFAR-10 Batch 1:  training cost 0.0874331  validation cost 1.20603\n",
      "training accuracy 1  validation accuracy 0.6028\n",
      "Epoch 65, CIFAR-10 Batch 1:  training cost 0.0842806  validation cost 1.23554\n",
      "training accuracy 1  validation accuracy 0.5926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, CIFAR-10 Batch 1:  training cost 0.0946017  validation cost 1.19495\n",
      "training accuracy 1  validation accuracy 0.6024\n",
      "Epoch 67, CIFAR-10 Batch 1:  training cost 0.0652385  validation cost 1.1967\n",
      "training accuracy 1  validation accuracy 0.6026\n",
      "Epoch 68, CIFAR-10 Batch 1:  training cost 0.0765104  validation cost 1.23355\n",
      "training accuracy 1  validation accuracy 0.5844\n",
      "Epoch 69, CIFAR-10 Batch 1:  training cost 0.0794299  validation cost 1.20189\n",
      "training accuracy 1  validation accuracy 0.5938\n",
      "Epoch 70, CIFAR-10 Batch 1:  training cost 0.0735249  validation cost 1.24586\n",
      "training accuracy 1  validation accuracy 0.59\n",
      "Epoch 71, CIFAR-10 Batch 1:  training cost 0.0665593  validation cost 1.20545\n",
      "training accuracy 1  validation accuracy 0.6028\n",
      "Epoch 72, CIFAR-10 Batch 1:  training cost 0.0667047  validation cost 1.20472\n",
      "training accuracy 1  validation accuracy 0.5968\n",
      "Epoch 73, CIFAR-10 Batch 1:  training cost 0.0713633  validation cost 1.20545\n",
      "training accuracy 1  validation accuracy 0.6016\n",
      "Epoch 74, CIFAR-10 Batch 1:  training cost 0.0806724  validation cost 1.19765\n",
      "training accuracy 1  validation accuracy 0.6062\n",
      "Epoch 75, CIFAR-10 Batch 1:  training cost 0.060079  validation cost 1.22676\n",
      "training accuracy 1  validation accuracy 0.596\n",
      "Epoch 76, CIFAR-10 Batch 1:  training cost 0.0486332  validation cost 1.22331\n",
      "training accuracy 1  validation accuracy 0.6032\n",
      "Epoch 77, CIFAR-10 Batch 1:  training cost 0.0487052  validation cost 1.21993\n",
      "training accuracy 1  validation accuracy 0.606\n",
      "Epoch 78, CIFAR-10 Batch 1:  training cost 0.0495364  validation cost 1.25282\n",
      "training accuracy 1  validation accuracy 0.5956\n",
      "Epoch 79, CIFAR-10 Batch 1:  training cost 0.044463  validation cost 1.2113\n",
      "training accuracy 1  validation accuracy 0.6014\n",
      "Epoch 80, CIFAR-10 Batch 1:  training cost 0.0694782  validation cost 1.23603\n",
      "training accuracy 1  validation accuracy 0.5912\n",
      "Epoch 81, CIFAR-10 Batch 1:  training cost 0.0612927  validation cost 1.25503\n",
      "training accuracy 1  validation accuracy 0.589\n",
      "Epoch 82, CIFAR-10 Batch 1:  training cost 0.0668467  validation cost 1.25799\n",
      "training accuracy 1  validation accuracy 0.5926\n",
      "Epoch 83, CIFAR-10 Batch 1:  training cost 0.055541  validation cost 1.22073\n",
      "training accuracy 1  validation accuracy 0.6032\n",
      "Epoch 84, CIFAR-10 Batch 1:  training cost 0.0536024  validation cost 1.27483\n",
      "training accuracy 1  validation accuracy 0.5862\n",
      "Epoch 85, CIFAR-10 Batch 1:  training cost 0.0596379  validation cost 1.30405\n",
      "training accuracy 1  validation accuracy 0.5796\n",
      "Epoch 86, CIFAR-10 Batch 1:  training cost 0.0427119  validation cost 1.28247\n",
      "training accuracy 1  validation accuracy 0.591\n",
      "Epoch 87, CIFAR-10 Batch 1:  training cost 0.0541105  validation cost 1.2833\n",
      "training accuracy 1  validation accuracy 0.5896\n",
      "Epoch 88, CIFAR-10 Batch 1:  training cost 0.0724975  validation cost 1.2913\n",
      "training accuracy 0.975  validation accuracy 0.5842\n",
      "Epoch 89, CIFAR-10 Batch 1:  training cost 0.0330826  validation cost 1.29646\n",
      "training accuracy 1  validation accuracy 0.5906\n",
      "Epoch 90, CIFAR-10 Batch 1:  training cost 0.0393897  validation cost 1.33177\n",
      "training accuracy 1  validation accuracy 0.5816\n",
      "Epoch 91, CIFAR-10 Batch 1:  training cost 0.0257189  validation cost 1.31385\n",
      "training accuracy 1  validation accuracy 0.5944\n",
      "Epoch 92, CIFAR-10 Batch 1:  training cost 0.0307208  validation cost 1.29294\n",
      "training accuracy 1  validation accuracy 0.591\n",
      "Epoch 93, CIFAR-10 Batch 1:  training cost 0.030466  validation cost 1.30301\n",
      "training accuracy 1  validation accuracy 0.5922\n",
      "Epoch 94, CIFAR-10 Batch 1:  training cost 0.0268539  validation cost 1.31058\n",
      "training accuracy 1  validation accuracy 0.592\n",
      "Epoch 95, CIFAR-10 Batch 1:  training cost 0.031837  validation cost 1.26337\n",
      "training accuracy 1  validation accuracy 0.6018\n",
      "Epoch 96, CIFAR-10 Batch 1:  training cost 0.0251486  validation cost 1.296\n",
      "training accuracy 1  validation accuracy 0.5924\n",
      "Epoch 97, CIFAR-10 Batch 1:  training cost 0.0382178  validation cost 1.35097\n",
      "training accuracy 1  validation accuracy 0.5916\n",
      "Epoch 98, CIFAR-10 Batch 1:  training cost 0.0282093  validation cost 1.37438\n",
      "training accuracy 1  validation accuracy 0.5766\n",
      "Epoch 99, CIFAR-10 Batch 1:  training cost 0.0232458  validation cost 1.31214\n",
      "training accuracy 1  validation accuracy 0.5998\n",
      "Epoch 100, CIFAR-10 Batch 1:  training cost 0.0198303  validation cost 1.40996\n",
      "training accuracy 1  validation accuracy 0.5924\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  training cost 2.17285  validation cost 2.0438\n",
      "training accuracy 0.2  validation accuracy 0.2568\n",
      "Epoch  1, CIFAR-10 Batch 2:  training cost 1.83486  validation cost 1.81464\n",
      "training accuracy 0.35  validation accuracy 0.3342\n",
      "Epoch  1, CIFAR-10 Batch 3:  training cost 1.54843  validation cost 1.6845\n",
      "training accuracy 0.425  validation accuracy 0.3586\n",
      "Epoch  1, CIFAR-10 Batch 4:  training cost 1.66978  validation cost 1.60356\n",
      "training accuracy 0.3  validation accuracy 0.4022\n",
      "Epoch  1, CIFAR-10 Batch 5:  training cost 1.62784  validation cost 1.52953\n",
      "training accuracy 0.425  validation accuracy 0.427\n",
      "Epoch  2, CIFAR-10 Batch 1:  training cost 1.64157  validation cost 1.46848\n",
      "training accuracy 0.475  validation accuracy 0.456\n",
      "Epoch  2, CIFAR-10 Batch 2:  training cost 1.49612  validation cost 1.45788\n",
      "training accuracy 0.5  validation accuracy 0.4552\n",
      "Epoch  2, CIFAR-10 Batch 3:  training cost 1.28945  validation cost 1.45183\n",
      "training accuracy 0.475  validation accuracy 0.461\n",
      "Epoch  2, CIFAR-10 Batch 4:  training cost 1.40327  validation cost 1.42882\n",
      "training accuracy 0.45  validation accuracy 0.4704\n",
      "Epoch  2, CIFAR-10 Batch 5:  training cost 1.41867  validation cost 1.38161\n",
      "training accuracy 0.5  validation accuracy 0.496\n",
      "Epoch  3, CIFAR-10 Batch 1:  training cost 1.40899  validation cost 1.37935\n",
      "training accuracy 0.525  validation accuracy 0.4982\n",
      "Epoch  3, CIFAR-10 Batch 2:  training cost 1.2954  validation cost 1.36822\n",
      "training accuracy 0.525  validation accuracy 0.4964\n",
      "Epoch  3, CIFAR-10 Batch 3:  training cost 1.09959  validation cost 1.31283\n",
      "training accuracy 0.525  validation accuracy 0.524\n",
      "Epoch  3, CIFAR-10 Batch 4:  training cost 1.23467  validation cost 1.34657\n",
      "training accuracy 0.45  validation accuracy 0.4982\n",
      "Epoch  3, CIFAR-10 Batch 5:  training cost 1.33072  validation cost 1.29252\n",
      "training accuracy 0.5  validation accuracy 0.5308\n",
      "Epoch  4, CIFAR-10 Batch 1:  training cost 1.26546  validation cost 1.24958\n",
      "training accuracy 0.6  validation accuracy 0.5488\n",
      "Epoch  4, CIFAR-10 Batch 2:  training cost 1.21096  validation cost 1.29383\n",
      "training accuracy 0.525  validation accuracy 0.522\n",
      "Epoch  4, CIFAR-10 Batch 3:  training cost 1.02346  validation cost 1.23733\n",
      "training accuracy 0.6  validation accuracy 0.5512\n",
      "Epoch  4, CIFAR-10 Batch 4:  training cost 1.09004  validation cost 1.26759\n",
      "training accuracy 0.6  validation accuracy 0.538\n",
      "Epoch  4, CIFAR-10 Batch 5:  training cost 1.24726  validation cost 1.19846\n",
      "training accuracy 0.5  validation accuracy 0.5648\n",
      "Epoch  5, CIFAR-10 Batch 1:  training cost 1.17809  validation cost 1.17904\n",
      "training accuracy 0.625  validation accuracy 0.572\n",
      "Epoch  5, CIFAR-10 Batch 2:  training cost 1.18307  validation cost 1.19018\n",
      "training accuracy 0.45  validation accuracy 0.5666\n",
      "Epoch  5, CIFAR-10 Batch 3:  training cost 0.983437  validation cost 1.17766\n",
      "training accuracy 0.675  validation accuracy 0.5714\n",
      "Epoch  5, CIFAR-10 Batch 4:  training cost 1.01057  validation cost 1.18862\n",
      "training accuracy 0.6  validation accuracy 0.5628\n",
      "Epoch  5, CIFAR-10 Batch 5:  training cost 1.13063  validation cost 1.15364\n",
      "training accuracy 0.575  validation accuracy 0.5848\n",
      "Epoch  6, CIFAR-10 Batch 1:  training cost 1.06783  validation cost 1.1683\n",
      "training accuracy 0.65  validation accuracy 0.5774\n",
      "Epoch  6, CIFAR-10 Batch 2:  training cost 1.14444  validation cost 1.10553\n",
      "training accuracy 0.55  validation accuracy 0.611\n",
      "Epoch  6, CIFAR-10 Batch 3:  training cost 0.896785  validation cost 1.1338\n",
      "training accuracy 0.625  validation accuracy 0.5908\n",
      "Epoch  6, CIFAR-10 Batch 4:  training cost 0.881614  validation cost 1.13172\n",
      "training accuracy 0.675  validation accuracy 0.5922\n",
      "Epoch  6, CIFAR-10 Batch 5:  training cost 1.02946  validation cost 1.07721\n",
      "training accuracy 0.65  validation accuracy 0.6122\n",
      "Epoch  7, CIFAR-10 Batch 1:  training cost 0.957912  validation cost 1.07783\n",
      "training accuracy 0.75  validation accuracy 0.6082\n",
      "Epoch  7, CIFAR-10 Batch 2:  training cost 1.08063  validation cost 1.06815\n",
      "training accuracy 0.6  validation accuracy 0.6182\n",
      "Epoch  7, CIFAR-10 Batch 3:  training cost 0.906078  validation cost 1.09706\n",
      "training accuracy 0.7  validation accuracy 0.6094\n",
      "Epoch  7, CIFAR-10 Batch 4:  training cost 0.901295  validation cost 1.10988\n",
      "training accuracy 0.7  validation accuracy 0.596\n",
      "Epoch  7, CIFAR-10 Batch 5:  training cost 0.992819  validation cost 1.07734\n",
      "training accuracy 0.6  validation accuracy 0.6068\n",
      "Epoch  8, CIFAR-10 Batch 1:  training cost 0.892285  validation cost 1.03653\n",
      "training accuracy 0.7  validation accuracy 0.6274\n",
      "Epoch  8, CIFAR-10 Batch 2:  training cost 1.01624  validation cost 1.0282\n",
      "training accuracy 0.65  validation accuracy 0.6238\n",
      "Epoch  8, CIFAR-10 Batch 3:  training cost 0.763571  validation cost 1.0123\n",
      "training accuracy 0.625  validation accuracy 0.6396\n",
      "Epoch  8, CIFAR-10 Batch 4:  training cost 0.877437  validation cost 1.00152\n",
      "training accuracy 0.625  validation accuracy 0.64\n",
      "Epoch  8, CIFAR-10 Batch 5:  training cost 0.878198  validation cost 1.02736\n",
      "training accuracy 0.65  validation accuracy 0.6326\n",
      "Epoch  9, CIFAR-10 Batch 1:  training cost 0.86051  validation cost 1.0115\n",
      "training accuracy 0.75  validation accuracy 0.6348\n",
      "Epoch  9, CIFAR-10 Batch 2:  training cost 1.02542  validation cost 0.999128\n",
      "training accuracy 0.65  validation accuracy 0.6418\n",
      "Epoch  9, CIFAR-10 Batch 3:  training cost 0.742909  validation cost 1.00574\n",
      "training accuracy 0.675  validation accuracy 0.6438\n",
      "Epoch  9, CIFAR-10 Batch 4:  training cost 0.759513  validation cost 1.03161\n",
      "training accuracy 0.725  validation accuracy 0.624\n",
      "Epoch  9, CIFAR-10 Batch 5:  training cost 0.832944  validation cost 0.999568\n",
      "training accuracy 0.675  validation accuracy 0.642\n",
      "Epoch 10, CIFAR-10 Batch 1:  training cost 0.865609  validation cost 1.04574\n",
      "training accuracy 0.725  validation accuracy 0.623\n",
      "Epoch 10, CIFAR-10 Batch 2:  training cost 0.941664  validation cost 0.967596\n",
      "training accuracy 0.625  validation accuracy 0.6554\n",
      "Epoch 10, CIFAR-10 Batch 3:  training cost 0.61739  validation cost 0.957498\n",
      "training accuracy 0.75  validation accuracy 0.6636\n",
      "Epoch 10, CIFAR-10 Batch 4:  training cost 0.777179  validation cost 1.02891\n",
      "training accuracy 0.775  validation accuracy 0.6234\n",
      "Epoch 10, CIFAR-10 Batch 5:  training cost 0.836171  validation cost 1.00356\n",
      "training accuracy 0.725  validation accuracy 0.6412\n",
      "Epoch 11, CIFAR-10 Batch 1:  training cost 0.785444  validation cost 0.957207\n",
      "training accuracy 0.75  validation accuracy 0.6654\n",
      "Epoch 11, CIFAR-10 Batch 2:  training cost 0.879334  validation cost 0.95753\n",
      "training accuracy 0.7  validation accuracy 0.6576\n",
      "Epoch 11, CIFAR-10 Batch 3:  training cost 0.652877  validation cost 0.933169\n",
      "training accuracy 0.75  validation accuracy 0.6694\n",
      "Epoch 11, CIFAR-10 Batch 4:  training cost 0.66107  validation cost 0.943429\n",
      "training accuracy 0.75  validation accuracy 0.6668\n",
      "Epoch 11, CIFAR-10 Batch 5:  training cost 0.703482  validation cost 0.933245\n",
      "training accuracy 0.725  validation accuracy 0.6628\n",
      "Epoch 12, CIFAR-10 Batch 1:  training cost 0.706007  validation cost 0.939944\n",
      "training accuracy 0.8  validation accuracy 0.6634\n",
      "Epoch 12, CIFAR-10 Batch 2:  training cost 0.857617  validation cost 0.948161\n",
      "training accuracy 0.675  validation accuracy 0.661\n",
      "Epoch 12, CIFAR-10 Batch 3:  training cost 0.529793  validation cost 0.906044\n",
      "training accuracy 0.775  validation accuracy 0.675\n",
      "Epoch 12, CIFAR-10 Batch 4:  training cost 0.622276  validation cost 0.936287\n",
      "training accuracy 0.8  validation accuracy 0.6684\n",
      "Epoch 12, CIFAR-10 Batch 5:  training cost 0.695718  validation cost 0.916375\n",
      "training accuracy 0.825  validation accuracy 0.6694\n",
      "Epoch 13, CIFAR-10 Batch 1:  training cost 0.697294  validation cost 0.890958\n",
      "training accuracy 0.75  validation accuracy 0.6804\n",
      "Epoch 13, CIFAR-10 Batch 2:  training cost 0.807411  validation cost 0.915362\n",
      "training accuracy 0.725  validation accuracy 0.6714\n",
      "Epoch 13, CIFAR-10 Batch 3:  training cost 0.550816  validation cost 0.893913\n",
      "training accuracy 0.775  validation accuracy 0.6794\n",
      "Epoch 13, CIFAR-10 Batch 4:  training cost 0.639335  validation cost 0.903822\n",
      "training accuracy 0.775  validation accuracy 0.6822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, CIFAR-10 Batch 5:  training cost 0.640926  validation cost 0.887723\n",
      "training accuracy 0.775  validation accuracy 0.6852\n",
      "Epoch 14, CIFAR-10 Batch 1:  training cost 0.699516  validation cost 0.900889\n",
      "training accuracy 0.8  validation accuracy 0.675\n",
      "Epoch 14, CIFAR-10 Batch 2:  training cost 0.777793  validation cost 0.877599\n",
      "training accuracy 0.75  validation accuracy 0.689\n",
      "Epoch 14, CIFAR-10 Batch 3:  training cost 0.521912  validation cost 0.891805\n",
      "training accuracy 0.875  validation accuracy 0.6852\n",
      "Epoch 14, CIFAR-10 Batch 4:  training cost 0.563907  validation cost 0.888115\n",
      "training accuracy 0.775  validation accuracy 0.6842\n",
      "Epoch 14, CIFAR-10 Batch 5:  training cost 0.612984  validation cost 0.881809\n",
      "training accuracy 0.875  validation accuracy 0.6838\n",
      "Epoch 15, CIFAR-10 Batch 1:  training cost 0.695346  validation cost 0.89231\n",
      "training accuracy 0.8  validation accuracy 0.6846\n",
      "Epoch 15, CIFAR-10 Batch 2:  training cost 0.76667  validation cost 0.944138\n",
      "training accuracy 0.75  validation accuracy 0.659\n",
      "Epoch 15, CIFAR-10 Batch 3:  training cost 0.417298  validation cost 0.853248\n",
      "training accuracy 0.95  validation accuracy 0.6998\n",
      "Epoch 15, CIFAR-10 Batch 4:  training cost 0.587947  validation cost 0.886587\n",
      "training accuracy 0.775  validation accuracy 0.6856\n",
      "Epoch 15, CIFAR-10 Batch 5:  training cost 0.605283  validation cost 0.864695\n",
      "training accuracy 0.875  validation accuracy 0.6928\n",
      "Epoch 16, CIFAR-10 Batch 1:  training cost 0.653142  validation cost 0.872819\n",
      "training accuracy 0.725  validation accuracy 0.6914\n",
      "Epoch 16, CIFAR-10 Batch 2:  training cost 0.69296  validation cost 0.842495\n",
      "training accuracy 0.8  validation accuracy 0.701\n",
      "Epoch 16, CIFAR-10 Batch 3:  training cost 0.424711  validation cost 0.834238\n",
      "training accuracy 0.9  validation accuracy 0.6998\n",
      "Epoch 16, CIFAR-10 Batch 4:  training cost 0.529935  validation cost 0.838722\n",
      "training accuracy 0.875  validation accuracy 0.6994\n",
      "Epoch 16, CIFAR-10 Batch 5:  training cost 0.538637  validation cost 0.838767\n",
      "training accuracy 0.8  validation accuracy 0.7032\n",
      "Epoch 17, CIFAR-10 Batch 1:  training cost 0.587692  validation cost 0.856008\n",
      "training accuracy 0.8  validation accuracy 0.6938\n",
      "Epoch 17, CIFAR-10 Batch 2:  training cost 0.59289  validation cost 0.837241\n",
      "training accuracy 0.85  validation accuracy 0.7008\n",
      "Epoch 17, CIFAR-10 Batch 3:  training cost 0.40734  validation cost 0.838368\n",
      "training accuracy 0.85  validation accuracy 0.7014\n",
      "Epoch 17, CIFAR-10 Batch 4:  training cost 0.483278  validation cost 0.83133\n",
      "training accuracy 0.85  validation accuracy 0.71\n",
      "Epoch 17, CIFAR-10 Batch 5:  training cost 0.57346  validation cost 0.901805\n",
      "training accuracy 0.8  validation accuracy 0.6734\n",
      "Epoch 18, CIFAR-10 Batch 1:  training cost 0.652444  validation cost 0.829176\n",
      "training accuracy 0.725  validation accuracy 0.7036\n",
      "Epoch 18, CIFAR-10 Batch 2:  training cost 0.543612  validation cost 0.812856\n",
      "training accuracy 0.825  validation accuracy 0.7124\n",
      "Epoch 18, CIFAR-10 Batch 3:  training cost 0.405223  validation cost 0.82859\n",
      "training accuracy 0.925  validation accuracy 0.701\n",
      "Epoch 18, CIFAR-10 Batch 4:  training cost 0.481804  validation cost 0.827738\n",
      "training accuracy 0.825  validation accuracy 0.7132\n",
      "Epoch 18, CIFAR-10 Batch 5:  training cost 0.514568  validation cost 0.804309\n",
      "training accuracy 0.875  validation accuracy 0.7148\n",
      "Epoch 19, CIFAR-10 Batch 1:  training cost 0.599702  validation cost 0.860371\n",
      "training accuracy 0.8  validation accuracy 0.6936\n",
      "Epoch 19, CIFAR-10 Batch 2:  training cost 0.5317  validation cost 0.803509\n",
      "training accuracy 0.9  validation accuracy 0.7158\n",
      "Epoch 19, CIFAR-10 Batch 3:  training cost 0.349451  validation cost 0.789804\n",
      "training accuracy 0.875  validation accuracy 0.7168\n",
      "Epoch 19, CIFAR-10 Batch 4:  training cost 0.452776  validation cost 0.832935\n",
      "training accuracy 0.85  validation accuracy 0.7106\n",
      "Epoch 19, CIFAR-10 Batch 5:  training cost 0.513955  validation cost 0.838224\n",
      "training accuracy 0.85  validation accuracy 0.6988\n",
      "Epoch 20, CIFAR-10 Batch 1:  training cost 0.566219  validation cost 0.853415\n",
      "training accuracy 0.775  validation accuracy 0.6918\n",
      "Epoch 20, CIFAR-10 Batch 2:  training cost 0.496262  validation cost 0.808525\n",
      "training accuracy 0.875  validation accuracy 0.7166\n",
      "Epoch 20, CIFAR-10 Batch 3:  training cost 0.322418  validation cost 0.789527\n",
      "training accuracy 0.925  validation accuracy 0.722\n",
      "Epoch 20, CIFAR-10 Batch 4:  training cost 0.414824  validation cost 0.796262\n",
      "training accuracy 0.9  validation accuracy 0.7164\n",
      "Epoch 20, CIFAR-10 Batch 5:  training cost 0.516256  validation cost 0.824273\n",
      "training accuracy 0.775  validation accuracy 0.7024\n",
      "Epoch 21, CIFAR-10 Batch 1:  training cost 0.580057  validation cost 0.774772\n",
      "training accuracy 0.75  validation accuracy 0.7252\n",
      "Epoch 21, CIFAR-10 Batch 2:  training cost 0.496111  validation cost 0.793469\n",
      "training accuracy 0.9  validation accuracy 0.722\n",
      "Epoch 21, CIFAR-10 Batch 3:  training cost 0.327288  validation cost 0.771347\n",
      "training accuracy 0.925  validation accuracy 0.729\n",
      "Epoch 21, CIFAR-10 Batch 4:  training cost 0.393065  validation cost 0.768684\n",
      "training accuracy 0.95  validation accuracy 0.73\n",
      "Epoch 21, CIFAR-10 Batch 5:  training cost 0.43068  validation cost 0.784649\n",
      "training accuracy 0.925  validation accuracy 0.7222\n",
      "Epoch 22, CIFAR-10 Batch 1:  training cost 0.573791  validation cost 0.831219\n",
      "training accuracy 0.8  validation accuracy 0.7098\n",
      "Epoch 22, CIFAR-10 Batch 2:  training cost 0.448546  validation cost 0.786745\n",
      "training accuracy 0.875  validation accuracy 0.7256\n",
      "Epoch 22, CIFAR-10 Batch 3:  training cost 0.316055  validation cost 0.755646\n",
      "training accuracy 0.925  validation accuracy 0.7348\n",
      "Epoch 22, CIFAR-10 Batch 4:  training cost 0.446998  validation cost 0.77861\n",
      "training accuracy 0.875  validation accuracy 0.729\n",
      "Epoch 22, CIFAR-10 Batch 5:  training cost 0.498682  validation cost 0.77281\n",
      "training accuracy 0.85  validation accuracy 0.7288\n",
      "Epoch 23, CIFAR-10 Batch 1:  training cost 0.535257  validation cost 0.764271\n",
      "training accuracy 0.8  validation accuracy 0.7304\n",
      "Epoch 23, CIFAR-10 Batch 2:  training cost 0.400491  validation cost 0.762809\n",
      "training accuracy 0.925  validation accuracy 0.7298\n",
      "Epoch 23, CIFAR-10 Batch 3:  training cost 0.364557  validation cost 0.809748\n",
      "training accuracy 0.9  validation accuracy 0.7082\n",
      "Epoch 23, CIFAR-10 Batch 4:  training cost 0.459189  validation cost 0.831707\n",
      "training accuracy 0.85  validation accuracy 0.702\n",
      "Epoch 23, CIFAR-10 Batch 5:  training cost 0.449833  validation cost 0.780888\n",
      "training accuracy 0.875  validation accuracy 0.7202\n",
      "Epoch 24, CIFAR-10 Batch 1:  training cost 0.516513  validation cost 0.801991\n",
      "training accuracy 0.85  validation accuracy 0.7152\n",
      "Epoch 24, CIFAR-10 Batch 2:  training cost 0.522027  validation cost 0.758055\n",
      "training accuracy 0.825  validation accuracy 0.7336\n",
      "Epoch 24, CIFAR-10 Batch 3:  training cost 0.286594  validation cost 0.762166\n",
      "training accuracy 0.975  validation accuracy 0.7324\n",
      "Epoch 24, CIFAR-10 Batch 4:  training cost 0.421418  validation cost 0.781035\n",
      "training accuracy 0.875  validation accuracy 0.73\n",
      "Epoch 24, CIFAR-10 Batch 5:  training cost 0.400363  validation cost 0.777446\n",
      "training accuracy 0.875  validation accuracy 0.7222\n",
      "Epoch 25, CIFAR-10 Batch 1:  training cost 0.532027  validation cost 0.767195\n",
      "training accuracy 0.8  validation accuracy 0.7248\n",
      "Epoch 25, CIFAR-10 Batch 2:  training cost 0.4159  validation cost 0.789664\n",
      "training accuracy 0.925  validation accuracy 0.7264\n",
      "Epoch 25, CIFAR-10 Batch 3:  training cost 0.272914  validation cost 0.747953\n",
      "training accuracy 0.975  validation accuracy 0.7398\n",
      "Epoch 25, CIFAR-10 Batch 4:  training cost 0.401824  validation cost 0.774025\n",
      "training accuracy 0.9  validation accuracy 0.7314\n",
      "Epoch 25, CIFAR-10 Batch 5:  training cost 0.411822  validation cost 0.778249\n",
      "training accuracy 0.925  validation accuracy 0.7248\n",
      "Epoch 26, CIFAR-10 Batch 1:  training cost 0.463807  validation cost 0.76343\n",
      "training accuracy 0.85  validation accuracy 0.7312\n",
      "Epoch 26, CIFAR-10 Batch 2:  training cost 0.394828  validation cost 0.747401\n",
      "training accuracy 0.925  validation accuracy 0.7348\n",
      "Epoch 26, CIFAR-10 Batch 3:  training cost 0.290207  validation cost 0.743912\n",
      "training accuracy 0.975  validation accuracy 0.7402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, CIFAR-10 Batch 4:  training cost 0.364023  validation cost 0.75816\n",
      "training accuracy 0.875  validation accuracy 0.7354\n",
      "Epoch 26, CIFAR-10 Batch 5:  training cost 0.374005  validation cost 0.761668\n",
      "training accuracy 0.875  validation accuracy 0.726\n",
      "Epoch 27, CIFAR-10 Batch 1:  training cost 0.5036  validation cost 0.779195\n",
      "training accuracy 0.8  validation accuracy 0.724\n",
      "Epoch 27, CIFAR-10 Batch 2:  training cost 0.390063  validation cost 0.728315\n",
      "training accuracy 0.875  validation accuracy 0.7424\n",
      "Epoch 27, CIFAR-10 Batch 3:  training cost 0.296236  validation cost 0.764301\n",
      "training accuracy 0.95  validation accuracy 0.7284\n",
      "Epoch 27, CIFAR-10 Batch 4:  training cost 0.347504  validation cost 0.775793\n",
      "training accuracy 0.9  validation accuracy 0.7306\n",
      "Epoch 27, CIFAR-10 Batch 5:  training cost 0.429892  validation cost 0.816122\n",
      "training accuracy 0.9  validation accuracy 0.7118\n",
      "Epoch 28, CIFAR-10 Batch 1:  training cost 0.454697  validation cost 0.746275\n",
      "training accuracy 0.85  validation accuracy 0.7386\n",
      "Epoch 28, CIFAR-10 Batch 2:  training cost 0.383084  validation cost 0.745304\n",
      "training accuracy 0.875  validation accuracy 0.7412\n",
      "Epoch 28, CIFAR-10 Batch 3:  training cost 0.270468  validation cost 0.743139\n",
      "training accuracy 0.925  validation accuracy 0.7414\n",
      "Epoch 28, CIFAR-10 Batch 4:  training cost 0.371049  validation cost 0.797569\n",
      "training accuracy 0.95  validation accuracy 0.7232\n",
      "Epoch 28, CIFAR-10 Batch 5:  training cost 0.410633  validation cost 0.758275\n",
      "training accuracy 0.925  validation accuracy 0.732\n",
      "Epoch 29, CIFAR-10 Batch 1:  training cost 0.444642  validation cost 0.727221\n",
      "training accuracy 0.8  validation accuracy 0.7426\n",
      "Epoch 29, CIFAR-10 Batch 2:  training cost 0.399532  validation cost 0.727688\n",
      "training accuracy 0.9  validation accuracy 0.748\n",
      "Epoch 29, CIFAR-10 Batch 3:  training cost 0.268488  validation cost 0.766209\n",
      "training accuracy 0.95  validation accuracy 0.736\n",
      "Epoch 29, CIFAR-10 Batch 4:  training cost 0.301599  validation cost 0.727791\n",
      "training accuracy 0.95  validation accuracy 0.746\n",
      "Epoch 29, CIFAR-10 Batch 5:  training cost 0.34083  validation cost 0.733479\n",
      "training accuracy 0.925  validation accuracy 0.7414\n",
      "Epoch 30, CIFAR-10 Batch 1:  training cost 0.398961  validation cost 0.74951\n",
      "training accuracy 0.875  validation accuracy 0.7356\n",
      "Epoch 30, CIFAR-10 Batch 2:  training cost 0.354583  validation cost 0.732037\n",
      "training accuracy 0.925  validation accuracy 0.7432\n",
      "Epoch 30, CIFAR-10 Batch 3:  training cost 0.246527  validation cost 0.727223\n",
      "training accuracy 0.95  validation accuracy 0.747\n",
      "Epoch 30, CIFAR-10 Batch 4:  training cost 0.324276  validation cost 0.745727\n",
      "training accuracy 0.875  validation accuracy 0.737\n",
      "Epoch 30, CIFAR-10 Batch 5:  training cost 0.337103  validation cost 0.739038\n",
      "training accuracy 0.925  validation accuracy 0.739\n",
      "Epoch 31, CIFAR-10 Batch 1:  training cost 0.447242  validation cost 0.747106\n",
      "training accuracy 0.85  validation accuracy 0.7414\n",
      "Epoch 31, CIFAR-10 Batch 2:  training cost 0.301631  validation cost 0.719167\n",
      "training accuracy 0.95  validation accuracy 0.7484\n",
      "Epoch 31, CIFAR-10 Batch 3:  training cost 0.232163  validation cost 0.747824\n",
      "training accuracy 0.975  validation accuracy 0.7392\n",
      "Epoch 31, CIFAR-10 Batch 4:  training cost 0.295834  validation cost 0.731637\n",
      "training accuracy 0.975  validation accuracy 0.747\n",
      "Epoch 31, CIFAR-10 Batch 5:  training cost 0.316442  validation cost 0.743384\n",
      "training accuracy 0.95  validation accuracy 0.7404\n",
      "Epoch 32, CIFAR-10 Batch 1:  training cost 0.37029  validation cost 0.745704\n",
      "training accuracy 0.925  validation accuracy 0.7362\n",
      "Epoch 32, CIFAR-10 Batch 2:  training cost 0.292791  validation cost 0.719296\n",
      "training accuracy 0.975  validation accuracy 0.7514\n",
      "Epoch 32, CIFAR-10 Batch 3:  training cost 0.27476  validation cost 0.743424\n",
      "training accuracy 0.95  validation accuracy 0.7392\n",
      "Epoch 32, CIFAR-10 Batch 4:  training cost 0.266971  validation cost 0.729314\n",
      "training accuracy 0.975  validation accuracy 0.7424\n",
      "Epoch 32, CIFAR-10 Batch 5:  training cost 0.318682  validation cost 0.710212\n",
      "training accuracy 0.975  validation accuracy 0.7426\n",
      "Epoch 33, CIFAR-10 Batch 1:  training cost 0.363309  validation cost 0.74239\n",
      "training accuracy 0.9  validation accuracy 0.7388\n",
      "Epoch 33, CIFAR-10 Batch 2:  training cost 0.309904  validation cost 0.725712\n",
      "training accuracy 0.925  validation accuracy 0.7452\n",
      "Epoch 33, CIFAR-10 Batch 3:  training cost 0.212761  validation cost 0.730098\n",
      "training accuracy 0.975  validation accuracy 0.7476\n",
      "Epoch 33, CIFAR-10 Batch 4:  training cost 0.224389  validation cost 0.718228\n",
      "training accuracy 0.975  validation accuracy 0.7506\n",
      "Epoch 33, CIFAR-10 Batch 5:  training cost 0.287705  validation cost 0.714202\n",
      "training accuracy 0.975  validation accuracy 0.7524\n",
      "Epoch 34, CIFAR-10 Batch 1:  training cost 0.37298  validation cost 0.74674\n",
      "training accuracy 0.9  validation accuracy 0.7356\n",
      "Epoch 34, CIFAR-10 Batch 2:  training cost 0.306105  validation cost 0.719821\n",
      "training accuracy 0.95  validation accuracy 0.7474\n",
      "Epoch 34, CIFAR-10 Batch 3:  training cost 0.193424  validation cost 0.742137\n",
      "training accuracy 0.975  validation accuracy 0.746\n",
      "Epoch 34, CIFAR-10 Batch 4:  training cost 0.241422  validation cost 0.729256\n",
      "training accuracy 0.95  validation accuracy 0.7424\n",
      "Epoch 34, CIFAR-10 Batch 5:  training cost 0.279167  validation cost 0.715553\n",
      "training accuracy 0.95  validation accuracy 0.7494\n",
      "Epoch 35, CIFAR-10 Batch 1:  training cost 0.358388  validation cost 0.717533\n",
      "training accuracy 0.925  validation accuracy 0.7502\n",
      "Epoch 35, CIFAR-10 Batch 2:  training cost 0.32402  validation cost 0.716903\n",
      "training accuracy 0.925  validation accuracy 0.7462\n",
      "Epoch 35, CIFAR-10 Batch 3:  training cost 0.244714  validation cost 0.748902\n",
      "training accuracy 0.975  validation accuracy 0.742\n",
      "Epoch 35, CIFAR-10 Batch 4:  training cost 0.251105  validation cost 0.730309\n",
      "training accuracy 0.95  validation accuracy 0.7458\n",
      "Epoch 35, CIFAR-10 Batch 5:  training cost 0.334337  validation cost 0.743146\n",
      "training accuracy 0.9  validation accuracy 0.7368\n",
      "Epoch 36, CIFAR-10 Batch 1:  training cost 0.38191  validation cost 0.759797\n",
      "training accuracy 0.9  validation accuracy 0.7336\n",
      "Epoch 36, CIFAR-10 Batch 2:  training cost 0.325635  validation cost 0.724943\n",
      "training accuracy 0.9  validation accuracy 0.747\n",
      "Epoch 36, CIFAR-10 Batch 3:  training cost 0.231068  validation cost 0.706486\n",
      "training accuracy 0.925  validation accuracy 0.7534\n",
      "Epoch 36, CIFAR-10 Batch 4:  training cost 0.231911  validation cost 0.755706\n",
      "training accuracy 0.975  validation accuracy 0.7368\n",
      "Epoch 36, CIFAR-10 Batch 5:  training cost 0.289374  validation cost 0.765985\n",
      "training accuracy 0.925  validation accuracy 0.7322\n",
      "Epoch 37, CIFAR-10 Batch 1:  training cost 0.359919  validation cost 0.697587\n",
      "training accuracy 0.9  validation accuracy 0.7582\n",
      "Epoch 37, CIFAR-10 Batch 2:  training cost 0.268136  validation cost 0.71584\n",
      "training accuracy 0.95  validation accuracy 0.7516\n",
      "Epoch 37, CIFAR-10 Batch 3:  training cost 0.2069  validation cost 0.760345\n",
      "training accuracy 0.975  validation accuracy 0.7394\n",
      "Epoch 37, CIFAR-10 Batch 4:  training cost 0.203073  validation cost 0.721188\n",
      "training accuracy 1  validation accuracy 0.7516\n",
      "Epoch 37, CIFAR-10 Batch 5:  training cost 0.306583  validation cost 0.697304\n",
      "training accuracy 0.925  validation accuracy 0.7508\n",
      "Epoch 38, CIFAR-10 Batch 1:  training cost 0.324401  validation cost 0.720776\n",
      "training accuracy 0.9  validation accuracy 0.749\n",
      "Epoch 38, CIFAR-10 Batch 2:  training cost 0.261267  validation cost 0.73043\n",
      "training accuracy 0.95  validation accuracy 0.744\n",
      "Epoch 38, CIFAR-10 Batch 3:  training cost 0.246343  validation cost 0.761033\n",
      "training accuracy 0.975  validation accuracy 0.7344\n",
      "Epoch 38, CIFAR-10 Batch 4:  training cost 0.207803  validation cost 0.731004\n",
      "training accuracy 0.975  validation accuracy 0.7426\n",
      "Epoch 38, CIFAR-10 Batch 5:  training cost 0.259195  validation cost 0.745404\n",
      "training accuracy 0.95  validation accuracy 0.7364\n",
      "Epoch 39, CIFAR-10 Batch 1:  training cost 0.329063  validation cost 0.738854\n",
      "training accuracy 0.875  validation accuracy 0.7394\n",
      "Epoch 39, CIFAR-10 Batch 2:  training cost 0.243679  validation cost 0.701835\n",
      "training accuracy 0.95  validation accuracy 0.7514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, CIFAR-10 Batch 3:  training cost 0.212697  validation cost 0.725626\n",
      "training accuracy 0.975  validation accuracy 0.7446\n",
      "Epoch 39, CIFAR-10 Batch 4:  training cost 0.231682  validation cost 0.70212\n",
      "training accuracy 0.95  validation accuracy 0.7558\n",
      "Epoch 39, CIFAR-10 Batch 5:  training cost 0.266256  validation cost 0.735659\n",
      "training accuracy 0.975  validation accuracy 0.7384\n",
      "Epoch 40, CIFAR-10 Batch 1:  training cost 0.300351  validation cost 0.704304\n",
      "training accuracy 0.875  validation accuracy 0.751\n",
      "Epoch 40, CIFAR-10 Batch 2:  training cost 0.262295  validation cost 0.701765\n",
      "training accuracy 0.975  validation accuracy 0.7544\n",
      "Epoch 40, CIFAR-10 Batch 3:  training cost 0.184713  validation cost 0.698453\n",
      "training accuracy 0.975  validation accuracy 0.7562\n",
      "Epoch 40, CIFAR-10 Batch 4:  training cost 0.231993  validation cost 0.726134\n",
      "training accuracy 0.975  validation accuracy 0.7468\n",
      "Epoch 40, CIFAR-10 Batch 5:  training cost 0.272627  validation cost 0.715222\n",
      "training accuracy 0.95  validation accuracy 0.7454\n",
      "Epoch 41, CIFAR-10 Batch 1:  training cost 0.293458  validation cost 0.697935\n",
      "training accuracy 0.875  validation accuracy 0.7578\n",
      "Epoch 41, CIFAR-10 Batch 2:  training cost 0.218932  validation cost 0.71774\n",
      "training accuracy 1  validation accuracy 0.7504\n",
      "Epoch 41, CIFAR-10 Batch 3:  training cost 0.179514  validation cost 0.715184\n",
      "training accuracy 0.975  validation accuracy 0.7478\n",
      "Epoch 41, CIFAR-10 Batch 4:  training cost 0.17257  validation cost 0.71396\n",
      "training accuracy 1  validation accuracy 0.7514\n",
      "Epoch 41, CIFAR-10 Batch 5:  training cost 0.237359  validation cost 0.699773\n",
      "training accuracy 0.95  validation accuracy 0.752\n",
      "Epoch 42, CIFAR-10 Batch 1:  training cost 0.267308  validation cost 0.714817\n",
      "training accuracy 0.95  validation accuracy 0.7452\n",
      "Epoch 42, CIFAR-10 Batch 2:  training cost 0.213837  validation cost 0.717669\n",
      "training accuracy 0.975  validation accuracy 0.7498\n",
      "Epoch 42, CIFAR-10 Batch 3:  training cost 0.192872  validation cost 0.697031\n",
      "training accuracy 0.925  validation accuracy 0.7566\n",
      "Epoch 42, CIFAR-10 Batch 4:  training cost 0.19695  validation cost 0.702583\n",
      "training accuracy 1  validation accuracy 0.755\n",
      "Epoch 42, CIFAR-10 Batch 5:  training cost 0.255096  validation cost 0.693341\n",
      "training accuracy 0.975  validation accuracy 0.7558\n",
      "Epoch 43, CIFAR-10 Batch 1:  training cost 0.301637  validation cost 0.712911\n",
      "training accuracy 0.925  validation accuracy 0.7466\n",
      "Epoch 43, CIFAR-10 Batch 2:  training cost 0.169312  validation cost 0.681725\n",
      "training accuracy 1  validation accuracy 0.7646\n",
      "Epoch 43, CIFAR-10 Batch 3:  training cost 0.157016  validation cost 0.705128\n",
      "training accuracy 0.975  validation accuracy 0.7532\n",
      "Epoch 43, CIFAR-10 Batch 4:  training cost 0.176031  validation cost 0.730148\n",
      "training accuracy 0.975  validation accuracy 0.7472\n",
      "Epoch 43, CIFAR-10 Batch 5:  training cost 0.243423  validation cost 0.709231\n",
      "training accuracy 0.95  validation accuracy 0.7528\n",
      "Epoch 44, CIFAR-10 Batch 1:  training cost 0.312781  validation cost 0.677734\n",
      "training accuracy 0.9  validation accuracy 0.758\n",
      "Epoch 44, CIFAR-10 Batch 2:  training cost 0.21267  validation cost 0.70879\n",
      "training accuracy 0.975  validation accuracy 0.7528\n",
      "Epoch 44, CIFAR-10 Batch 3:  training cost 0.155782  validation cost 0.688327\n",
      "training accuracy 0.975  validation accuracy 0.7574\n",
      "Epoch 44, CIFAR-10 Batch 4:  training cost 0.190965  validation cost 0.713462\n",
      "training accuracy 0.975  validation accuracy 0.7548\n",
      "Epoch 44, CIFAR-10 Batch 5:  training cost 0.215207  validation cost 0.70273\n",
      "training accuracy 1  validation accuracy 0.7508\n",
      "Epoch 45, CIFAR-10 Batch 1:  training cost 0.238764  validation cost 0.697631\n",
      "training accuracy 0.9  validation accuracy 0.7602\n",
      "Epoch 45, CIFAR-10 Batch 2:  training cost 0.148637  validation cost 0.710163\n",
      "training accuracy 0.975  validation accuracy 0.752\n",
      "Epoch 45, CIFAR-10 Batch 3:  training cost 0.128872  validation cost 0.683233\n",
      "training accuracy 0.975  validation accuracy 0.7634\n",
      "Epoch 45, CIFAR-10 Batch 4:  training cost 0.163719  validation cost 0.704166\n",
      "training accuracy 0.975  validation accuracy 0.7564\n",
      "Epoch 45, CIFAR-10 Batch 5:  training cost 0.203704  validation cost 0.727973\n",
      "training accuracy 0.975  validation accuracy 0.742\n",
      "Epoch 46, CIFAR-10 Batch 1:  training cost 0.259689  validation cost 0.690487\n",
      "training accuracy 0.9  validation accuracy 0.7566\n",
      "Epoch 46, CIFAR-10 Batch 2:  training cost 0.175089  validation cost 0.678691\n",
      "training accuracy 1  validation accuracy 0.766\n",
      "Epoch 46, CIFAR-10 Batch 3:  training cost 0.169136  validation cost 0.685717\n",
      "training accuracy 0.925  validation accuracy 0.7636\n",
      "Epoch 46, CIFAR-10 Batch 4:  training cost 0.146257  validation cost 0.692343\n",
      "training accuracy 0.975  validation accuracy 0.7642\n",
      "Epoch 46, CIFAR-10 Batch 5:  training cost 0.192544  validation cost 0.708036\n",
      "training accuracy 0.975  validation accuracy 0.7474\n",
      "Epoch 47, CIFAR-10 Batch 1:  training cost 0.255361  validation cost 0.741694\n",
      "training accuracy 0.925  validation accuracy 0.745\n",
      "Epoch 47, CIFAR-10 Batch 2:  training cost 0.190352  validation cost 0.728401\n",
      "training accuracy 0.975  validation accuracy 0.7482\n",
      "Epoch 47, CIFAR-10 Batch 3:  training cost 0.171288  validation cost 0.692889\n",
      "training accuracy 1  validation accuracy 0.753\n",
      "Epoch 47, CIFAR-10 Batch 4:  training cost 0.133342  validation cost 0.707617\n",
      "training accuracy 0.975  validation accuracy 0.7544\n",
      "Epoch 47, CIFAR-10 Batch 5:  training cost 0.175089  validation cost 0.681759\n",
      "training accuracy 1  validation accuracy 0.7588\n",
      "Epoch 48, CIFAR-10 Batch 1:  training cost 0.252133  validation cost 0.704273\n",
      "training accuracy 0.925  validation accuracy 0.7518\n",
      "Epoch 48, CIFAR-10 Batch 2:  training cost 0.145461  validation cost 0.694034\n",
      "training accuracy 1  validation accuracy 0.764\n",
      "Epoch 48, CIFAR-10 Batch 3:  training cost 0.162324  validation cost 0.70822\n",
      "training accuracy 0.975  validation accuracy 0.7568\n",
      "Epoch 48, CIFAR-10 Batch 4:  training cost 0.151401  validation cost 0.709038\n",
      "training accuracy 0.975  validation accuracy 0.7468\n",
      "Epoch 48, CIFAR-10 Batch 5:  training cost 0.195888  validation cost 0.70838\n",
      "training accuracy 0.975  validation accuracy 0.7508\n",
      "Epoch 49, CIFAR-10 Batch 1:  training cost 0.228949  validation cost 0.700748\n",
      "training accuracy 0.9  validation accuracy 0.7564\n",
      "Epoch 49, CIFAR-10 Batch 2:  training cost 0.175612  validation cost 0.705822\n",
      "training accuracy 0.95  validation accuracy 0.7524\n",
      "Epoch 49, CIFAR-10 Batch 3:  training cost 0.173941  validation cost 0.681892\n",
      "training accuracy 0.975  validation accuracy 0.761\n",
      "Epoch 49, CIFAR-10 Batch 4:  training cost 0.159869  validation cost 0.673177\n",
      "training accuracy 0.975  validation accuracy 0.7704\n",
      "Epoch 49, CIFAR-10 Batch 5:  training cost 0.138742  validation cost 0.680973\n",
      "training accuracy 1  validation accuracy 0.7652\n",
      "Epoch 50, CIFAR-10 Batch 1:  training cost 0.221079  validation cost 0.691679\n",
      "training accuracy 0.925  validation accuracy 0.7568\n",
      "Epoch 50, CIFAR-10 Batch 2:  training cost 0.159639  validation cost 0.663593\n",
      "training accuracy 0.975  validation accuracy 0.7702\n",
      "Epoch 50, CIFAR-10 Batch 3:  training cost 0.1459  validation cost 0.658619\n",
      "training accuracy 0.95  validation accuracy 0.769\n",
      "Epoch 50, CIFAR-10 Batch 4:  training cost 0.173933  validation cost 0.6758\n",
      "training accuracy 0.975  validation accuracy 0.7666\n",
      "Epoch 50, CIFAR-10 Batch 5:  training cost 0.143696  validation cost 0.704174\n",
      "training accuracy 1  validation accuracy 0.7558\n",
      "Epoch 51, CIFAR-10 Batch 1:  training cost 0.215433  validation cost 0.69789\n",
      "training accuracy 0.975  validation accuracy 0.7548\n",
      "Epoch 51, CIFAR-10 Batch 2:  training cost 0.157366  validation cost 0.683643\n",
      "training accuracy 0.975  validation accuracy 0.7618\n",
      "Epoch 51, CIFAR-10 Batch 3:  training cost 0.197553  validation cost 0.689366\n",
      "training accuracy 0.95  validation accuracy 0.7588\n",
      "Epoch 51, CIFAR-10 Batch 4:  training cost 0.188418  validation cost 0.730114\n",
      "training accuracy 0.975  validation accuracy 0.747\n",
      "Epoch 51, CIFAR-10 Batch 5:  training cost 0.152505  validation cost 0.730676\n",
      "training accuracy 1  validation accuracy 0.7486\n",
      "Epoch 52, CIFAR-10 Batch 1:  training cost 0.202228  validation cost 0.675833\n",
      "training accuracy 0.95  validation accuracy 0.7644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, CIFAR-10 Batch 2:  training cost 0.139011  validation cost 0.685674\n",
      "training accuracy 0.975  validation accuracy 0.7654\n",
      "Epoch 52, CIFAR-10 Batch 3:  training cost 0.170835  validation cost 0.682838\n",
      "training accuracy 0.95  validation accuracy 0.7646\n",
      "Epoch 52, CIFAR-10 Batch 4:  training cost 0.140538  validation cost 0.67577\n",
      "training accuracy 0.975  validation accuracy 0.767\n",
      "Epoch 52, CIFAR-10 Batch 5:  training cost 0.130671  validation cost 0.678382\n",
      "training accuracy 1  validation accuracy 0.7638\n",
      "Epoch 53, CIFAR-10 Batch 1:  training cost 0.222419  validation cost 0.701749\n",
      "training accuracy 0.95  validation accuracy 0.7534\n",
      "Epoch 53, CIFAR-10 Batch 2:  training cost 0.171  validation cost 0.672567\n",
      "training accuracy 0.975  validation accuracy 0.7704\n",
      "Epoch 53, CIFAR-10 Batch 3:  training cost 0.177371  validation cost 0.667735\n",
      "training accuracy 0.975  validation accuracy 0.768\n",
      "Epoch 53, CIFAR-10 Batch 4:  training cost 0.150846  validation cost 0.67933\n",
      "training accuracy 0.975  validation accuracy 0.7632\n",
      "Epoch 53, CIFAR-10 Batch 5:  training cost 0.145661  validation cost 0.679438\n",
      "training accuracy 0.975  validation accuracy 0.7654\n",
      "Epoch 54, CIFAR-10 Batch 1:  training cost 0.249954  validation cost 0.68138\n",
      "training accuracy 0.9  validation accuracy 0.7652\n",
      "Epoch 54, CIFAR-10 Batch 2:  training cost 0.148824  validation cost 0.700267\n",
      "training accuracy 0.975  validation accuracy 0.7608\n",
      "Epoch 54, CIFAR-10 Batch 3:  training cost 0.176713  validation cost 0.70368\n",
      "training accuracy 0.975  validation accuracy 0.7584\n",
      "Epoch 54, CIFAR-10 Batch 4:  training cost 0.146598  validation cost 0.680283\n",
      "training accuracy 1  validation accuracy 0.759\n",
      "Epoch 54, CIFAR-10 Batch 5:  training cost 0.162475  validation cost 0.695346\n",
      "training accuracy 0.975  validation accuracy 0.7562\n",
      "Epoch 55, CIFAR-10 Batch 1:  training cost 0.215335  validation cost 0.67337\n",
      "training accuracy 0.95  validation accuracy 0.7644\n",
      "Epoch 55, CIFAR-10 Batch 2:  training cost 0.150263  validation cost 0.668864\n",
      "training accuracy 0.975  validation accuracy 0.7658\n",
      "Epoch 55, CIFAR-10 Batch 3:  training cost 0.158044  validation cost 0.692246\n",
      "training accuracy 0.95  validation accuracy 0.7574\n",
      "Epoch 55, CIFAR-10 Batch 4:  training cost 0.147837  validation cost 0.670282\n",
      "training accuracy 0.975  validation accuracy 0.7672\n",
      "Epoch 55, CIFAR-10 Batch 5:  training cost 0.0946944  validation cost 0.657191\n",
      "training accuracy 1  validation accuracy 0.7706\n",
      "Epoch 56, CIFAR-10 Batch 1:  training cost 0.222573  validation cost 0.689708\n",
      "training accuracy 0.925  validation accuracy 0.7616\n",
      "Epoch 56, CIFAR-10 Batch 2:  training cost 0.174745  validation cost 0.695902\n",
      "training accuracy 0.975  validation accuracy 0.7582\n",
      "Epoch 56, CIFAR-10 Batch 3:  training cost 0.167223  validation cost 0.661617\n",
      "training accuracy 0.975  validation accuracy 0.7728\n",
      "Epoch 56, CIFAR-10 Batch 4:  training cost 0.135722  validation cost 0.674801\n",
      "training accuracy 1  validation accuracy 0.771\n",
      "Epoch 56, CIFAR-10 Batch 5:  training cost 0.14449  validation cost 0.669649\n",
      "training accuracy 1  validation accuracy 0.7716\n",
      "Epoch 57, CIFAR-10 Batch 1:  training cost 0.182079  validation cost 0.664327\n",
      "training accuracy 0.95  validation accuracy 0.7694\n",
      "Epoch 57, CIFAR-10 Batch 2:  training cost 0.159742  validation cost 0.676216\n",
      "training accuracy 0.95  validation accuracy 0.7718\n",
      "Epoch 57, CIFAR-10 Batch 3:  training cost 0.12358  validation cost 0.679992\n",
      "training accuracy 0.975  validation accuracy 0.7696\n",
      "Epoch 57, CIFAR-10 Batch 4:  training cost 0.182369  validation cost 0.692642\n",
      "training accuracy 0.95  validation accuracy 0.7594\n",
      "Epoch 57, CIFAR-10 Batch 5:  training cost 0.127597  validation cost 0.677313\n",
      "training accuracy 1  validation accuracy 0.7588\n",
      "Epoch 58, CIFAR-10 Batch 1:  training cost 0.184486  validation cost 0.667253\n",
      "training accuracy 0.975  validation accuracy 0.7752\n",
      "Epoch 58, CIFAR-10 Batch 2:  training cost 0.153352  validation cost 0.684337\n",
      "training accuracy 0.975  validation accuracy 0.7662\n",
      "Epoch 58, CIFAR-10 Batch 3:  training cost 0.113424  validation cost 0.672961\n",
      "training accuracy 1  validation accuracy 0.77\n",
      "Epoch 58, CIFAR-10 Batch 4:  training cost 0.14663  validation cost 0.68305\n",
      "training accuracy 0.975  validation accuracy 0.7624\n",
      "Epoch 58, CIFAR-10 Batch 5:  training cost 0.138832  validation cost 0.672268\n",
      "training accuracy 1  validation accuracy 0.7706\n",
      "Epoch 59, CIFAR-10 Batch 1:  training cost 0.190214  validation cost 0.68633\n",
      "training accuracy 0.925  validation accuracy 0.763\n",
      "Epoch 59, CIFAR-10 Batch 2:  training cost 0.168918  validation cost 0.723412\n",
      "training accuracy 0.975  validation accuracy 0.7492\n",
      "Epoch 59, CIFAR-10 Batch 3:  training cost 0.160574  validation cost 0.68757\n",
      "training accuracy 1  validation accuracy 0.767\n",
      "Epoch 59, CIFAR-10 Batch 4:  training cost 0.158802  validation cost 0.706845\n",
      "training accuracy 0.975  validation accuracy 0.7586\n",
      "Epoch 59, CIFAR-10 Batch 5:  training cost 0.107114  validation cost 0.648613\n",
      "training accuracy 1  validation accuracy 0.7782\n",
      "Epoch 60, CIFAR-10 Batch 1:  training cost 0.146269  validation cost 0.66472\n",
      "training accuracy 1  validation accuracy 0.7732\n",
      "Epoch 60, CIFAR-10 Batch 2:  training cost 0.135339  validation cost 0.677551\n",
      "training accuracy 1  validation accuracy 0.7698\n",
      "Epoch 60, CIFAR-10 Batch 3:  training cost 0.167995  validation cost 0.677728\n",
      "training accuracy 0.95  validation accuracy 0.768\n",
      "Epoch 60, CIFAR-10 Batch 4:  training cost 0.126582  validation cost 0.639425\n",
      "training accuracy 1  validation accuracy 0.779\n",
      "Epoch 60, CIFAR-10 Batch 5:  training cost 0.136941  validation cost 0.668605\n",
      "training accuracy 0.975  validation accuracy 0.7698\n",
      "Epoch 61, CIFAR-10 Batch 1:  training cost 0.146868  validation cost 0.657894\n",
      "training accuracy 0.975  validation accuracy 0.7726\n",
      "Epoch 61, CIFAR-10 Batch 2:  training cost 0.109229  validation cost 0.689954\n",
      "training accuracy 1  validation accuracy 0.7646\n",
      "Epoch 61, CIFAR-10 Batch 3:  training cost 0.152115  validation cost 0.698051\n",
      "training accuracy 0.975  validation accuracy 0.764\n",
      "Epoch 61, CIFAR-10 Batch 4:  training cost 0.126743  validation cost 0.673397\n",
      "training accuracy 0.975  validation accuracy 0.7718\n",
      "Epoch 61, CIFAR-10 Batch 5:  training cost 0.112244  validation cost 0.671038\n",
      "training accuracy 1  validation accuracy 0.7662\n",
      "Epoch 62, CIFAR-10 Batch 1:  training cost 0.154481  validation cost 0.70635\n",
      "training accuracy 0.95  validation accuracy 0.7624\n",
      "Epoch 62, CIFAR-10 Batch 2:  training cost 0.10516  validation cost 0.654536\n",
      "training accuracy 1  validation accuracy 0.7804\n",
      "Epoch 62, CIFAR-10 Batch 3:  training cost 0.119809  validation cost 0.651389\n",
      "training accuracy 1  validation accuracy 0.7778\n",
      "Epoch 62, CIFAR-10 Batch 4:  training cost 0.13717  validation cost 0.666484\n",
      "training accuracy 1  validation accuracy 0.7726\n",
      "Epoch 62, CIFAR-10 Batch 5:  training cost 0.117829  validation cost 0.679493\n",
      "training accuracy 1  validation accuracy 0.7618\n",
      "Epoch 63, CIFAR-10 Batch 1:  training cost 0.133736  validation cost 0.660657\n",
      "training accuracy 0.95  validation accuracy 0.773\n",
      "Epoch 63, CIFAR-10 Batch 2:  training cost 0.101711  validation cost 0.649034\n",
      "training accuracy 1  validation accuracy 0.7754\n",
      "Epoch 63, CIFAR-10 Batch 3:  training cost 0.128052  validation cost 0.673473\n",
      "training accuracy 1  validation accuracy 0.767\n",
      "Epoch 63, CIFAR-10 Batch 4:  training cost 0.118148  validation cost 0.656973\n",
      "training accuracy 1  validation accuracy 0.775\n",
      "Epoch 63, CIFAR-10 Batch 5:  training cost 0.0999976  validation cost 0.668131\n",
      "training accuracy 1  validation accuracy 0.7776\n",
      "Epoch 64, CIFAR-10 Batch 1:  training cost 0.151094  validation cost 0.668517\n",
      "training accuracy 0.975  validation accuracy 0.7756\n",
      "Epoch 64, CIFAR-10 Batch 2:  training cost 0.0995906  validation cost 0.653648\n",
      "training accuracy 1  validation accuracy 0.7762\n",
      "Epoch 64, CIFAR-10 Batch 3:  training cost 0.127877  validation cost 0.666921\n",
      "training accuracy 1  validation accuracy 0.777\n",
      "Epoch 64, CIFAR-10 Batch 4:  training cost 0.122054  validation cost 0.654681\n",
      "training accuracy 0.975  validation accuracy 0.776\n",
      "Epoch 64, CIFAR-10 Batch 5:  training cost 0.104606  validation cost 0.66121\n",
      "training accuracy 1  validation accuracy 0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, CIFAR-10 Batch 1:  training cost 0.130297  validation cost 0.678348\n",
      "training accuracy 0.975  validation accuracy 0.767\n",
      "Epoch 65, CIFAR-10 Batch 2:  training cost 0.10595  validation cost 0.681545\n",
      "training accuracy 1  validation accuracy 0.7654\n",
      "Epoch 65, CIFAR-10 Batch 3:  training cost 0.0998434  validation cost 0.683817\n",
      "training accuracy 1  validation accuracy 0.7736\n",
      "Epoch 65, CIFAR-10 Batch 4:  training cost 0.116013  validation cost 0.669552\n",
      "training accuracy 0.975  validation accuracy 0.7726\n",
      "Epoch 65, CIFAR-10 Batch 5:  training cost 0.09229  validation cost 0.657041\n",
      "training accuracy 1  validation accuracy 0.7772\n",
      "Epoch 66, CIFAR-10 Batch 1:  training cost 0.136731  validation cost 0.685786\n",
      "training accuracy 0.975  validation accuracy 0.7672\n",
      "Epoch 66, CIFAR-10 Batch 2:  training cost 0.0962382  validation cost 0.684208\n",
      "training accuracy 1  validation accuracy 0.77\n",
      "Epoch 66, CIFAR-10 Batch 3:  training cost 0.113568  validation cost 0.679256\n",
      "training accuracy 1  validation accuracy 0.7674\n",
      "Epoch 66, CIFAR-10 Batch 4:  training cost 0.138891  validation cost 0.685602\n",
      "training accuracy 0.975  validation accuracy 0.7658\n",
      "Epoch 66, CIFAR-10 Batch 5:  training cost 0.0868411  validation cost 0.651787\n",
      "training accuracy 1  validation accuracy 0.7766\n",
      "Epoch 67, CIFAR-10 Batch 1:  training cost 0.157199  validation cost 0.670312\n",
      "training accuracy 1  validation accuracy 0.7708\n",
      "Epoch 67, CIFAR-10 Batch 2:  training cost 0.095595  validation cost 0.656956\n",
      "training accuracy 1  validation accuracy 0.7728\n",
      "Epoch 67, CIFAR-10 Batch 3:  training cost 0.150514  validation cost 0.661994\n",
      "training accuracy 0.975  validation accuracy 0.7752\n",
      "Epoch 67, CIFAR-10 Batch 4:  training cost 0.0909977  validation cost 0.649118\n",
      "training accuracy 1  validation accuracy 0.7804\n",
      "Epoch 67, CIFAR-10 Batch 5:  training cost 0.121856  validation cost 0.706397\n",
      "training accuracy 1  validation accuracy 0.7536\n",
      "Epoch 68, CIFAR-10 Batch 1:  training cost 0.145597  validation cost 0.688672\n",
      "training accuracy 0.975  validation accuracy 0.7636\n",
      "Epoch 68, CIFAR-10 Batch 2:  training cost 0.106726  validation cost 0.659459\n",
      "training accuracy 1  validation accuracy 0.7772\n",
      "Epoch 68, CIFAR-10 Batch 3:  training cost 0.108702  validation cost 0.68446\n",
      "training accuracy 1  validation accuracy 0.7678\n",
      "Epoch 68, CIFAR-10 Batch 4:  training cost 0.134934  validation cost 0.682346\n",
      "training accuracy 0.95  validation accuracy 0.7616\n",
      "Epoch 68, CIFAR-10 Batch 5:  training cost 0.117729  validation cost 0.690951\n",
      "training accuracy 0.975  validation accuracy 0.7582\n",
      "Epoch 69, CIFAR-10 Batch 1:  training cost 0.140642  validation cost 0.678142\n",
      "training accuracy 0.975  validation accuracy 0.7602\n",
      "Epoch 69, CIFAR-10 Batch 2:  training cost 0.0840502  validation cost 0.651554\n",
      "training accuracy 1  validation accuracy 0.7782\n",
      "Epoch 69, CIFAR-10 Batch 3:  training cost 0.110766  validation cost 0.66244\n",
      "training accuracy 0.95  validation accuracy 0.7798\n",
      "Epoch 69, CIFAR-10 Batch 4:  training cost 0.0973987  validation cost 0.671057\n",
      "training accuracy 1  validation accuracy 0.7678\n",
      "Epoch 69, CIFAR-10 Batch 5:  training cost 0.0796132  validation cost 0.658026\n",
      "training accuracy 1  validation accuracy 0.7678\n",
      "Epoch 70, CIFAR-10 Batch 1:  training cost 0.114443  validation cost 0.659822\n",
      "training accuracy 1  validation accuracy 0.7732\n",
      "Epoch 70, CIFAR-10 Batch 2:  training cost 0.0996121  validation cost 0.694304\n",
      "training accuracy 1  validation accuracy 0.7596\n",
      "Epoch 70, CIFAR-10 Batch 3:  training cost 0.104254  validation cost 0.674466\n",
      "training accuracy 1  validation accuracy 0.7738\n",
      "Epoch 70, CIFAR-10 Batch 4:  training cost 0.128392  validation cost 0.682756\n",
      "training accuracy 1  validation accuracy 0.7654\n",
      "Epoch 70, CIFAR-10 Batch 5:  training cost 0.0829693  validation cost 0.662072\n",
      "training accuracy 1  validation accuracy 0.776\n",
      "Epoch 71, CIFAR-10 Batch 1:  training cost 0.120904  validation cost 0.665916\n",
      "training accuracy 1  validation accuracy 0.7724\n",
      "Epoch 71, CIFAR-10 Batch 2:  training cost 0.0894633  validation cost 0.659711\n",
      "training accuracy 1  validation accuracy 0.7714\n",
      "Epoch 71, CIFAR-10 Batch 3:  training cost 0.0920546  validation cost 0.684726\n",
      "training accuracy 0.975  validation accuracy 0.7662\n",
      "Epoch 71, CIFAR-10 Batch 4:  training cost 0.0911896  validation cost 0.656587\n",
      "training accuracy 1  validation accuracy 0.7754\n",
      "Epoch 71, CIFAR-10 Batch 5:  training cost 0.075473  validation cost 0.657688\n",
      "training accuracy 1  validation accuracy 0.7734\n",
      "Epoch 72, CIFAR-10 Batch 1:  training cost 0.120457  validation cost 0.676359\n",
      "training accuracy 0.975  validation accuracy 0.767\n",
      "Epoch 72, CIFAR-10 Batch 2:  training cost 0.0893587  validation cost 0.656014\n",
      "training accuracy 1  validation accuracy 0.7786\n",
      "Epoch 72, CIFAR-10 Batch 3:  training cost 0.16877  validation cost 0.664806\n",
      "training accuracy 0.975  validation accuracy 0.7714\n",
      "Epoch 72, CIFAR-10 Batch 4:  training cost 0.0783587  validation cost 0.678446\n",
      "training accuracy 1  validation accuracy 0.769\n",
      "Epoch 72, CIFAR-10 Batch 5:  training cost 0.0866186  validation cost 0.659488\n",
      "training accuracy 1  validation accuracy 0.7764\n",
      "Epoch 73, CIFAR-10 Batch 1:  training cost 0.124819  validation cost 0.666248\n",
      "training accuracy 1  validation accuracy 0.7736\n",
      "Epoch 73, CIFAR-10 Batch 2:  training cost 0.0866293  validation cost 0.662026\n",
      "training accuracy 1  validation accuracy 0.776\n",
      "Epoch 73, CIFAR-10 Batch 3:  training cost 0.0916822  validation cost 0.656803\n",
      "training accuracy 1  validation accuracy 0.7766\n",
      "Epoch 73, CIFAR-10 Batch 4:  training cost 0.0667856  validation cost 0.655825\n",
      "training accuracy 1  validation accuracy 0.7776\n",
      "Epoch 73, CIFAR-10 Batch 5:  training cost 0.0972346  validation cost 0.663769\n",
      "training accuracy 1  validation accuracy 0.7692\n",
      "Epoch 74, CIFAR-10 Batch 1:  training cost 0.102008  validation cost 0.650908\n",
      "training accuracy 1  validation accuracy 0.775\n",
      "Epoch 74, CIFAR-10 Batch 2:  training cost 0.0983355  validation cost 0.678184\n",
      "training accuracy 1  validation accuracy 0.7694\n",
      "Epoch 74, CIFAR-10 Batch 3:  training cost 0.161521  validation cost 0.682245\n",
      "training accuracy 0.95  validation accuracy 0.7632\n",
      "Epoch 74, CIFAR-10 Batch 4:  training cost 0.0683087  validation cost 0.673378\n",
      "training accuracy 1  validation accuracy 0.7696\n",
      "Epoch 74, CIFAR-10 Batch 5:  training cost 0.0868017  validation cost 0.655046\n",
      "training accuracy 1  validation accuracy 0.7798\n",
      "Epoch 75, CIFAR-10 Batch 1:  training cost 0.09926  validation cost 0.68334\n",
      "training accuracy 0.975  validation accuracy 0.7624\n",
      "Epoch 75, CIFAR-10 Batch 2:  training cost 0.0827201  validation cost 0.673372\n",
      "training accuracy 1  validation accuracy 0.7694\n",
      "Epoch 75, CIFAR-10 Batch 3:  training cost 0.0881644  validation cost 0.660817\n",
      "training accuracy 1  validation accuracy 0.7772\n",
      "Epoch 75, CIFAR-10 Batch 4:  training cost 0.0967064  validation cost 0.663161\n",
      "training accuracy 0.975  validation accuracy 0.773\n",
      "Epoch 75, CIFAR-10 Batch 5:  training cost 0.0760069  validation cost 0.660607\n",
      "training accuracy 1  validation accuracy 0.773\n",
      "Epoch 76, CIFAR-10 Batch 1:  training cost 0.113366  validation cost 0.669611\n",
      "training accuracy 1  validation accuracy 0.7722\n",
      "Epoch 76, CIFAR-10 Batch 2:  training cost 0.0893686  validation cost 0.672076\n",
      "training accuracy 1  validation accuracy 0.7772\n",
      "Epoch 76, CIFAR-10 Batch 3:  training cost 0.119816  validation cost 0.647476\n",
      "training accuracy 1  validation accuracy 0.7834\n",
      "Epoch 76, CIFAR-10 Batch 4:  training cost 0.0632949  validation cost 0.659927\n",
      "training accuracy 1  validation accuracy 0.777\n",
      "Epoch 76, CIFAR-10 Batch 5:  training cost 0.0913107  validation cost 0.662904\n",
      "training accuracy 0.975  validation accuracy 0.776\n",
      "Epoch 77, CIFAR-10 Batch 1:  training cost 0.086333  validation cost 0.677943\n",
      "training accuracy 1  validation accuracy 0.768\n",
      "Epoch 77, CIFAR-10 Batch 2:  training cost 0.0827016  validation cost 0.6577\n",
      "training accuracy 1  validation accuracy 0.776\n",
      "Epoch 77, CIFAR-10 Batch 3:  training cost 0.0875568  validation cost 0.682547\n",
      "training accuracy 1  validation accuracy 0.7728\n",
      "Epoch 77, CIFAR-10 Batch 4:  training cost 0.0806544  validation cost 0.664835\n",
      "training accuracy 1  validation accuracy 0.7692\n",
      "Epoch 77, CIFAR-10 Batch 5:  training cost 0.104695  validation cost 0.671483\n",
      "training accuracy 1  validation accuracy 0.7726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, CIFAR-10 Batch 1:  training cost 0.111203  validation cost 0.686243\n",
      "training accuracy 0.975  validation accuracy 0.7684\n",
      "Epoch 78, CIFAR-10 Batch 2:  training cost 0.0826447  validation cost 0.659252\n",
      "training accuracy 1  validation accuracy 0.7768\n",
      "Epoch 78, CIFAR-10 Batch 3:  training cost 0.10793  validation cost 0.657566\n",
      "training accuracy 1  validation accuracy 0.7754\n",
      "Epoch 78, CIFAR-10 Batch 4:  training cost 0.0756187  validation cost 0.656249\n",
      "training accuracy 1  validation accuracy 0.7794\n",
      "Epoch 78, CIFAR-10 Batch 5:  training cost 0.0785495  validation cost 0.655164\n",
      "training accuracy 1  validation accuracy 0.7742\n",
      "Epoch 79, CIFAR-10 Batch 1:  training cost 0.102492  validation cost 0.649423\n",
      "training accuracy 1  validation accuracy 0.7794\n",
      "Epoch 79, CIFAR-10 Batch 2:  training cost 0.0842803  validation cost 0.685509\n",
      "training accuracy 1  validation accuracy 0.7734\n",
      "Epoch 79, CIFAR-10 Batch 3:  training cost 0.0984822  validation cost 0.681818\n",
      "training accuracy 1  validation accuracy 0.77\n",
      "Epoch 79, CIFAR-10 Batch 4:  training cost 0.0757854  validation cost 0.674134\n",
      "training accuracy 1  validation accuracy 0.7708\n",
      "Epoch 79, CIFAR-10 Batch 5:  training cost 0.0785067  validation cost 0.656365\n",
      "training accuracy 1  validation accuracy 0.7824\n",
      "Epoch 80, CIFAR-10 Batch 1:  training cost 0.0844338  validation cost 0.658198\n",
      "training accuracy 0.975  validation accuracy 0.7782\n",
      "Epoch 80, CIFAR-10 Batch 2:  training cost 0.0871977  validation cost 0.67376\n",
      "training accuracy 1  validation accuracy 0.7682\n",
      "Epoch 80, CIFAR-10 Batch 3:  training cost 0.0959762  validation cost 0.657906\n",
      "training accuracy 1  validation accuracy 0.7794\n",
      "Epoch 80, CIFAR-10 Batch 4:  training cost 0.0815994  validation cost 0.659216\n",
      "training accuracy 1  validation accuracy 0.7774\n",
      "Epoch 80, CIFAR-10 Batch 5:  training cost 0.070781  validation cost 0.643199\n",
      "training accuracy 1  validation accuracy 0.7848\n",
      "Epoch 81, CIFAR-10 Batch 1:  training cost 0.104405  validation cost 0.671373\n",
      "training accuracy 0.975  validation accuracy 0.7712\n",
      "Epoch 81, CIFAR-10 Batch 2:  training cost 0.0748811  validation cost 0.651476\n",
      "training accuracy 1  validation accuracy 0.7796\n",
      "Epoch 81, CIFAR-10 Batch 3:  training cost 0.130686  validation cost 0.671866\n",
      "training accuracy 1  validation accuracy 0.7704\n",
      "Epoch 81, CIFAR-10 Batch 4:  training cost 0.0764505  validation cost 0.637799\n",
      "training accuracy 1  validation accuracy 0.7874\n",
      "Epoch 81, CIFAR-10 Batch 5:  training cost 0.0691378  validation cost 0.65279\n",
      "training accuracy 1  validation accuracy 0.778\n",
      "Epoch 82, CIFAR-10 Batch 1:  training cost 0.0907837  validation cost 0.674672\n",
      "training accuracy 1  validation accuracy 0.7718\n",
      "Epoch 82, CIFAR-10 Batch 2:  training cost 0.0776501  validation cost 0.646738\n",
      "training accuracy 1  validation accuracy 0.7812\n",
      "Epoch 82, CIFAR-10 Batch 3:  training cost 0.0961403  validation cost 0.644719\n",
      "training accuracy 1  validation accuracy 0.7816\n",
      "Epoch 82, CIFAR-10 Batch 4:  training cost 0.0821393  validation cost 0.6616\n",
      "training accuracy 1  validation accuracy 0.7744\n",
      "Epoch 82, CIFAR-10 Batch 5:  training cost 0.0742917  validation cost 0.664135\n",
      "training accuracy 1  validation accuracy 0.7796\n",
      "Epoch 83, CIFAR-10 Batch 1:  training cost 0.0898873  validation cost 0.696484\n",
      "training accuracy 1  validation accuracy 0.769\n",
      "Epoch 83, CIFAR-10 Batch 2:  training cost 0.0616899  validation cost 0.655452\n",
      "training accuracy 1  validation accuracy 0.7758\n",
      "Epoch 83, CIFAR-10 Batch 3:  training cost 0.107113  validation cost 0.681209\n",
      "training accuracy 1  validation accuracy 0.7694\n",
      "Epoch 83, CIFAR-10 Batch 4:  training cost 0.076199  validation cost 0.650533\n",
      "training accuracy 1  validation accuracy 0.7824\n",
      "Epoch 83, CIFAR-10 Batch 5:  training cost 0.0785877  validation cost 0.663197\n",
      "training accuracy 1  validation accuracy 0.7768\n",
      "Epoch 84, CIFAR-10 Batch 1:  training cost 0.0986697  validation cost 0.694758\n",
      "training accuracy 1  validation accuracy 0.7642\n",
      "Epoch 84, CIFAR-10 Batch 2:  training cost 0.0786683  validation cost 0.64909\n",
      "training accuracy 1  validation accuracy 0.7814\n",
      "Epoch 84, CIFAR-10 Batch 3:  training cost 0.100384  validation cost 0.665076\n",
      "training accuracy 1  validation accuracy 0.7744\n",
      "Epoch 84, CIFAR-10 Batch 4:  training cost 0.0620822  validation cost 0.646513\n",
      "training accuracy 1  validation accuracy 0.7798\n",
      "Epoch 84, CIFAR-10 Batch 5:  training cost 0.0602023  validation cost 0.664902\n",
      "training accuracy 1  validation accuracy 0.7786\n",
      "Epoch 85, CIFAR-10 Batch 1:  training cost 0.0900032  validation cost 0.670757\n",
      "training accuracy 1  validation accuracy 0.7738\n",
      "Epoch 85, CIFAR-10 Batch 2:  training cost 0.0882207  validation cost 0.669905\n",
      "training accuracy 1  validation accuracy 0.7754\n",
      "Epoch 85, CIFAR-10 Batch 3:  training cost 0.101098  validation cost 0.676841\n",
      "training accuracy 1  validation accuracy 0.7734\n",
      "Epoch 85, CIFAR-10 Batch 4:  training cost 0.0770271  validation cost 0.659194\n",
      "training accuracy 1  validation accuracy 0.7772\n",
      "Epoch 85, CIFAR-10 Batch 5:  training cost 0.0905121  validation cost 0.658555\n",
      "training accuracy 1  validation accuracy 0.7804\n",
      "Epoch 86, CIFAR-10 Batch 1:  training cost 0.0768775  validation cost 0.664784\n",
      "training accuracy 1  validation accuracy 0.7778\n",
      "Epoch 86, CIFAR-10 Batch 2:  training cost 0.0783387  validation cost 0.6896\n",
      "training accuracy 1  validation accuracy 0.768\n",
      "Epoch 86, CIFAR-10 Batch 3:  training cost 0.0779254  validation cost 0.656502\n",
      "training accuracy 1  validation accuracy 0.7806\n",
      "Epoch 86, CIFAR-10 Batch 4:  training cost 0.0780264  validation cost 0.64284\n",
      "training accuracy 0.975  validation accuracy 0.7842\n",
      "Epoch 86, CIFAR-10 Batch 5:  training cost 0.0759803  validation cost 0.656854\n",
      "training accuracy 1  validation accuracy 0.7782\n",
      "Epoch 87, CIFAR-10 Batch 1:  training cost 0.0764745  validation cost 0.644169\n",
      "training accuracy 1  validation accuracy 0.7882\n",
      "Epoch 87, CIFAR-10 Batch 2:  training cost 0.0628409  validation cost 0.670581\n",
      "training accuracy 1  validation accuracy 0.774\n",
      "Epoch 87, CIFAR-10 Batch 3:  training cost 0.0893543  validation cost 0.661612\n",
      "training accuracy 1  validation accuracy 0.7762\n",
      "Epoch 87, CIFAR-10 Batch 4:  training cost 0.0759805  validation cost 0.641079\n",
      "training accuracy 1  validation accuracy 0.7814\n",
      "Epoch 87, CIFAR-10 Batch 5:  training cost 0.0945145  validation cost 0.709918\n",
      "training accuracy 1  validation accuracy 0.7624\n",
      "Epoch 88, CIFAR-10 Batch 1:  training cost 0.0726583  validation cost 0.68126\n",
      "training accuracy 1  validation accuracy 0.773\n",
      "Epoch 88, CIFAR-10 Batch 2:  training cost 0.0909393  validation cost 0.671905\n",
      "training accuracy 1  validation accuracy 0.775\n",
      "Epoch 88, CIFAR-10 Batch 3:  training cost 0.0771097  validation cost 0.646993\n",
      "training accuracy 1  validation accuracy 0.7802\n",
      "Epoch 88, CIFAR-10 Batch 4:  training cost 0.0744866  validation cost 0.653079\n",
      "training accuracy 1  validation accuracy 0.7858\n",
      "Epoch 88, CIFAR-10 Batch 5:  training cost 0.069615  validation cost 0.661917\n",
      "training accuracy 1  validation accuracy 0.7756\n",
      "Epoch 89, CIFAR-10 Batch 1:  training cost 0.0772559  validation cost 0.668161\n",
      "training accuracy 1  validation accuracy 0.7792\n",
      "Epoch 89, CIFAR-10 Batch 2:  training cost 0.0538484  validation cost 0.642062\n",
      "training accuracy 1  validation accuracy 0.7814\n",
      "Epoch 89, CIFAR-10 Batch 3:  training cost 0.0929169  validation cost 0.646602\n",
      "training accuracy 0.975  validation accuracy 0.7808\n",
      "Epoch 89, CIFAR-10 Batch 4:  training cost 0.0610992  validation cost 0.645209\n",
      "training accuracy 1  validation accuracy 0.787\n",
      "Epoch 89, CIFAR-10 Batch 5:  training cost 0.0541928  validation cost 0.643342\n",
      "training accuracy 1  validation accuracy 0.7838\n",
      "Epoch 90, CIFAR-10 Batch 1:  training cost 0.0753373  validation cost 0.678776\n",
      "training accuracy 1  validation accuracy 0.7738\n",
      "Epoch 90, CIFAR-10 Batch 2:  training cost 0.0903223  validation cost 0.661748\n",
      "training accuracy 0.975  validation accuracy 0.775\n",
      "Epoch 90, CIFAR-10 Batch 3:  training cost 0.091568  validation cost 0.673727\n",
      "training accuracy 0.975  validation accuracy 0.775\n",
      "Epoch 90, CIFAR-10 Batch 4:  training cost 0.0593956  validation cost 0.639427\n",
      "training accuracy 1  validation accuracy 0.7856\n",
      "Epoch 90, CIFAR-10 Batch 5:  training cost 0.0617597  validation cost 0.67412\n",
      "training accuracy 1  validation accuracy 0.7728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, CIFAR-10 Batch 1:  training cost 0.0591611  validation cost 0.64513\n",
      "training accuracy 1  validation accuracy 0.786\n",
      "Epoch 91, CIFAR-10 Batch 2:  training cost 0.118309  validation cost 0.677542\n",
      "training accuracy 0.975  validation accuracy 0.7722\n",
      "Epoch 91, CIFAR-10 Batch 3:  training cost 0.0711254  validation cost 0.642071\n",
      "training accuracy 1  validation accuracy 0.7832\n",
      "Epoch 91, CIFAR-10 Batch 4:  training cost 0.0744535  validation cost 0.665897\n",
      "training accuracy 1  validation accuracy 0.7754\n",
      "Epoch 91, CIFAR-10 Batch 5:  training cost 0.0552788  validation cost 0.655048\n",
      "training accuracy 1  validation accuracy 0.7796\n",
      "Epoch 92, CIFAR-10 Batch 1:  training cost 0.0541424  validation cost 0.651557\n",
      "training accuracy 1  validation accuracy 0.7848\n",
      "Epoch 92, CIFAR-10 Batch 2:  training cost 0.0544929  validation cost 0.632448\n",
      "training accuracy 1  validation accuracy 0.7864\n",
      "Epoch 92, CIFAR-10 Batch 3:  training cost 0.0804155  validation cost 0.666612\n",
      "training accuracy 1  validation accuracy 0.7774\n",
      "Epoch 92, CIFAR-10 Batch 4:  training cost 0.0569682  validation cost 0.642126\n",
      "training accuracy 1  validation accuracy 0.7846\n",
      "Epoch 92, CIFAR-10 Batch 5:  training cost 0.0649784  validation cost 0.692775\n",
      "training accuracy 1  validation accuracy 0.7626\n",
      "Epoch 93, CIFAR-10 Batch 1:  training cost 0.0522911  validation cost 0.652153\n",
      "training accuracy 1  validation accuracy 0.7794\n",
      "Epoch 93, CIFAR-10 Batch 2:  training cost 0.0723666  validation cost 0.661725\n",
      "training accuracy 1  validation accuracy 0.776\n",
      "Epoch 93, CIFAR-10 Batch 3:  training cost 0.0706391  validation cost 0.6376\n",
      "training accuracy 1  validation accuracy 0.786\n",
      "Epoch 93, CIFAR-10 Batch 4:  training cost 0.0721303  validation cost 0.65302\n",
      "training accuracy 1  validation accuracy 0.7842\n",
      "Epoch 93, CIFAR-10 Batch 5:  training cost 0.0573955  validation cost 0.661902\n",
      "training accuracy 1  validation accuracy 0.7728\n",
      "Epoch 94, CIFAR-10 Batch 1:  training cost 0.0605177  validation cost 0.65851\n",
      "training accuracy 1  validation accuracy 0.775\n",
      "Epoch 94, CIFAR-10 Batch 2:  training cost 0.0797827  validation cost 0.673581\n",
      "training accuracy 1  validation accuracy 0.7712\n",
      "Epoch 94, CIFAR-10 Batch 3:  training cost 0.094054  validation cost 0.632764\n",
      "training accuracy 0.975  validation accuracy 0.7906\n",
      "Epoch 94, CIFAR-10 Batch 4:  training cost 0.0833704  validation cost 0.655371\n",
      "training accuracy 1  validation accuracy 0.782\n",
      "Epoch 94, CIFAR-10 Batch 5:  training cost 0.0574732  validation cost 0.636233\n",
      "training accuracy 1  validation accuracy 0.7812\n",
      "Epoch 95, CIFAR-10 Batch 1:  training cost 0.0591416  validation cost 0.641299\n",
      "training accuracy 1  validation accuracy 0.7878\n",
      "Epoch 95, CIFAR-10 Batch 2:  training cost 0.0577081  validation cost 0.674519\n",
      "training accuracy 1  validation accuracy 0.7772\n",
      "Epoch 95, CIFAR-10 Batch 3:  training cost 0.0865795  validation cost 0.647292\n",
      "training accuracy 1  validation accuracy 0.7846\n",
      "Epoch 95, CIFAR-10 Batch 4:  training cost 0.046463  validation cost 0.663702\n",
      "training accuracy 1  validation accuracy 0.7764\n",
      "Epoch 95, CIFAR-10 Batch 5:  training cost 0.0571854  validation cost 0.665512\n",
      "training accuracy 1  validation accuracy 0.7742\n",
      "Epoch 96, CIFAR-10 Batch 1:  training cost 0.0770586  validation cost 0.693754\n",
      "training accuracy 1  validation accuracy 0.774\n",
      "Epoch 96, CIFAR-10 Batch 2:  training cost 0.0533959  validation cost 0.653899\n",
      "training accuracy 1  validation accuracy 0.7796\n",
      "Epoch 96, CIFAR-10 Batch 3:  training cost 0.0760386  validation cost 0.664604\n",
      "training accuracy 1  validation accuracy 0.777\n",
      "Epoch 96, CIFAR-10 Batch 4:  training cost 0.0382962  validation cost 0.642844\n",
      "training accuracy 1  validation accuracy 0.7858\n",
      "Epoch 96, CIFAR-10 Batch 5:  training cost 0.0651711  validation cost 0.647532\n",
      "training accuracy 1  validation accuracy 0.7886\n",
      "Epoch 97, CIFAR-10 Batch 1:  training cost 0.0588471  validation cost 0.648645\n",
      "training accuracy 1  validation accuracy 0.7852\n",
      "Epoch 97, CIFAR-10 Batch 2:  training cost 0.048977  validation cost 0.626561\n",
      "training accuracy 1  validation accuracy 0.7942\n",
      "Epoch 97, CIFAR-10 Batch 3:  training cost 0.0870513  validation cost 0.659196\n",
      "training accuracy 1  validation accuracy 0.7808\n",
      "Epoch 97, CIFAR-10 Batch 4:  training cost 0.0673262  validation cost 0.664982\n",
      "training accuracy 1  validation accuracy 0.7878\n",
      "Epoch 97, CIFAR-10 Batch 5:  training cost 0.0518004  validation cost 0.636107\n",
      "training accuracy 1  validation accuracy 0.7814\n",
      "Epoch 98, CIFAR-10 Batch 1:  training cost 0.0664642  validation cost 0.671901\n",
      "training accuracy 1  validation accuracy 0.7734\n",
      "Epoch 98, CIFAR-10 Batch 2:  training cost 0.0693383  validation cost 0.671538\n",
      "training accuracy 1  validation accuracy 0.775\n",
      "Epoch 98, CIFAR-10 Batch 3:  training cost 0.124651  validation cost 0.654918\n",
      "training accuracy 0.975  validation accuracy 0.7818\n",
      "Epoch 98, CIFAR-10 Batch 4:  training cost 0.065179  validation cost 0.650886\n",
      "training accuracy 1  validation accuracy 0.778\n",
      "Epoch 98, CIFAR-10 Batch 5:  training cost 0.0818311  validation cost 0.638374\n",
      "training accuracy 0.975  validation accuracy 0.7876\n",
      "Epoch 99, CIFAR-10 Batch 1:  training cost 0.0559167  validation cost 0.679647\n",
      "training accuracy 1  validation accuracy 0.7732\n",
      "Epoch 99, CIFAR-10 Batch 2:  training cost 0.0615576  validation cost 0.646729\n",
      "training accuracy 1  validation accuracy 0.7828\n",
      "Epoch 99, CIFAR-10 Batch 3:  training cost 0.0820442  validation cost 0.661259\n",
      "training accuracy 1  validation accuracy 0.7798\n",
      "Epoch 99, CIFAR-10 Batch 4:  training cost 0.0581683  validation cost 0.687574\n",
      "training accuracy 1  validation accuracy 0.771\n",
      "Epoch 99, CIFAR-10 Batch 5:  training cost 0.0625025  validation cost 0.645368\n",
      "training accuracy 1  validation accuracy 0.7846\n",
      "Epoch 100, CIFAR-10 Batch 1:  training cost 0.0517367  validation cost 0.642123\n",
      "training accuracy 1  validation accuracy 0.7858\n",
      "Epoch 100, CIFAR-10 Batch 2:  training cost 0.0731137  validation cost 0.664625\n",
      "training accuracy 0.975  validation accuracy 0.78\n",
      "Epoch 100, CIFAR-10 Batch 3:  training cost 0.111261  validation cost 0.638761\n",
      "training accuracy 1  validation accuracy 0.7826\n",
      "Epoch 100, CIFAR-10 Batch 4:  training cost 0.0620635  validation cost 0.637492\n",
      "training accuracy 1  validation accuracy 0.7886\n",
      "Epoch 100, CIFAR-10 Batch 5:  training cost 0.0613075  validation cost 0.653497\n",
      "training accuracy 1  validation accuracy 0.7778\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7665150316455697\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI/rpcm68AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9b8a62358>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
