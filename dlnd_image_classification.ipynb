{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26bb102828>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    normalized_x = x / 255.0\n",
    "    return normalized_x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # IDK why the method is called 2 times in the test\n",
    "    # 1 with a list, other with a nd-array\n",
    "    \n",
    "    qtd = 168\n",
    "    if type(x) == np.ndarray:\n",
    "        qtd=x.shape[0]\n",
    "    else:\n",
    "        qtd=len(x)\n",
    "    \n",
    "    # The actual function is here\n",
    "    hot_x = np.zeros((qtd, 10))\n",
    "    for i, label in enumerate(x):\n",
    "        x_label = np.zeros(10)\n",
    "        x_label[label] = 1\n",
    "        hot_x[i] = x_label\n",
    "    return hot_x\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, \n",
    "                          shape=[None, image_shape[0], image_shape[1], image_shape[2]], \n",
    "                          name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    x_shape = [x for x in filter(lambda x: x != None, x_tensor.get_shape().as_list())]\n",
    "    shape = list(conv_ksize) + [x_shape[2], conv_num_outputs]\n",
    "    \n",
    "    c_strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    p_strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    p_ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    \n",
    "    W_conv = weight_variable(shape)\n",
    "    b_conv = bias_variable([conv_num_outputs])\n",
    "    conv_op = tf.nn.conv2d(x_tensor, W_conv, strides=c_strides, padding='SAME')\n",
    "    # Add bias\n",
    "    conv_op = tf.nn.bias_add(conv_op, b_conv)\n",
    "    h_conv = tf.nn.relu(conv_op)\n",
    "    \n",
    "    return tf.nn.max_pool(h_conv, ksize=p_ksize, strides=p_strides, padding='SAME')\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # Seems this solution is not working\n",
    "    #flat_shape = np.prod(np.array(x_tensor.shape[1:]))\n",
    "    #dim = tf.reduce_prod(tf.shape(x_tensor)[1:])\n",
    "    #flat = tf.reshape(x_tensor, [-1, dim])\n",
    "    #return flat\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv1 = conv2d_maxpool(x, 32, (3, 3), (2, 2), (4, 4), (2, 2))\n",
    "    conv2 = conv2d_maxpool(conv1, 64, (3, 3), (2, 2), (2, 2), (2, 2))\n",
    "    conv3 = conv2d_maxpool(conv2, 128, (3, 3), (2, 2), (2, 2), (2, 2))\n",
    "    \n",
    "    flattened = flatten(conv3)\n",
    "    \n",
    "    # Dropout\n",
    "    # Course review\n",
    "    fc1 = fully_conn(flattened, 128)\n",
    "    dropout1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    fc2 = fully_conn(fc1, 64)\n",
    "    dropout2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    fc3 = fully_conn(fc2, 32)\n",
    "    fc4 = fully_conn(fc3, 16)\n",
    "    \n",
    "    y = output(fc4, 10)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    return session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    c = session.run(cost, feed_dict={x: feature_batch, y: label_batch })\n",
    "    acc = session.run(accuracy, feed_dict={x: feature_batch, y: label_batch })\n",
    "    \n",
    "    c1 = session.run(cost, feed_dict={x: valid_features, y: valid_labels })\n",
    "    acc1 = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels })\n",
    "    \n",
    "    # Course Review\n",
    "    # Use the global variables valid_features and valid_labels to calculate validation accuracy.\n",
    "    print('training cost %g' % c, \" validation cost %g\" % c1)\n",
    "    print('training accuracy %g' % acc, \" validation accuracy %g\" % acc1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 300\n",
    "batch_size = 32\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  training cost 2.14491  validation cost 1.97686\n",
      "training accuracy 0.25  validation accuracy 0.2472\n",
      "Epoch  2, CIFAR-10 Batch 1:  training cost 2.01564  validation cost 1.8641\n",
      "training accuracy 0.25  validation accuracy 0.3034\n",
      "Epoch  3, CIFAR-10 Batch 1:  training cost 1.66777  validation cost 1.72509\n",
      "training accuracy 0.375  validation accuracy 0.3612\n",
      "Epoch  4, CIFAR-10 Batch 1:  training cost 1.63736  validation cost 1.70017\n",
      "training accuracy 0.5  validation accuracy 0.3778\n",
      "Epoch  5, CIFAR-10 Batch 1:  training cost 1.44173  validation cost 1.57141\n",
      "training accuracy 0.5  validation accuracy 0.4194\n",
      "Epoch  6, CIFAR-10 Batch 1:  training cost 1.13985  validation cost 1.46224\n",
      "training accuracy 0.625  validation accuracy 0.4706\n",
      "Epoch  7, CIFAR-10 Batch 1:  training cost 1.02495  validation cost 1.41217\n",
      "training accuracy 0.625  validation accuracy 0.4898\n",
      "Epoch  8, CIFAR-10 Batch 1:  training cost 0.888131  validation cost 1.37216\n",
      "training accuracy 0.75  validation accuracy 0.5038\n",
      "Epoch  9, CIFAR-10 Batch 1:  training cost 0.813854  validation cost 1.34887\n",
      "training accuracy 0.75  validation accuracy 0.5166\n",
      "Epoch 10, CIFAR-10 Batch 1:  training cost 0.721217  validation cost 1.31493\n",
      "training accuracy 0.75  validation accuracy 0.5338\n",
      "Epoch 11, CIFAR-10 Batch 1:  training cost 0.616694  validation cost 1.33189\n",
      "training accuracy 0.75  validation accuracy 0.5346\n",
      "Epoch 12, CIFAR-10 Batch 1:  training cost 0.565757  validation cost 1.33836\n",
      "training accuracy 0.75  validation accuracy 0.5356\n",
      "Epoch 13, CIFAR-10 Batch 1:  training cost 0.496515  validation cost 1.37975\n",
      "training accuracy 1  validation accuracy 0.5344\n",
      "Epoch 14, CIFAR-10 Batch 1:  training cost 0.402164  validation cost 1.32978\n",
      "training accuracy 0.875  validation accuracy 0.5474\n",
      "Epoch 15, CIFAR-10 Batch 1:  training cost 0.317521  validation cost 1.35786\n",
      "training accuracy 1  validation accuracy 0.5446\n",
      "Epoch 16, CIFAR-10 Batch 1:  training cost 0.289522  validation cost 1.3932\n",
      "training accuracy 0.875  validation accuracy 0.5454\n",
      "Epoch 17, CIFAR-10 Batch 1:  training cost 0.178141  validation cost 1.45461\n",
      "training accuracy 1  validation accuracy 0.5388\n",
      "Epoch 18, CIFAR-10 Batch 1:  training cost 0.185197  validation cost 1.51545\n",
      "training accuracy 1  validation accuracy 0.5366\n",
      "Epoch 19, CIFAR-10 Batch 1:  training cost 0.148423  validation cost 1.56238\n",
      "training accuracy 1  validation accuracy 0.5296\n",
      "Epoch 20, CIFAR-10 Batch 1:  training cost 0.144451  validation cost 1.5927\n",
      "training accuracy 1  validation accuracy 0.5316\n",
      "Epoch 21, CIFAR-10 Batch 1:  training cost 0.103409  validation cost 1.65542\n",
      "training accuracy 1  validation accuracy 0.534\n",
      "Epoch 22, CIFAR-10 Batch 1:  training cost 0.0889724  validation cost 1.65832\n",
      "training accuracy 1  validation accuracy 0.529\n",
      "Epoch 23, CIFAR-10 Batch 1:  training cost 0.0965749  validation cost 1.72709\n",
      "training accuracy 1  validation accuracy 0.5162\n",
      "Epoch 24, CIFAR-10 Batch 1:  training cost 0.0973492  validation cost 1.83478\n",
      "training accuracy 1  validation accuracy 0.5132\n",
      "Epoch 25, CIFAR-10 Batch 1:  training cost 0.109462  validation cost 1.89664\n",
      "training accuracy 1  validation accuracy 0.5136\n",
      "Epoch 26, CIFAR-10 Batch 1:  training cost 0.0637986  validation cost 1.9484\n",
      "training accuracy 1  validation accuracy 0.5296\n",
      "Epoch 27, CIFAR-10 Batch 1:  training cost 0.0546412  validation cost 2.12407\n",
      "training accuracy 1  validation accuracy 0.507\n",
      "Epoch 28, CIFAR-10 Batch 1:  training cost 0.0338961  validation cost 2.02572\n",
      "training accuracy 1  validation accuracy 0.5286\n",
      "Epoch 29, CIFAR-10 Batch 1:  training cost 0.0436091  validation cost 2.16118\n",
      "training accuracy 1  validation accuracy 0.5206\n",
      "Epoch 30, CIFAR-10 Batch 1:  training cost 0.0385016  validation cost 2.08026\n",
      "training accuracy 1  validation accuracy 0.5276\n",
      "Epoch 31, CIFAR-10 Batch 1:  training cost 0.0184619  validation cost 2.09768\n",
      "training accuracy 1  validation accuracy 0.5358\n",
      "Epoch 32, CIFAR-10 Batch 1:  training cost 0.0392421  validation cost 2.32657\n",
      "training accuracy 1  validation accuracy 0.524\n",
      "Epoch 33, CIFAR-10 Batch 1:  training cost 0.0336741  validation cost 2.21573\n",
      "training accuracy 1  validation accuracy 0.5206\n",
      "Epoch 34, CIFAR-10 Batch 1:  training cost 0.0387003  validation cost 2.2651\n",
      "training accuracy 1  validation accuracy 0.5338\n",
      "Epoch 35, CIFAR-10 Batch 1:  training cost 0.0421433  validation cost 2.48612\n",
      "training accuracy 1  validation accuracy 0.5258\n",
      "Epoch 36, CIFAR-10 Batch 1:  training cost 0.0132713  validation cost 2.42835\n",
      "training accuracy 1  validation accuracy 0.5314\n",
      "Epoch 37, CIFAR-10 Batch 1:  training cost 0.0412472  validation cost 2.49658\n",
      "training accuracy 1  validation accuracy 0.5216\n",
      "Epoch 38, CIFAR-10 Batch 1:  training cost 0.052997  validation cost 2.56975\n",
      "training accuracy 1  validation accuracy 0.5224\n",
      "Epoch 39, CIFAR-10 Batch 1:  training cost 0.00915289  validation cost 2.46961\n",
      "training accuracy 1  validation accuracy 0.5176\n",
      "Epoch 40, CIFAR-10 Batch 1:  training cost 0.0059572  validation cost 2.89886\n",
      "training accuracy 1  validation accuracy 0.5086\n",
      "Epoch 41, CIFAR-10 Batch 1:  training cost 0.0129343  validation cost 2.62269\n",
      "training accuracy 1  validation accuracy 0.5192\n",
      "Epoch 42, CIFAR-10 Batch 1:  training cost 0.0310186  validation cost 2.84283\n",
      "training accuracy 1  validation accuracy 0.5132\n",
      "Epoch 43, CIFAR-10 Batch 1:  training cost 0.00533757  validation cost 2.80844\n",
      "training accuracy 1  validation accuracy 0.5132\n",
      "Epoch 44, CIFAR-10 Batch 1:  training cost 0.0206395  validation cost 2.91654\n",
      "training accuracy 1  validation accuracy 0.5066\n",
      "Epoch 45, CIFAR-10 Batch 1:  training cost 0.0183166  validation cost 2.85303\n",
      "training accuracy 1  validation accuracy 0.5156\n",
      "Epoch 46, CIFAR-10 Batch 1:  training cost 0.0113159  validation cost 2.77721\n",
      "training accuracy 1  validation accuracy 0.5114\n",
      "Epoch 47, CIFAR-10 Batch 1:  training cost 0.0104829  validation cost 2.81276\n",
      "training accuracy 1  validation accuracy 0.519\n",
      "Epoch 48, CIFAR-10 Batch 1:  training cost 0.0137335  validation cost 2.88102\n",
      "training accuracy 1  validation accuracy 0.5212\n",
      "Epoch 49, CIFAR-10 Batch 1:  training cost 0.0154597  validation cost 2.99121\n",
      "training accuracy 1  validation accuracy 0.5206\n",
      "Epoch 50, CIFAR-10 Batch 1:  training cost 0.0155535  validation cost 2.93944\n",
      "training accuracy 1  validation accuracy 0.5164\n",
      "Epoch 51, CIFAR-10 Batch 1:  training cost 0.00303921  validation cost 3.15388\n",
      "training accuracy 1  validation accuracy 0.5146\n",
      "Epoch 52, CIFAR-10 Batch 1:  training cost 0.0123817  validation cost 3.11666\n",
      "training accuracy 1  validation accuracy 0.5128\n",
      "Epoch 53, CIFAR-10 Batch 1:  training cost 0.0172508  validation cost 3.19302\n",
      "training accuracy 1  validation accuracy 0.51\n",
      "Epoch 54, CIFAR-10 Batch 1:  training cost 0.00996555  validation cost 3.35595\n",
      "training accuracy 1  validation accuracy 0.504\n",
      "Epoch 55, CIFAR-10 Batch 1:  training cost 0.0183975  validation cost 3.46494\n",
      "training accuracy 1  validation accuracy 0.517\n",
      "Epoch 56, CIFAR-10 Batch 1:  training cost 0.00232252  validation cost 3.31037\n",
      "training accuracy 1  validation accuracy 0.5184\n",
      "Epoch 57, CIFAR-10 Batch 1:  training cost 0.00136735  validation cost 3.38055\n",
      "training accuracy 1  validation accuracy 0.5026\n",
      "Epoch 58, CIFAR-10 Batch 1:  training cost 0.0120351  validation cost 3.33034\n",
      "training accuracy 1  validation accuracy 0.5174\n",
      "Epoch 59, CIFAR-10 Batch 1:  training cost 0.00247965  validation cost 3.32391\n",
      "training accuracy 1  validation accuracy 0.5228\n",
      "Epoch 60, CIFAR-10 Batch 1:  training cost 0.00101228  validation cost 3.48801\n",
      "training accuracy 1  validation accuracy 0.5092\n",
      "Epoch 61, CIFAR-10 Batch 1:  training cost 0.0011426  validation cost 3.4994\n",
      "training accuracy 1  validation accuracy 0.511\n",
      "Epoch 62, CIFAR-10 Batch 1:  training cost 0.00695903  validation cost 3.34011\n",
      "training accuracy 1  validation accuracy 0.5224\n",
      "Epoch 63, CIFAR-10 Batch 1:  training cost 0.00488073  validation cost 3.34285\n",
      "training accuracy 1  validation accuracy 0.5164\n",
      "Epoch 64, CIFAR-10 Batch 1:  training cost 0.00133037  validation cost 3.48817\n",
      "training accuracy 1  validation accuracy 0.5132\n",
      "Epoch 65, CIFAR-10 Batch 1:  training cost 0.00321241  validation cost 3.49753\n",
      "training accuracy 1  validation accuracy 0.5062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, CIFAR-10 Batch 1:  training cost 0.00158077  validation cost 3.49673\n",
      "training accuracy 1  validation accuracy 0.5228\n",
      "Epoch 67, CIFAR-10 Batch 1:  training cost 0.00713266  validation cost 3.59211\n",
      "training accuracy 1  validation accuracy 0.5176\n",
      "Epoch 68, CIFAR-10 Batch 1:  training cost 0.00359048  validation cost 3.63432\n",
      "training accuracy 1  validation accuracy 0.5212\n",
      "Epoch 69, CIFAR-10 Batch 1:  training cost 0.00297889  validation cost 3.57579\n",
      "training accuracy 1  validation accuracy 0.5168\n",
      "Epoch 70, CIFAR-10 Batch 1:  training cost 0.00167292  validation cost 3.61707\n",
      "training accuracy 1  validation accuracy 0.522\n",
      "Epoch 71, CIFAR-10 Batch 1:  training cost 0.00574331  validation cost 3.63778\n",
      "training accuracy 1  validation accuracy 0.5246\n",
      "Epoch 72, CIFAR-10 Batch 1:  training cost 0.00135225  validation cost 3.69978\n",
      "training accuracy 1  validation accuracy 0.5162\n",
      "Epoch 73, CIFAR-10 Batch 1:  training cost 0.00363931  validation cost 3.61009\n",
      "training accuracy 1  validation accuracy 0.519\n",
      "Epoch 74, CIFAR-10 Batch 1:  training cost 0.0095673  validation cost 3.73676\n",
      "training accuracy 1  validation accuracy 0.5178\n",
      "Epoch 75, CIFAR-10 Batch 1:  training cost 0.00390749  validation cost 3.78103\n",
      "training accuracy 1  validation accuracy 0.5216\n",
      "Epoch 76, CIFAR-10 Batch 1:  training cost 0.00143032  validation cost 3.6068\n",
      "training accuracy 1  validation accuracy 0.5236\n",
      "Epoch 77, CIFAR-10 Batch 1:  training cost 0.00711506  validation cost 3.65465\n",
      "training accuracy 1  validation accuracy 0.5138\n",
      "Epoch 78, CIFAR-10 Batch 1:  training cost 0.00313078  validation cost 3.73047\n",
      "training accuracy 1  validation accuracy 0.5236\n",
      "Epoch 79, CIFAR-10 Batch 1:  training cost 0.00356636  validation cost 3.81426\n",
      "training accuracy 1  validation accuracy 0.5158\n",
      "Epoch 80, CIFAR-10 Batch 1:  training cost 0.00114155  validation cost 3.90312\n",
      "training accuracy 1  validation accuracy 0.5134\n",
      "Epoch 81, CIFAR-10 Batch 1:  training cost 0.00146528  validation cost 3.74499\n",
      "training accuracy 1  validation accuracy 0.5112\n",
      "Epoch 82, CIFAR-10 Batch 1:  training cost 0.000537704  validation cost 3.6949\n",
      "training accuracy 1  validation accuracy 0.5148\n",
      "Epoch 83, CIFAR-10 Batch 1:  training cost 0.00177233  validation cost 4.00157\n",
      "training accuracy 1  validation accuracy 0.5212\n",
      "Epoch 84, CIFAR-10 Batch 1:  training cost 0.00198226  validation cost 3.78457\n",
      "training accuracy 1  validation accuracy 0.5188\n",
      "Epoch 85, CIFAR-10 Batch 1:  training cost 0.00591786  validation cost 3.7059\n",
      "training accuracy 1  validation accuracy 0.524\n",
      "Epoch 86, CIFAR-10 Batch 1:  training cost 0.00259081  validation cost 3.74039\n",
      "training accuracy 1  validation accuracy 0.5194\n",
      "Epoch 87, CIFAR-10 Batch 1:  training cost 0.00443405  validation cost 3.86343\n",
      "training accuracy 1  validation accuracy 0.518\n",
      "Epoch 88, CIFAR-10 Batch 1:  training cost 0.0011315  validation cost 3.78986\n",
      "training accuracy 1  validation accuracy 0.5204\n",
      "Epoch 89, CIFAR-10 Batch 1:  training cost 0.0206247  validation cost 3.82454\n",
      "training accuracy 1  validation accuracy 0.519\n",
      "Epoch 90, CIFAR-10 Batch 1:  training cost 0.00695792  validation cost 3.80706\n",
      "training accuracy 1  validation accuracy 0.5216\n",
      "Epoch 91, CIFAR-10 Batch 1:  training cost 0.00781175  validation cost 3.89487\n",
      "training accuracy 1  validation accuracy 0.5224\n",
      "Epoch 92, CIFAR-10 Batch 1:  training cost 0.00346529  validation cost 3.85007\n",
      "training accuracy 1  validation accuracy 0.5176\n",
      "Epoch 93, CIFAR-10 Batch 1:  training cost 0.00349341  validation cost 3.79433\n",
      "training accuracy 1  validation accuracy 0.5234\n",
      "Epoch 94, CIFAR-10 Batch 1:  training cost 0.00985823  validation cost 3.82053\n",
      "training accuracy 1  validation accuracy 0.524\n",
      "Epoch 95, CIFAR-10 Batch 1:  training cost 0.000278416  validation cost 3.81139\n",
      "training accuracy 1  validation accuracy 0.5272\n",
      "Epoch 96, CIFAR-10 Batch 1:  training cost 0.00559887  validation cost 3.7272\n",
      "training accuracy 1  validation accuracy 0.5222\n",
      "Epoch 97, CIFAR-10 Batch 1:  training cost 0.00125141  validation cost 3.94984\n",
      "training accuracy 1  validation accuracy 0.5226\n",
      "Epoch 98, CIFAR-10 Batch 1:  training cost 0.00334884  validation cost 3.87525\n",
      "training accuracy 1  validation accuracy 0.516\n",
      "Epoch 99, CIFAR-10 Batch 1:  training cost 0.000701423  validation cost 4.00932\n",
      "training accuracy 1  validation accuracy 0.5182\n",
      "Epoch 100, CIFAR-10 Batch 1:  training cost 0.00130955  validation cost 3.67655\n",
      "training accuracy 1  validation accuracy 0.5316\n",
      "Epoch 101, CIFAR-10 Batch 1:  training cost 0.000244783  validation cost 4.00435\n",
      "training accuracy 1  validation accuracy 0.5242\n",
      "Epoch 102, CIFAR-10 Batch 1:  training cost 0.000161647  validation cost 4.18712\n",
      "training accuracy 1  validation accuracy 0.5164\n",
      "Epoch 103, CIFAR-10 Batch 1:  training cost 0.00033394  validation cost 4.124\n",
      "training accuracy 1  validation accuracy 0.5214\n",
      "Epoch 104, CIFAR-10 Batch 1:  training cost 0.00588331  validation cost 4.1822\n",
      "training accuracy 1  validation accuracy 0.4972\n",
      "Epoch 105, CIFAR-10 Batch 1:  training cost 0.000377863  validation cost 3.75905\n",
      "training accuracy 1  validation accuracy 0.5284\n",
      "Epoch 106, CIFAR-10 Batch 1:  training cost 0.000391773  validation cost 3.86446\n",
      "training accuracy 1  validation accuracy 0.5256\n",
      "Epoch 107, CIFAR-10 Batch 1:  training cost 0.0037085  validation cost 3.81928\n",
      "training accuracy 1  validation accuracy 0.5264\n",
      "Epoch 108, CIFAR-10 Batch 1:  training cost 0.000165842  validation cost 3.998\n",
      "training accuracy 1  validation accuracy 0.5224\n",
      "Epoch 109, CIFAR-10 Batch 1:  training cost 0.00110617  validation cost 4.07266\n",
      "training accuracy 1  validation accuracy 0.5268\n",
      "Epoch 110, CIFAR-10 Batch 1:  training cost 0.000244453  validation cost 3.84314\n",
      "training accuracy 1  validation accuracy 0.5318\n",
      "Epoch 111, CIFAR-10 Batch 1:  training cost 0.00169808  validation cost 3.94923\n",
      "training accuracy 1  validation accuracy 0.52\n",
      "Epoch 112, CIFAR-10 Batch 1:  training cost 0.000283626  validation cost 4.10597\n",
      "training accuracy 1  validation accuracy 0.5232\n",
      "Epoch 113, CIFAR-10 Batch 1:  training cost 0.0012838  validation cost 4.10522\n",
      "training accuracy 1  validation accuracy 0.5068\n",
      "Epoch 114, CIFAR-10 Batch 1:  training cost 0.0062599  validation cost 3.96123\n",
      "training accuracy 1  validation accuracy 0.5164\n",
      "Epoch 115, CIFAR-10 Batch 1:  training cost 7.18169e-05  validation cost 3.92516\n",
      "training accuracy 1  validation accuracy 0.5264\n",
      "Epoch 116, CIFAR-10 Batch 1:  training cost 0.000499684  validation cost 4.22701\n",
      "training accuracy 1  validation accuracy 0.5254\n",
      "Epoch 117, CIFAR-10 Batch 1:  training cost 0.000420311  validation cost 3.97041\n",
      "training accuracy 1  validation accuracy 0.5254\n",
      "Epoch 118, CIFAR-10 Batch 1:  training cost 0.00737827  validation cost 3.82083\n",
      "training accuracy 1  validation accuracy 0.529\n",
      "Epoch 119, CIFAR-10 Batch 1:  training cost 0.00108686  validation cost 4.07628\n",
      "training accuracy 1  validation accuracy 0.515\n",
      "Epoch 120, CIFAR-10 Batch 1:  training cost 0.00156365  validation cost 4.11346\n",
      "training accuracy 1  validation accuracy 0.5292\n",
      "Epoch 121, CIFAR-10 Batch 1:  training cost 0.0034437  validation cost 3.86211\n",
      "training accuracy 1  validation accuracy 0.5354\n",
      "Epoch 122, CIFAR-10 Batch 1:  training cost 0.000297281  validation cost 4.21677\n",
      "training accuracy 1  validation accuracy 0.5222\n",
      "Epoch 123, CIFAR-10 Batch 1:  training cost 0.000674326  validation cost 4.25371\n",
      "training accuracy 1  validation accuracy 0.5094\n",
      "Epoch 124, CIFAR-10 Batch 1:  training cost 0.000231  validation cost 3.85605\n",
      "training accuracy 1  validation accuracy 0.519\n",
      "Epoch 125, CIFAR-10 Batch 1:  training cost 0.000183586  validation cost 3.74354\n",
      "training accuracy 1  validation accuracy 0.529\n",
      "Epoch 126, CIFAR-10 Batch 1:  training cost 0.00666095  validation cost 3.90219\n",
      "training accuracy 1  validation accuracy 0.5228\n",
      "Epoch 127, CIFAR-10 Batch 1:  training cost 0.00326435  validation cost 4.15403\n",
      "training accuracy 1  validation accuracy 0.5216\n",
      "Epoch 128, CIFAR-10 Batch 1:  training cost 0.000675483  validation cost 4.19231\n",
      "training accuracy 1  validation accuracy 0.5326\n",
      "Epoch 129, CIFAR-10 Batch 1:  training cost 0.0014177  validation cost 4.02833\n",
      "training accuracy 1  validation accuracy 0.5244\n",
      "Epoch 130, CIFAR-10 Batch 1:  training cost 0.000543645  validation cost 4.17233\n",
      "training accuracy 1  validation accuracy 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, CIFAR-10 Batch 1:  training cost 0.00153434  validation cost 4.27917\n",
      "training accuracy 1  validation accuracy 0.5138\n",
      "Epoch 132, CIFAR-10 Batch 1:  training cost 0.0114024  validation cost 4.04358\n",
      "training accuracy 1  validation accuracy 0.5176\n",
      "Epoch 133, CIFAR-10 Batch 1:  training cost 0.00315825  validation cost 3.87186\n",
      "training accuracy 1  validation accuracy 0.5264\n",
      "Epoch 134, CIFAR-10 Batch 1:  training cost 0.0014008  validation cost 4.08211\n",
      "training accuracy 1  validation accuracy 0.514\n",
      "Epoch 135, CIFAR-10 Batch 1:  training cost 0.000148095  validation cost 4.19551\n",
      "training accuracy 1  validation accuracy 0.5156\n",
      "Epoch 136, CIFAR-10 Batch 1:  training cost 0.000471226  validation cost 4.49586\n",
      "training accuracy 1  validation accuracy 0.5178\n",
      "Epoch 137, CIFAR-10 Batch 1:  training cost 0.00460613  validation cost 3.83569\n",
      "training accuracy 1  validation accuracy 0.5252\n",
      "Epoch 138, CIFAR-10 Batch 1:  training cost 0.00252607  validation cost 3.82983\n",
      "training accuracy 1  validation accuracy 0.5254\n",
      "Epoch 139, CIFAR-10 Batch 1:  training cost 0.0080101  validation cost 4.18779\n",
      "training accuracy 1  validation accuracy 0.5302\n",
      "Epoch 140, CIFAR-10 Batch 1:  training cost 0.000212959  validation cost 4.09769\n",
      "training accuracy 1  validation accuracy 0.5244\n",
      "Epoch 141, CIFAR-10 Batch 1:  training cost 0.00145494  validation cost 3.94091\n",
      "training accuracy 1  validation accuracy 0.5242\n",
      "Epoch 142, CIFAR-10 Batch 1:  training cost 0.0010598  validation cost 3.9732\n",
      "training accuracy 1  validation accuracy 0.5346\n",
      "Epoch 143, CIFAR-10 Batch 1:  training cost 0.000179042  validation cost 4.1226\n",
      "training accuracy 1  validation accuracy 0.5346\n",
      "Epoch 144, CIFAR-10 Batch 1:  training cost 0.00024554  validation cost 4.09664\n",
      "training accuracy 1  validation accuracy 0.5288\n",
      "Epoch 145, CIFAR-10 Batch 1:  training cost 0.00161486  validation cost 4.31801\n",
      "training accuracy 1  validation accuracy 0.5252\n",
      "Epoch 146, CIFAR-10 Batch 1:  training cost 0.00432771  validation cost 3.95234\n",
      "training accuracy 1  validation accuracy 0.5184\n",
      "Epoch 147, CIFAR-10 Batch 1:  training cost 0.00555113  validation cost 4.04213\n",
      "training accuracy 1  validation accuracy 0.5322\n",
      "Epoch 148, CIFAR-10 Batch 1:  training cost 0.00112581  validation cost 4.20005\n",
      "training accuracy 1  validation accuracy 0.5274\n",
      "Epoch 149, CIFAR-10 Batch 1:  training cost 0.00180329  validation cost 3.94975\n",
      "training accuracy 1  validation accuracy 0.5238\n",
      "Epoch 150, CIFAR-10 Batch 1:  training cost 0.000840038  validation cost 4.06201\n",
      "training accuracy 1  validation accuracy 0.517\n",
      "Epoch 151, CIFAR-10 Batch 1:  training cost 0.00316072  validation cost 4.24365\n",
      "training accuracy 1  validation accuracy 0.5276\n",
      "Epoch 152, CIFAR-10 Batch 1:  training cost 0.00121741  validation cost 4.30423\n",
      "training accuracy 1  validation accuracy 0.5254\n",
      "Epoch 153, CIFAR-10 Batch 1:  training cost 0.00579259  validation cost 4.19133\n",
      "training accuracy 1  validation accuracy 0.5204\n",
      "Epoch 154, CIFAR-10 Batch 1:  training cost 0.00378028  validation cost 4.19273\n",
      "training accuracy 1  validation accuracy 0.5204\n",
      "Epoch 155, CIFAR-10 Batch 1:  training cost 0.0184875  validation cost 4.27252\n",
      "training accuracy 1  validation accuracy 0.5176\n",
      "Epoch 156, CIFAR-10 Batch 1:  training cost 7.20827e-05  validation cost 4.07486\n",
      "training accuracy 1  validation accuracy 0.5288\n",
      "Epoch 157, CIFAR-10 Batch 1:  training cost 2.02796e-05  validation cost 4.05941\n",
      "training accuracy 1  validation accuracy 0.5304\n",
      "Epoch 158, CIFAR-10 Batch 1:  training cost 0.00056741  validation cost 3.6988\n",
      "training accuracy 1  validation accuracy 0.5328\n",
      "Epoch 159, CIFAR-10 Batch 1:  training cost 0.00281954  validation cost 4.08405\n",
      "training accuracy 1  validation accuracy 0.5284\n",
      "Epoch 160, CIFAR-10 Batch 1:  training cost 6.70539e-06  validation cost 4.09351\n",
      "training accuracy 1  validation accuracy 0.5226\n",
      "Epoch 161, CIFAR-10 Batch 1:  training cost 0.000641795  validation cost 3.93481\n",
      "training accuracy 1  validation accuracy 0.527\n",
      "Epoch 162, CIFAR-10 Batch 1:  training cost 0.0033696  validation cost 3.89484\n",
      "training accuracy 1  validation accuracy 0.5256\n",
      "Epoch 163, CIFAR-10 Batch 1:  training cost 5.20996e-05  validation cost 4.04425\n",
      "training accuracy 1  validation accuracy 0.5366\n",
      "Epoch 164, CIFAR-10 Batch 1:  training cost 0.000122308  validation cost 3.75785\n",
      "training accuracy 1  validation accuracy 0.5264\n",
      "Epoch 165, CIFAR-10 Batch 1:  training cost 3.12757e-05  validation cost 3.75451\n",
      "training accuracy 1  validation accuracy 0.5174\n",
      "Epoch 166, CIFAR-10 Batch 1:  training cost 0.000290551  validation cost 3.98389\n",
      "training accuracy 1  validation accuracy 0.5248\n",
      "Epoch 167, CIFAR-10 Batch 1:  training cost 0.000612253  validation cost 4.07278\n",
      "training accuracy 1  validation accuracy 0.5258\n",
      "Epoch 168, CIFAR-10 Batch 1:  training cost 9.00642e-05  validation cost 4.29079\n",
      "training accuracy 1  validation accuracy 0.5338\n",
      "Epoch 169, CIFAR-10 Batch 1:  training cost 0.00031832  validation cost 4.38924\n",
      "training accuracy 1  validation accuracy 0.5288\n",
      "Epoch 170, CIFAR-10 Batch 1:  training cost 0.00100062  validation cost 4.12191\n",
      "training accuracy 1  validation accuracy 0.5138\n",
      "Epoch 171, CIFAR-10 Batch 1:  training cost 0.000138586  validation cost 3.75471\n",
      "training accuracy 1  validation accuracy 0.5322\n",
      "Epoch 172, CIFAR-10 Batch 1:  training cost 0.000430617  validation cost 3.9962\n",
      "training accuracy 1  validation accuracy 0.528\n",
      "Epoch 173, CIFAR-10 Batch 1:  training cost 0.000267885  validation cost 3.97768\n",
      "training accuracy 1  validation accuracy 0.5336\n",
      "Epoch 174, CIFAR-10 Batch 1:  training cost 2.66718e-05  validation cost 4.19645\n",
      "training accuracy 1  validation accuracy 0.5298\n",
      "Epoch 175, CIFAR-10 Batch 1:  training cost 3.52988e-05  validation cost 4.26824\n",
      "training accuracy 1  validation accuracy 0.525\n",
      "Epoch 176, CIFAR-10 Batch 1:  training cost 0.00264473  validation cost 4.46674\n",
      "training accuracy 1  validation accuracy 0.5296\n",
      "Epoch 177, CIFAR-10 Batch 1:  training cost 3.76234e-05  validation cost 3.68871\n",
      "training accuracy 1  validation accuracy 0.522\n",
      "Epoch 178, CIFAR-10 Batch 1:  training cost 0.000633227  validation cost 3.92769\n",
      "training accuracy 1  validation accuracy 0.5272\n",
      "Epoch 179, CIFAR-10 Batch 1:  training cost 0.000674893  validation cost 4.1973\n",
      "training accuracy 1  validation accuracy 0.5298\n",
      "Epoch 180, CIFAR-10 Batch 1:  training cost 3.97839e-05  validation cost 4.43381\n",
      "training accuracy 1  validation accuracy 0.529\n",
      "Epoch 181, CIFAR-10 Batch 1:  training cost 0.000151803  validation cost 4.24136\n",
      "training accuracy 1  validation accuracy 0.524\n",
      "Epoch 182, CIFAR-10 Batch 1:  training cost 1.76425e-05  validation cost 4.2119\n",
      "training accuracy 1  validation accuracy 0.5272\n",
      "Epoch 183, CIFAR-10 Batch 1:  training cost 0.00163033  validation cost 3.98036\n",
      "training accuracy 1  validation accuracy 0.5292\n",
      "Epoch 184, CIFAR-10 Batch 1:  training cost 0.000102378  validation cost 4.3525\n",
      "training accuracy 1  validation accuracy 0.522\n",
      "Epoch 185, CIFAR-10 Batch 1:  training cost 0.000782172  validation cost 3.88602\n",
      "training accuracy 1  validation accuracy 0.5316\n",
      "Epoch 186, CIFAR-10 Batch 1:  training cost 5.6624e-06  validation cost 4.16109\n",
      "training accuracy 1  validation accuracy 0.5286\n",
      "Epoch 187, CIFAR-10 Batch 1:  training cost 2.60606e-05  validation cost 4.3627\n",
      "training accuracy 1  validation accuracy 0.5308\n",
      "Epoch 188, CIFAR-10 Batch 1:  training cost 0.000876698  validation cost 4.14069\n",
      "training accuracy 1  validation accuracy 0.5316\n",
      "Epoch 189, CIFAR-10 Batch 1:  training cost 0.000324755  validation cost 4.09965\n",
      "training accuracy 1  validation accuracy 0.5276\n",
      "Epoch 190, CIFAR-10 Batch 1:  training cost 0.000600217  validation cost 3.93985\n",
      "training accuracy 1  validation accuracy 0.5354\n",
      "Epoch 191, CIFAR-10 Batch 1:  training cost 9.08005e-05  validation cost 4.18323\n",
      "training accuracy 1  validation accuracy 0.5376\n",
      "Epoch 192, CIFAR-10 Batch 1:  training cost 0.000855567  validation cost 4.57375\n",
      "training accuracy 1  validation accuracy 0.528\n",
      "Epoch 193, CIFAR-10 Batch 1:  training cost 0.00117188  validation cost 4.5785\n",
      "training accuracy 1  validation accuracy 0.5306\n",
      "Epoch 194, CIFAR-10 Batch 1:  training cost 0.00239164  validation cost 3.87573\n",
      "training accuracy 1  validation accuracy 0.5296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, CIFAR-10 Batch 1:  training cost 4.53234e-05  validation cost 4.0272\n",
      "training accuracy 1  validation accuracy 0.5296\n",
      "Epoch 196, CIFAR-10 Batch 1:  training cost 0.00113992  validation cost 4.10783\n",
      "training accuracy 1  validation accuracy 0.5216\n",
      "Epoch 197, CIFAR-10 Batch 1:  training cost 6.36736e-05  validation cost 4.19874\n",
      "training accuracy 1  validation accuracy 0.5258\n",
      "Epoch 198, CIFAR-10 Batch 1:  training cost 5.94552e-06  validation cost 4.22721\n",
      "training accuracy 1  validation accuracy 0.5278\n",
      "Epoch 199, CIFAR-10 Batch 1:  training cost 0.000271392  validation cost 3.98581\n",
      "training accuracy 1  validation accuracy 0.5312\n",
      "Epoch 200, CIFAR-10 Batch 1:  training cost 0.000570088  validation cost 4.22227\n",
      "training accuracy 1  validation accuracy 0.5316\n",
      "Epoch 201, CIFAR-10 Batch 1:  training cost 7.37558e-05  validation cost 4.32915\n",
      "training accuracy 1  validation accuracy 0.5402\n",
      "Epoch 202, CIFAR-10 Batch 1:  training cost 1.30977e-05  validation cost 4.27613\n",
      "training accuracy 1  validation accuracy 0.5416\n",
      "Epoch 203, CIFAR-10 Batch 1:  training cost 5.52308e-05  validation cost 4.34148\n",
      "training accuracy 1  validation accuracy 0.5412\n",
      "Epoch 204, CIFAR-10 Batch 1:  training cost 0.00239851  validation cost 4.03841\n",
      "training accuracy 1  validation accuracy 0.526\n",
      "Epoch 205, CIFAR-10 Batch 1:  training cost 0.000237779  validation cost 3.96619\n",
      "training accuracy 1  validation accuracy 0.5258\n",
      "Epoch 206, CIFAR-10 Batch 1:  training cost 0.00240284  validation cost 4.18489\n",
      "training accuracy 1  validation accuracy 0.5434\n",
      "Epoch 207, CIFAR-10 Batch 1:  training cost 1.50795e-05  validation cost 4.28807\n",
      "training accuracy 1  validation accuracy 0.532\n",
      "Epoch 208, CIFAR-10 Batch 1:  training cost 0.00243531  validation cost 4.03995\n",
      "training accuracy 1  validation accuracy 0.5262\n",
      "Epoch 209, CIFAR-10 Batch 1:  training cost 0.000324296  validation cost 4.15388\n",
      "training accuracy 1  validation accuracy 0.5364\n",
      "Epoch 210, CIFAR-10 Batch 1:  training cost 0.0031777  validation cost 3.81807\n",
      "training accuracy 1  validation accuracy 0.5304\n",
      "Epoch 211, CIFAR-10 Batch 1:  training cost 0.000827641  validation cost 4.12338\n",
      "training accuracy 1  validation accuracy 0.532\n",
      "Epoch 212, CIFAR-10 Batch 1:  training cost 1.72248e-05  validation cost 4.0766\n",
      "training accuracy 1  validation accuracy 0.5394\n",
      "Epoch 213, CIFAR-10 Batch 1:  training cost 0.000467841  validation cost 4.17888\n",
      "training accuracy 1  validation accuracy 0.5228\n",
      "Epoch 214, CIFAR-10 Batch 1:  training cost 0.000280667  validation cost 4.21606\n",
      "training accuracy 1  validation accuracy 0.523\n",
      "Epoch 215, CIFAR-10 Batch 1:  training cost 0.000467276  validation cost 3.95515\n",
      "training accuracy 1  validation accuracy 0.5234\n",
      "Epoch 216, CIFAR-10 Batch 1:  training cost 2.05924e-05  validation cost 4.14745\n",
      "training accuracy 1  validation accuracy 0.5354\n",
      "Epoch 217, CIFAR-10 Batch 1:  training cost 0.000555941  validation cost 4.34428\n",
      "training accuracy 1  validation accuracy 0.5092\n",
      "Epoch 218, CIFAR-10 Batch 1:  training cost 0.00835092  validation cost 4.09695\n",
      "training accuracy 1  validation accuracy 0.5272\n",
      "Epoch 219, CIFAR-10 Batch 1:  training cost 0.00086675  validation cost 4.3267\n",
      "training accuracy 1  validation accuracy 0.5246\n",
      "Epoch 220, CIFAR-10 Batch 1:  training cost 0.000718302  validation cost 4.20659\n",
      "training accuracy 1  validation accuracy 0.5366\n",
      "Epoch 221, CIFAR-10 Batch 1:  training cost 0.00157579  validation cost 4.12074\n",
      "training accuracy 1  validation accuracy 0.5242\n",
      "Epoch 222, CIFAR-10 Batch 1:  training cost 0.00214785  validation cost 3.90059\n",
      "training accuracy 1  validation accuracy 0.5298\n",
      "Epoch 223, CIFAR-10 Batch 1:  training cost 0.000344253  validation cost 4.07431\n",
      "training accuracy 1  validation accuracy 0.53\n",
      "Epoch 224, CIFAR-10 Batch 1:  training cost 0.000147821  validation cost 4.15704\n",
      "training accuracy 1  validation accuracy 0.5372\n",
      "Epoch 225, CIFAR-10 Batch 1:  training cost 0.00599516  validation cost 4.20493\n",
      "training accuracy 1  validation accuracy 0.5332\n",
      "Epoch 226, CIFAR-10 Batch 1:  training cost 0.010448  validation cost 4.23394\n",
      "training accuracy 1  validation accuracy 0.5338\n",
      "Epoch 227, CIFAR-10 Batch 1:  training cost 0.000648264  validation cost 4.1764\n",
      "training accuracy 1  validation accuracy 0.5226\n",
      "Epoch 228, CIFAR-10 Batch 1:  training cost 0.000106871  validation cost 4.45908\n",
      "training accuracy 1  validation accuracy 0.5358\n",
      "Epoch 229, CIFAR-10 Batch 1:  training cost 6.83932e-05  validation cost 4.60056\n",
      "training accuracy 1  validation accuracy 0.5218\n",
      "Epoch 230, CIFAR-10 Batch 1:  training cost 0.000366335  validation cost 4.14217\n",
      "training accuracy 1  validation accuracy 0.5258\n",
      "Epoch 231, CIFAR-10 Batch 1:  training cost 0.00347931  validation cost 3.93133\n",
      "training accuracy 1  validation accuracy 0.5274\n",
      "Epoch 232, CIFAR-10 Batch 1:  training cost 0.00142623  validation cost 4.04621\n",
      "training accuracy 1  validation accuracy 0.5306\n",
      "Epoch 233, CIFAR-10 Batch 1:  training cost 0.000293807  validation cost 4.1195\n",
      "training accuracy 1  validation accuracy 0.5424\n",
      "Epoch 234, CIFAR-10 Batch 1:  training cost 0.00546098  validation cost 4.15708\n",
      "training accuracy 1  validation accuracy 0.5484\n",
      "Epoch 235, CIFAR-10 Batch 1:  training cost 0.0003117  validation cost 4.23564\n",
      "training accuracy 1  validation accuracy 0.535\n",
      "Epoch 236, CIFAR-10 Batch 1:  training cost 0.000145671  validation cost 4.02785\n",
      "training accuracy 1  validation accuracy 0.5338\n",
      "Epoch 237, CIFAR-10 Batch 1:  training cost 0.00230299  validation cost 4.092\n",
      "training accuracy 1  validation accuracy 0.5424\n",
      "Epoch 238, CIFAR-10 Batch 1:  training cost 0.00147708  validation cost 4.53287\n",
      "training accuracy 1  validation accuracy 0.5138\n",
      "Epoch 239, CIFAR-10 Batch 1:  training cost 0.000149065  validation cost 4.05308\n",
      "training accuracy 1  validation accuracy 0.5218\n",
      "Epoch 240, CIFAR-10 Batch 1:  training cost 0.00238317  validation cost 3.66218\n",
      "training accuracy 1  validation accuracy 0.5298\n",
      "Epoch 241, CIFAR-10 Batch 1:  training cost 0.000116728  validation cost 4.0797\n",
      "training accuracy 1  validation accuracy 0.538\n",
      "Epoch 242, CIFAR-10 Batch 1:  training cost 6.62928e-05  validation cost 4.25127\n",
      "training accuracy 1  validation accuracy 0.536\n",
      "Epoch 243, CIFAR-10 Batch 1:  training cost 8.95545e-06  validation cost 4.2496\n",
      "training accuracy 1  validation accuracy 0.5466\n",
      "Epoch 244, CIFAR-10 Batch 1:  training cost 0.000620569  validation cost 3.83464\n",
      "training accuracy 1  validation accuracy 0.5278\n",
      "Epoch 245, CIFAR-10 Batch 1:  training cost 0.00367975  validation cost 4.06558\n",
      "training accuracy 1  validation accuracy 0.5266\n",
      "Epoch 246, CIFAR-10 Batch 1:  training cost 0.00213402  validation cost 4.05688\n",
      "training accuracy 1  validation accuracy 0.5268\n",
      "Epoch 247, CIFAR-10 Batch 1:  training cost 4.43418e-05  validation cost 3.89548\n",
      "training accuracy 1  validation accuracy 0.539\n",
      "Epoch 248, CIFAR-10 Batch 1:  training cost 9.08558e-05  validation cost 4.32703\n",
      "training accuracy 1  validation accuracy 0.5244\n",
      "Epoch 249, CIFAR-10 Batch 1:  training cost 0.000825788  validation cost 4.20092\n",
      "training accuracy 1  validation accuracy 0.5306\n",
      "Epoch 250, CIFAR-10 Batch 1:  training cost 0.000100491  validation cost 4.30203\n",
      "training accuracy 1  validation accuracy 0.5274\n",
      "Epoch 251, CIFAR-10 Batch 1:  training cost 8.57763e-05  validation cost 4.25547\n",
      "training accuracy 1  validation accuracy 0.5282\n",
      "Epoch 252, CIFAR-10 Batch 1:  training cost 0.000657511  validation cost 3.81664\n",
      "training accuracy 1  validation accuracy 0.515\n",
      "Epoch 253, CIFAR-10 Batch 1:  training cost 0.000102663  validation cost 4.07724\n",
      "training accuracy 1  validation accuracy 0.5366\n",
      "Epoch 254, CIFAR-10 Batch 1:  training cost 5.40513e-05  validation cost 4.16727\n",
      "training accuracy 1  validation accuracy 0.5308\n",
      "Epoch 255, CIFAR-10 Batch 1:  training cost 0.000267564  validation cost 4.11278\n",
      "training accuracy 1  validation accuracy 0.529\n",
      "Epoch 256, CIFAR-10 Batch 1:  training cost 0.000384573  validation cost 4.2999\n",
      "training accuracy 1  validation accuracy 0.5352\n",
      "Epoch 257, CIFAR-10 Batch 1:  training cost 5.4215e-05  validation cost 3.95052\n",
      "training accuracy 1  validation accuracy 0.5356\n",
      "Epoch 258, CIFAR-10 Batch 1:  training cost 0.000233807  validation cost 3.99174\n",
      "training accuracy 1  validation accuracy 0.5128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259, CIFAR-10 Batch 1:  training cost 0.00017802  validation cost 4.01571\n",
      "training accuracy 1  validation accuracy 0.5302\n",
      "Epoch 260, CIFAR-10 Batch 1:  training cost 0.000100831  validation cost 3.96682\n",
      "training accuracy 1  validation accuracy 0.5438\n",
      "Epoch 261, CIFAR-10 Batch 1:  training cost 0.000942728  validation cost 4.19542\n",
      "training accuracy 1  validation accuracy 0.542\n",
      "Epoch 262, CIFAR-10 Batch 1:  training cost 0.00467578  validation cost 4.00859\n",
      "training accuracy 1  validation accuracy 0.5356\n",
      "Epoch 263, CIFAR-10 Batch 1:  training cost 0.000208866  validation cost 3.8659\n",
      "training accuracy 1  validation accuracy 0.5378\n",
      "Epoch 264, CIFAR-10 Batch 1:  training cost 0.000590871  validation cost 4.20946\n",
      "training accuracy 1  validation accuracy 0.535\n",
      "Epoch 265, CIFAR-10 Batch 1:  training cost 0.000547879  validation cost 4.06401\n",
      "training accuracy 1  validation accuracy 0.5238\n",
      "Epoch 266, CIFAR-10 Batch 1:  training cost 0.00212187  validation cost 4.01041\n",
      "training accuracy 1  validation accuracy 0.5272\n",
      "Epoch 267, CIFAR-10 Batch 1:  training cost 0.0001657  validation cost 3.9323\n",
      "training accuracy 1  validation accuracy 0.5378\n",
      "Epoch 268, CIFAR-10 Batch 1:  training cost 0.00204933  validation cost 4.22249\n",
      "training accuracy 1  validation accuracy 0.53\n",
      "Epoch 269, CIFAR-10 Batch 1:  training cost 0.00179474  validation cost 4.48898\n",
      "training accuracy 1  validation accuracy 0.521\n",
      "Epoch 270, CIFAR-10 Batch 1:  training cost 0.00411098  validation cost 3.8787\n",
      "training accuracy 1  validation accuracy 0.5212\n",
      "Epoch 271, CIFAR-10 Batch 1:  training cost 0.000407604  validation cost 3.77682\n",
      "training accuracy 1  validation accuracy 0.5226\n",
      "Epoch 272, CIFAR-10 Batch 1:  training cost 0.000984034  validation cost 3.97216\n",
      "training accuracy 1  validation accuracy 0.5342\n",
      "Epoch 273, CIFAR-10 Batch 1:  training cost 0.000280685  validation cost 4.11651\n",
      "training accuracy 1  validation accuracy 0.5472\n",
      "Epoch 274, CIFAR-10 Batch 1:  training cost 0.000182561  validation cost 4.1948\n",
      "training accuracy 1  validation accuracy 0.5496\n",
      "Epoch 275, CIFAR-10 Batch 1:  training cost 0.000253292  validation cost 4.2826\n",
      "training accuracy 1  validation accuracy 0.5444\n",
      "Epoch 276, CIFAR-10 Batch 1:  training cost 0.000189789  validation cost 4.35498\n",
      "training accuracy 1  validation accuracy 0.5448\n",
      "Epoch 277, CIFAR-10 Batch 1:  training cost 0.000150383  validation cost 4.4253\n",
      "training accuracy 1  validation accuracy 0.545\n",
      "Epoch 278, CIFAR-10 Batch 1:  training cost 0.000117367  validation cost 4.4937\n",
      "training accuracy 1  validation accuracy 0.5454\n",
      "Epoch 279, CIFAR-10 Batch 1:  training cost 8.8914e-05  validation cost 4.56079\n",
      "training accuracy 1  validation accuracy 0.545\n",
      "Epoch 280, CIFAR-10 Batch 1:  training cost 6.671e-05  validation cost 4.62725\n",
      "training accuracy 1  validation accuracy 0.5456\n",
      "Epoch 281, CIFAR-10 Batch 1:  training cost 5.12795e-05  validation cost 4.69265\n",
      "training accuracy 1  validation accuracy 0.547\n",
      "Epoch 282, CIFAR-10 Batch 1:  training cost 4.16568e-05  validation cost 4.75707\n",
      "training accuracy 1  validation accuracy 0.5474\n",
      "Epoch 283, CIFAR-10 Batch 1:  training cost 3.40595e-05  validation cost 4.82094\n",
      "training accuracy 1  validation accuracy 0.5474\n",
      "Epoch 284, CIFAR-10 Batch 1:  training cost 2.86517e-05  validation cost 4.88458\n",
      "training accuracy 1  validation accuracy 0.548\n",
      "Epoch 285, CIFAR-10 Batch 1:  training cost 2.38098e-05  validation cost 4.94812\n",
      "training accuracy 1  validation accuracy 0.548\n",
      "Epoch 286, CIFAR-10 Batch 1:  training cost 1.99958e-05  validation cost 5.01166\n",
      "training accuracy 1  validation accuracy 0.548\n",
      "Epoch 287, CIFAR-10 Batch 1:  training cost 1.63455e-05  validation cost 5.075\n",
      "training accuracy 1  validation accuracy 0.5482\n",
      "Epoch 288, CIFAR-10 Batch 1:  training cost 1.27846e-05  validation cost 5.13837\n",
      "training accuracy 1  validation accuracy 0.5482\n",
      "Epoch 289, CIFAR-10 Batch 1:  training cost 1.04155e-05  validation cost 5.20159\n",
      "training accuracy 1  validation accuracy 0.5482\n",
      "Epoch 290, CIFAR-10 Batch 1:  training cost 8.21028e-06  validation cost 5.26496\n",
      "training accuracy 1  validation accuracy 0.5478\n",
      "Epoch 291, CIFAR-10 Batch 1:  training cost 6.76495e-06  validation cost 5.32867\n",
      "training accuracy 1  validation accuracy 0.5474\n",
      "Epoch 292, CIFAR-10 Batch 1:  training cost 5.2898e-06  validation cost 5.39201\n",
      "training accuracy 1  validation accuracy 0.5478\n",
      "Epoch 293, CIFAR-10 Batch 1:  training cost 4.20206e-06  validation cost 5.45575\n",
      "training accuracy 1  validation accuracy 0.5478\n",
      "Epoch 294, CIFAR-10 Batch 1:  training cost 3.33782e-06  validation cost 5.51945\n",
      "training accuracy 1  validation accuracy 0.5492\n",
      "Epoch 295, CIFAR-10 Batch 1:  training cost 2.74178e-06  validation cost 5.58319\n",
      "training accuracy 1  validation accuracy 0.5492\n",
      "Epoch 296, CIFAR-10 Batch 1:  training cost 2.14575e-06  validation cost 5.64691\n",
      "training accuracy 1  validation accuracy 0.5496\n",
      "Epoch 297, CIFAR-10 Batch 1:  training cost 1.74342e-06  validation cost 5.71079\n",
      "training accuracy 1  validation accuracy 0.5498\n",
      "Epoch 298, CIFAR-10 Batch 1:  training cost 1.4305e-06  validation cost 5.77461\n",
      "training accuracy 1  validation accuracy 0.5506\n",
      "Epoch 299, CIFAR-10 Batch 1:  training cost 1.11758e-06  validation cost 5.83809\n",
      "training accuracy 1  validation accuracy 0.5496\n",
      "Epoch 300, CIFAR-10 Batch 1:  training cost 9.23869e-07  validation cost 5.9019\n",
      "training accuracy 1  validation accuracy 0.5494\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  training cost 2.12768  validation cost 1.94043\n",
      "training accuracy 0.25  validation accuracy 0.2878\n",
      "Epoch  1, CIFAR-10 Batch 2:  training cost 1.64904  validation cost 1.79383\n",
      "training accuracy 0.375  validation accuracy 0.3542\n",
      "Epoch  1, CIFAR-10 Batch 3:  training cost 1.06592  validation cost 1.62588\n",
      "training accuracy 0.625  validation accuracy 0.4104\n",
      "Epoch  1, CIFAR-10 Batch 4:  training cost 1.71672  validation cost 1.58838\n",
      "training accuracy 0.25  validation accuracy 0.4216\n",
      "Epoch  1, CIFAR-10 Batch 5:  training cost 1.59946  validation cost 1.60037\n",
      "training accuracy 0.375  validation accuracy 0.4434\n",
      "Epoch  2, CIFAR-10 Batch 1:  training cost 1.40658  validation cost 1.38915\n",
      "training accuracy 0.5  validation accuracy 0.4952\n",
      "Epoch  2, CIFAR-10 Batch 2:  training cost 1.17471  validation cost 1.33933\n",
      "training accuracy 0.5  validation accuracy 0.5072\n",
      "Epoch  2, CIFAR-10 Batch 3:  training cost 0.985149  validation cost 1.29188\n",
      "training accuracy 0.625  validation accuracy 0.5232\n",
      "Epoch  2, CIFAR-10 Batch 4:  training cost 1.56313  validation cost 1.28555\n",
      "training accuracy 0.5  validation accuracy 0.5318\n",
      "Epoch  2, CIFAR-10 Batch 5:  training cost 1.68009  validation cost 1.28031\n",
      "training accuracy 0.375  validation accuracy 0.5426\n",
      "Epoch  3, CIFAR-10 Batch 1:  training cost 1.24446  validation cost 1.22969\n",
      "training accuracy 0.5  validation accuracy 0.5444\n",
      "Epoch  3, CIFAR-10 Batch 2:  training cost 0.998304  validation cost 1.29792\n",
      "training accuracy 0.5  validation accuracy 0.5218\n",
      "Epoch  3, CIFAR-10 Batch 3:  training cost 0.748136  validation cost 1.20116\n",
      "training accuracy 0.75  validation accuracy 0.562\n",
      "Epoch  3, CIFAR-10 Batch 4:  training cost 1.00521  validation cost 1.23468\n",
      "training accuracy 0.625  validation accuracy 0.5594\n",
      "Epoch  3, CIFAR-10 Batch 5:  training cost 1.43861  validation cost 1.19144\n",
      "training accuracy 0.375  validation accuracy 0.5694\n",
      "Epoch  4, CIFAR-10 Batch 1:  training cost 1.13431  validation cost 1.1536\n",
      "training accuracy 0.75  validation accuracy 0.5796\n",
      "Epoch  4, CIFAR-10 Batch 2:  training cost 0.804431  validation cost 1.22608\n",
      "training accuracy 0.625  validation accuracy 0.5528\n",
      "Epoch  4, CIFAR-10 Batch 3:  training cost 0.649461  validation cost 1.1485\n",
      "training accuracy 0.875  validation accuracy 0.5854\n",
      "Epoch  4, CIFAR-10 Batch 4:  training cost 0.816281  validation cost 1.15453\n",
      "training accuracy 0.75  validation accuracy 0.5886\n",
      "Epoch  4, CIFAR-10 Batch 5:  training cost 1.25449  validation cost 1.14545\n",
      "training accuracy 0.75  validation accuracy 0.5852\n",
      "Epoch  5, CIFAR-10 Batch 1:  training cost 1.0288  validation cost 1.1118\n",
      "training accuracy 0.75  validation accuracy 0.5974\n",
      "Epoch  5, CIFAR-10 Batch 2:  training cost 0.681023  validation cost 1.15976\n",
      "training accuracy 0.75  validation accuracy 0.5838\n",
      "Epoch  5, CIFAR-10 Batch 3:  training cost 0.623161  validation cost 1.10829\n",
      "training accuracy 0.75  validation accuracy 0.6006\n",
      "Epoch  5, CIFAR-10 Batch 4:  training cost 0.619146  validation cost 1.12135\n",
      "training accuracy 0.875  validation accuracy 0.606\n",
      "Epoch  5, CIFAR-10 Batch 5:  training cost 0.977288  validation cost 1.11822\n",
      "training accuracy 0.75  validation accuracy 0.5988\n",
      "Epoch  6, CIFAR-10 Batch 1:  training cost 0.931097  validation cost 1.112\n",
      "training accuracy 0.75  validation accuracy 0.6014\n",
      "Epoch  6, CIFAR-10 Batch 2:  training cost 0.560892  validation cost 1.11638\n",
      "training accuracy 0.875  validation accuracy 0.6022\n",
      "Epoch  6, CIFAR-10 Batch 3:  training cost 0.554813  validation cost 1.0946\n",
      "training accuracy 0.75  validation accuracy 0.6122\n",
      "Epoch  6, CIFAR-10 Batch 4:  training cost 0.511417  validation cost 1.07728\n",
      "training accuracy 0.875  validation accuracy 0.6202\n",
      "Epoch  6, CIFAR-10 Batch 5:  training cost 0.896139  validation cost 1.07907\n",
      "training accuracy 0.75  validation accuracy 0.6196\n",
      "Epoch  7, CIFAR-10 Batch 1:  training cost 0.84869  validation cost 1.1208\n",
      "training accuracy 0.75  validation accuracy 0.5996\n",
      "Epoch  7, CIFAR-10 Batch 2:  training cost 0.486766  validation cost 1.09369\n",
      "training accuracy 0.875  validation accuracy 0.611\n",
      "Epoch  7, CIFAR-10 Batch 3:  training cost 0.463162  validation cost 1.0653\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch  7, CIFAR-10 Batch 4:  training cost 0.488726  validation cost 1.06179\n",
      "training accuracy 0.875  validation accuracy 0.6312\n",
      "Epoch  7, CIFAR-10 Batch 5:  training cost 0.780836  validation cost 1.09245\n",
      "training accuracy 0.625  validation accuracy 0.6226\n",
      "Epoch  8, CIFAR-10 Batch 1:  training cost 0.695022  validation cost 1.10405\n",
      "training accuracy 0.75  validation accuracy 0.6132\n",
      "Epoch  8, CIFAR-10 Batch 2:  training cost 0.472398  validation cost 1.09944\n",
      "training accuracy 0.75  validation accuracy 0.6112\n",
      "Epoch  8, CIFAR-10 Batch 3:  training cost 0.458919  validation cost 1.08273\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch  8, CIFAR-10 Batch 4:  training cost 0.4231  validation cost 1.05027\n",
      "training accuracy 1  validation accuracy 0.637\n",
      "Epoch  8, CIFAR-10 Batch 5:  training cost 0.69442  validation cost 1.08406\n",
      "training accuracy 0.75  validation accuracy 0.6238\n",
      "Epoch  9, CIFAR-10 Batch 1:  training cost 0.62814  validation cost 1.08359\n",
      "training accuracy 0.75  validation accuracy 0.625\n",
      "Epoch  9, CIFAR-10 Batch 2:  training cost 0.407672  validation cost 1.10111\n",
      "training accuracy 0.875  validation accuracy 0.6166\n",
      "Epoch  9, CIFAR-10 Batch 3:  training cost 0.38184  validation cost 1.09307\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch  9, CIFAR-10 Batch 4:  training cost 0.353956  validation cost 1.07275\n",
      "training accuracy 1  validation accuracy 0.6406\n",
      "Epoch  9, CIFAR-10 Batch 5:  training cost 0.613608  validation cost 1.09147\n",
      "training accuracy 0.75  validation accuracy 0.6354\n",
      "Epoch 10, CIFAR-10 Batch 1:  training cost 0.572246  validation cost 1.08915\n",
      "training accuracy 0.75  validation accuracy 0.629\n",
      "Epoch 10, CIFAR-10 Batch 2:  training cost 0.255947  validation cost 1.12357\n",
      "training accuracy 1  validation accuracy 0.6072\n",
      "Epoch 10, CIFAR-10 Batch 3:  training cost 0.412113  validation cost 1.09883\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 10, CIFAR-10 Batch 4:  training cost 0.293723  validation cost 1.11215\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 10, CIFAR-10 Batch 5:  training cost 0.607926  validation cost 1.10578\n",
      "training accuracy 0.875  validation accuracy 0.627\n",
      "Epoch 11, CIFAR-10 Batch 1:  training cost 0.530303  validation cost 1.08959\n",
      "training accuracy 0.75  validation accuracy 0.6292\n",
      "Epoch 11, CIFAR-10 Batch 2:  training cost 0.232685  validation cost 1.10704\n",
      "training accuracy 1  validation accuracy 0.62\n",
      "Epoch 11, CIFAR-10 Batch 3:  training cost 0.333212  validation cost 1.07246\n",
      "training accuracy 0.875  validation accuracy 0.6456\n",
      "Epoch 11, CIFAR-10 Batch 4:  training cost 0.252428  validation cost 1.1114\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 11, CIFAR-10 Batch 5:  training cost 0.481072  validation cost 1.10878\n",
      "training accuracy 0.875  validation accuracy 0.6334\n",
      "Epoch 12, CIFAR-10 Batch 1:  training cost 0.430144  validation cost 1.09393\n",
      "training accuracy 0.875  validation accuracy 0.6316\n",
      "Epoch 12, CIFAR-10 Batch 2:  training cost 0.239267  validation cost 1.14577\n",
      "training accuracy 1  validation accuracy 0.6176\n",
      "Epoch 12, CIFAR-10 Batch 3:  training cost 0.356114  validation cost 1.09507\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 12, CIFAR-10 Batch 4:  training cost 0.222626  validation cost 1.12353\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 12, CIFAR-10 Batch 5:  training cost 0.461872  validation cost 1.12802\n",
      "training accuracy 0.875  validation accuracy 0.6314\n",
      "Epoch 13, CIFAR-10 Batch 1:  training cost 0.382888  validation cost 1.13313\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 13, CIFAR-10 Batch 2:  training cost 0.159951  validation cost 1.10615\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 13, CIFAR-10 Batch 3:  training cost 0.303755  validation cost 1.12113\n",
      "training accuracy 1  validation accuracy 0.64\n",
      "Epoch 13, CIFAR-10 Batch 4:  training cost 0.209698  validation cost 1.11786\n",
      "training accuracy 1  validation accuracy 0.6406\n",
      "Epoch 13, CIFAR-10 Batch 5:  training cost 0.34827  validation cost 1.12942\n",
      "training accuracy 1  validation accuracy 0.6274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, CIFAR-10 Batch 1:  training cost 0.395544  validation cost 1.1724\n",
      "training accuracy 1  validation accuracy 0.6204\n",
      "Epoch 14, CIFAR-10 Batch 2:  training cost 0.166865  validation cost 1.11144\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 14, CIFAR-10 Batch 3:  training cost 0.321961  validation cost 1.14486\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 14, CIFAR-10 Batch 4:  training cost 0.265827  validation cost 1.11336\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 14, CIFAR-10 Batch 5:  training cost 0.343866  validation cost 1.1249\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 15, CIFAR-10 Batch 1:  training cost 0.325646  validation cost 1.17103\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 15, CIFAR-10 Batch 2:  training cost 0.172605  validation cost 1.1305\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 15, CIFAR-10 Batch 3:  training cost 0.278747  validation cost 1.1862\n",
      "training accuracy 0.875  validation accuracy 0.6292\n",
      "Epoch 15, CIFAR-10 Batch 4:  training cost 0.193377  validation cost 1.17304\n",
      "training accuracy 1  validation accuracy 0.6416\n",
      "Epoch 15, CIFAR-10 Batch 5:  training cost 0.318385  validation cost 1.13183\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 16, CIFAR-10 Batch 1:  training cost 0.23083  validation cost 1.17062\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 16, CIFAR-10 Batch 2:  training cost 0.135081  validation cost 1.11114\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 16, CIFAR-10 Batch 3:  training cost 0.320904  validation cost 1.17607\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 16, CIFAR-10 Batch 4:  training cost 0.158591  validation cost 1.18955\n",
      "training accuracy 1  validation accuracy 0.6398\n",
      "Epoch 16, CIFAR-10 Batch 5:  training cost 0.351285  validation cost 1.14518\n",
      "training accuracy 0.875  validation accuracy 0.6318\n",
      "Epoch 17, CIFAR-10 Batch 1:  training cost 0.181743  validation cost 1.19714\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 17, CIFAR-10 Batch 2:  training cost 0.088729  validation cost 1.15058\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 17, CIFAR-10 Batch 3:  training cost 0.324077  validation cost 1.20649\n",
      "training accuracy 0.875  validation accuracy 0.6326\n",
      "Epoch 17, CIFAR-10 Batch 4:  training cost 0.164634  validation cost 1.21215\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 17, CIFAR-10 Batch 5:  training cost 0.225984  validation cost 1.21048\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 18, CIFAR-10 Batch 1:  training cost 0.138233  validation cost 1.21384\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 18, CIFAR-10 Batch 2:  training cost 0.118516  validation cost 1.14142\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 18, CIFAR-10 Batch 3:  training cost 0.236208  validation cost 1.28767\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 18, CIFAR-10 Batch 4:  training cost 0.14345  validation cost 1.24883\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 18, CIFAR-10 Batch 5:  training cost 0.265896  validation cost 1.2149\n",
      "training accuracy 0.875  validation accuracy 0.6336\n",
      "Epoch 19, CIFAR-10 Batch 1:  training cost 0.149686  validation cost 1.20353\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 19, CIFAR-10 Batch 2:  training cost 0.127989  validation cost 1.16664\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 19, CIFAR-10 Batch 3:  training cost 0.227982  validation cost 1.269\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 19, CIFAR-10 Batch 4:  training cost 0.13967  validation cost 1.28772\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 19, CIFAR-10 Batch 5:  training cost 0.187606  validation cost 1.24596\n",
      "training accuracy 1  validation accuracy 0.6212\n",
      "Epoch 20, CIFAR-10 Batch 1:  training cost 0.143111  validation cost 1.24156\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 20, CIFAR-10 Batch 2:  training cost 0.0736294  validation cost 1.19714\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 20, CIFAR-10 Batch 3:  training cost 0.221634  validation cost 1.23983\n",
      "training accuracy 1  validation accuracy 0.6434\n",
      "Epoch 20, CIFAR-10 Batch 4:  training cost 0.0629239  validation cost 1.29182\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 20, CIFAR-10 Batch 5:  training cost 0.184446  validation cost 1.25967\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 21, CIFAR-10 Batch 1:  training cost 0.125751  validation cost 1.28379\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 21, CIFAR-10 Batch 2:  training cost 0.0524689  validation cost 1.24272\n",
      "training accuracy 1  validation accuracy 0.6152\n",
      "Epoch 21, CIFAR-10 Batch 3:  training cost 0.1558  validation cost 1.36073\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 21, CIFAR-10 Batch 4:  training cost 0.149112  validation cost 1.28417\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 21, CIFAR-10 Batch 5:  training cost 0.184279  validation cost 1.24833\n",
      "training accuracy 1  validation accuracy 0.6366\n",
      "Epoch 22, CIFAR-10 Batch 1:  training cost 0.201294  validation cost 1.28536\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 22, CIFAR-10 Batch 2:  training cost 0.0515371  validation cost 1.18882\n",
      "training accuracy 1  validation accuracy 0.643\n",
      "Epoch 22, CIFAR-10 Batch 3:  training cost 0.184683  validation cost 1.3461\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 22, CIFAR-10 Batch 4:  training cost 0.12892  validation cost 1.29075\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 22, CIFAR-10 Batch 5:  training cost 0.161386  validation cost 1.33623\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 23, CIFAR-10 Batch 1:  training cost 0.108292  validation cost 1.28261\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 23, CIFAR-10 Batch 2:  training cost 0.0226923  validation cost 1.23517\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 23, CIFAR-10 Batch 3:  training cost 0.116409  validation cost 1.34473\n",
      "training accuracy 1  validation accuracy 0.6376\n",
      "Epoch 23, CIFAR-10 Batch 4:  training cost 0.095129  validation cost 1.35093\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 23, CIFAR-10 Batch 5:  training cost 0.150902  validation cost 1.3549\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 24, CIFAR-10 Batch 1:  training cost 0.130307  validation cost 1.31449\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 24, CIFAR-10 Batch 2:  training cost 0.0410799  validation cost 1.29023\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 24, CIFAR-10 Batch 3:  training cost 0.170407  validation cost 1.35495\n",
      "training accuracy 1  validation accuracy 0.6426\n",
      "Epoch 24, CIFAR-10 Batch 4:  training cost 0.0666421  validation cost 1.37593\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 24, CIFAR-10 Batch 5:  training cost 0.112237  validation cost 1.36581\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 25, CIFAR-10 Batch 1:  training cost 0.113554  validation cost 1.34135\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 25, CIFAR-10 Batch 2:  training cost 0.0381991  validation cost 1.34865\n",
      "training accuracy 1  validation accuracy 0.6192\n",
      "Epoch 25, CIFAR-10 Batch 3:  training cost 0.143249  validation cost 1.40576\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 25, CIFAR-10 Batch 4:  training cost 0.134387  validation cost 1.35066\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 25, CIFAR-10 Batch 5:  training cost 0.102137  validation cost 1.40995\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 26, CIFAR-10 Batch 1:  training cost 0.105672  validation cost 1.34127\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 26, CIFAR-10 Batch 2:  training cost 0.0424416  validation cost 1.33952\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 26, CIFAR-10 Batch 3:  training cost 0.114937  validation cost 1.35465\n",
      "training accuracy 1  validation accuracy 0.643\n",
      "Epoch 26, CIFAR-10 Batch 4:  training cost 0.102952  validation cost 1.3554\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 26, CIFAR-10 Batch 5:  training cost 0.117097  validation cost 1.4985\n",
      "training accuracy 1  validation accuracy 0.6132\n",
      "Epoch 27, CIFAR-10 Batch 1:  training cost 0.104345  validation cost 1.3651\n",
      "training accuracy 1  validation accuracy 0.6362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, CIFAR-10 Batch 2:  training cost 0.062507  validation cost 1.35777\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 27, CIFAR-10 Batch 3:  training cost 0.150328  validation cost 1.3587\n",
      "training accuracy 1  validation accuracy 0.6366\n",
      "Epoch 27, CIFAR-10 Batch 4:  training cost 0.165633  validation cost 1.38379\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 27, CIFAR-10 Batch 5:  training cost 0.127407  validation cost 1.35339\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 28, CIFAR-10 Batch 1:  training cost 0.13814  validation cost 1.3733\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 28, CIFAR-10 Batch 2:  training cost 0.0523102  validation cost 1.31964\n",
      "training accuracy 1  validation accuracy 0.6378\n",
      "Epoch 28, CIFAR-10 Batch 3:  training cost 0.120036  validation cost 1.41272\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 28, CIFAR-10 Batch 4:  training cost 0.14859  validation cost 1.39507\n",
      "training accuracy 0.875  validation accuracy 0.6266\n",
      "Epoch 28, CIFAR-10 Batch 5:  training cost 0.0993408  validation cost 1.45156\n",
      "training accuracy 1  validation accuracy 0.6204\n",
      "Epoch 29, CIFAR-10 Batch 1:  training cost 0.0588249  validation cost 1.41159\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 29, CIFAR-10 Batch 2:  training cost 0.0222329  validation cost 1.36173\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 29, CIFAR-10 Batch 3:  training cost 0.10706  validation cost 1.42836\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 29, CIFAR-10 Batch 4:  training cost 0.0721828  validation cost 1.42726\n",
      "training accuracy 1  validation accuracy 0.6388\n",
      "Epoch 29, CIFAR-10 Batch 5:  training cost 0.122558  validation cost 1.52945\n",
      "training accuracy 1  validation accuracy 0.6206\n",
      "Epoch 30, CIFAR-10 Batch 1:  training cost 0.0996359  validation cost 1.46811\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 30, CIFAR-10 Batch 2:  training cost 0.0284261  validation cost 1.37351\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 30, CIFAR-10 Batch 3:  training cost 0.197101  validation cost 1.46352\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 30, CIFAR-10 Batch 4:  training cost 0.104447  validation cost 1.45332\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 30, CIFAR-10 Batch 5:  training cost 0.0693198  validation cost 1.4781\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 31, CIFAR-10 Batch 1:  training cost 0.0911918  validation cost 1.51291\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 31, CIFAR-10 Batch 2:  training cost 0.0493687  validation cost 1.3854\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 31, CIFAR-10 Batch 3:  training cost 0.0568144  validation cost 1.50855\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 31, CIFAR-10 Batch 4:  training cost 0.036062  validation cost 1.49326\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 31, CIFAR-10 Batch 5:  training cost 0.0738688  validation cost 1.49355\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 32, CIFAR-10 Batch 1:  training cost 0.099517  validation cost 1.54504\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 32, CIFAR-10 Batch 2:  training cost 0.0171195  validation cost 1.42974\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 32, CIFAR-10 Batch 3:  training cost 0.0861557  validation cost 1.48815\n",
      "training accuracy 1  validation accuracy 0.619\n",
      "Epoch 32, CIFAR-10 Batch 4:  training cost 0.0574595  validation cost 1.59991\n",
      "training accuracy 1  validation accuracy 0.6168\n",
      "Epoch 32, CIFAR-10 Batch 5:  training cost 0.0629066  validation cost 1.50128\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 33, CIFAR-10 Batch 1:  training cost 0.0433372  validation cost 1.56292\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 33, CIFAR-10 Batch 2:  training cost 0.0189195  validation cost 1.44696\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 33, CIFAR-10 Batch 3:  training cost 0.0987299  validation cost 1.59253\n",
      "training accuracy 1  validation accuracy 0.6158\n",
      "Epoch 33, CIFAR-10 Batch 4:  training cost 0.0237816  validation cost 1.52199\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 33, CIFAR-10 Batch 5:  training cost 0.0701784  validation cost 1.53565\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 34, CIFAR-10 Batch 1:  training cost 0.0566651  validation cost 1.61379\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 34, CIFAR-10 Batch 2:  training cost 0.0300237  validation cost 1.46379\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 34, CIFAR-10 Batch 3:  training cost 0.0518822  validation cost 1.572\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 34, CIFAR-10 Batch 4:  training cost 0.0330146  validation cost 1.51456\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 34, CIFAR-10 Batch 5:  training cost 0.0986093  validation cost 1.50113\n",
      "training accuracy 1  validation accuracy 0.6364\n",
      "Epoch 35, CIFAR-10 Batch 1:  training cost 0.0444133  validation cost 1.63416\n",
      "training accuracy 1  validation accuracy 0.6202\n",
      "Epoch 35, CIFAR-10 Batch 2:  training cost 0.0449316  validation cost 1.45531\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 35, CIFAR-10 Batch 3:  training cost 0.082573  validation cost 1.56881\n",
      "training accuracy 1  validation accuracy 0.639\n",
      "Epoch 35, CIFAR-10 Batch 4:  training cost 0.0173366  validation cost 1.54923\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 35, CIFAR-10 Batch 5:  training cost 0.106002  validation cost 1.57419\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 36, CIFAR-10 Batch 1:  training cost 0.053176  validation cost 1.72245\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 36, CIFAR-10 Batch 2:  training cost 0.0291702  validation cost 1.51455\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 36, CIFAR-10 Batch 3:  training cost 0.0493322  validation cost 1.67952\n",
      "training accuracy 1  validation accuracy 0.612\n",
      "Epoch 36, CIFAR-10 Batch 4:  training cost 0.0549892  validation cost 1.58142\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 36, CIFAR-10 Batch 5:  training cost 0.0533499  validation cost 1.61635\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 37, CIFAR-10 Batch 1:  training cost 0.0337547  validation cost 1.65546\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 37, CIFAR-10 Batch 2:  training cost 0.0326668  validation cost 1.5001\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 37, CIFAR-10 Batch 3:  training cost 0.0659779  validation cost 1.705\n",
      "training accuracy 1  validation accuracy 0.615\n",
      "Epoch 37, CIFAR-10 Batch 4:  training cost 0.0445891  validation cost 1.6242\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 37, CIFAR-10 Batch 5:  training cost 0.0744156  validation cost 1.64539\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 38, CIFAR-10 Batch 1:  training cost 0.0546913  validation cost 1.65314\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 38, CIFAR-10 Batch 2:  training cost 0.0185655  validation cost 1.58712\n",
      "training accuracy 1  validation accuracy 0.6172\n",
      "Epoch 38, CIFAR-10 Batch 3:  training cost 0.0298105  validation cost 1.62414\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 38, CIFAR-10 Batch 4:  training cost 0.0353745  validation cost 1.56945\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 38, CIFAR-10 Batch 5:  training cost 0.0791885  validation cost 1.6896\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 39, CIFAR-10 Batch 1:  training cost 0.0674126  validation cost 1.70439\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 39, CIFAR-10 Batch 2:  training cost 0.0210478  validation cost 1.52185\n",
      "training accuracy 1  validation accuracy 0.6106\n",
      "Epoch 39, CIFAR-10 Batch 3:  training cost 0.0523341  validation cost 1.78254\n",
      "training accuracy 1  validation accuracy 0.6136\n",
      "Epoch 39, CIFAR-10 Batch 4:  training cost 0.0161192  validation cost 1.67278\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 39, CIFAR-10 Batch 5:  training cost 0.0728167  validation cost 1.59499\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 40, CIFAR-10 Batch 1:  training cost 0.0311438  validation cost 1.68077\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 40, CIFAR-10 Batch 2:  training cost 0.0204617  validation cost 1.58128\n",
      "training accuracy 1  validation accuracy 0.6246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, CIFAR-10 Batch 3:  training cost 0.0589861  validation cost 1.71351\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 40, CIFAR-10 Batch 4:  training cost 0.0443798  validation cost 1.75547\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 40, CIFAR-10 Batch 5:  training cost 0.149095  validation cost 1.61889\n",
      "training accuracy 0.875  validation accuracy 0.6372\n",
      "Epoch 41, CIFAR-10 Batch 1:  training cost 0.0409012  validation cost 1.68532\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 41, CIFAR-10 Batch 2:  training cost 0.0147016  validation cost 1.59779\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 41, CIFAR-10 Batch 3:  training cost 0.0416881  validation cost 1.75956\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 41, CIFAR-10 Batch 4:  training cost 0.0108399  validation cost 1.75611\n",
      "training accuracy 1  validation accuracy 0.6196\n",
      "Epoch 41, CIFAR-10 Batch 5:  training cost 0.103691  validation cost 1.71827\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 42, CIFAR-10 Batch 1:  training cost 0.0150974  validation cost 1.78798\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 42, CIFAR-10 Batch 2:  training cost 0.0181745  validation cost 1.67469\n",
      "training accuracy 1  validation accuracy 0.6186\n",
      "Epoch 42, CIFAR-10 Batch 3:  training cost 0.0587816  validation cost 1.78673\n",
      "training accuracy 1  validation accuracy 0.616\n",
      "Epoch 42, CIFAR-10 Batch 4:  training cost 0.0309983  validation cost 1.68848\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 42, CIFAR-10 Batch 5:  training cost 0.0583327  validation cost 1.70922\n",
      "training accuracy 1  validation accuracy 0.6392\n",
      "Epoch 43, CIFAR-10 Batch 1:  training cost 0.0188253  validation cost 1.78541\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 43, CIFAR-10 Batch 2:  training cost 0.0317262  validation cost 1.67366\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 43, CIFAR-10 Batch 3:  training cost 0.0718199  validation cost 1.81004\n",
      "training accuracy 1  validation accuracy 0.608\n",
      "Epoch 43, CIFAR-10 Batch 4:  training cost 0.0276435  validation cost 1.74007\n",
      "training accuracy 1  validation accuracy 0.621\n",
      "Epoch 43, CIFAR-10 Batch 5:  training cost 0.0363356  validation cost 1.72245\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 44, CIFAR-10 Batch 1:  training cost 0.0363186  validation cost 1.78576\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 44, CIFAR-10 Batch 2:  training cost 0.0244868  validation cost 1.6946\n",
      "training accuracy 1  validation accuracy 0.6156\n",
      "Epoch 44, CIFAR-10 Batch 3:  training cost 0.0858256  validation cost 1.7908\n",
      "training accuracy 1  validation accuracy 0.6118\n",
      "Epoch 44, CIFAR-10 Batch 4:  training cost 0.0272094  validation cost 1.68285\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 44, CIFAR-10 Batch 5:  training cost 0.0885346  validation cost 1.7073\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 45, CIFAR-10 Batch 1:  training cost 0.0169676  validation cost 1.83232\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 45, CIFAR-10 Batch 2:  training cost 0.0170713  validation cost 1.65661\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 45, CIFAR-10 Batch 3:  training cost 0.0768893  validation cost 1.91573\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 45, CIFAR-10 Batch 4:  training cost 0.0205466  validation cost 1.71653\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 45, CIFAR-10 Batch 5:  training cost 0.0620448  validation cost 1.79848\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 46, CIFAR-10 Batch 1:  training cost 0.008821  validation cost 1.85873\n",
      "training accuracy 1  validation accuracy 0.6186\n",
      "Epoch 46, CIFAR-10 Batch 2:  training cost 0.0167869  validation cost 1.7502\n",
      "training accuracy 1  validation accuracy 0.617\n",
      "Epoch 46, CIFAR-10 Batch 3:  training cost 0.0420728  validation cost 1.96049\n",
      "training accuracy 1  validation accuracy 0.6064\n",
      "Epoch 46, CIFAR-10 Batch 4:  training cost 0.0332083  validation cost 1.74822\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 46, CIFAR-10 Batch 5:  training cost 0.0810298  validation cost 1.77484\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 47, CIFAR-10 Batch 1:  training cost 0.00468967  validation cost 1.81153\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 47, CIFAR-10 Batch 2:  training cost 0.0111384  validation cost 1.75151\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 47, CIFAR-10 Batch 3:  training cost 0.0428634  validation cost 2.06014\n",
      "training accuracy 1  validation accuracy 0.6016\n",
      "Epoch 47, CIFAR-10 Batch 4:  training cost 0.0219655  validation cost 1.78309\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 47, CIFAR-10 Batch 5:  training cost 0.0502095  validation cost 1.78323\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 48, CIFAR-10 Batch 1:  training cost 0.00927844  validation cost 1.74287\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 48, CIFAR-10 Batch 2:  training cost 0.00769303  validation cost 1.71988\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 48, CIFAR-10 Batch 3:  training cost 0.0668775  validation cost 1.91265\n",
      "training accuracy 1  validation accuracy 0.6094\n",
      "Epoch 48, CIFAR-10 Batch 4:  training cost 0.0353577  validation cost 1.76906\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 48, CIFAR-10 Batch 5:  training cost 0.0449884  validation cost 1.75049\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 49, CIFAR-10 Batch 1:  training cost 0.0216475  validation cost 1.92805\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 49, CIFAR-10 Batch 2:  training cost 0.0257074  validation cost 1.81113\n",
      "training accuracy 1  validation accuracy 0.609\n",
      "Epoch 49, CIFAR-10 Batch 3:  training cost 0.0183818  validation cost 1.96913\n",
      "training accuracy 1  validation accuracy 0.612\n",
      "Epoch 49, CIFAR-10 Batch 4:  training cost 0.0219529  validation cost 1.75472\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 49, CIFAR-10 Batch 5:  training cost 0.0647873  validation cost 1.8273\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 50, CIFAR-10 Batch 1:  training cost 0.0282057  validation cost 1.88778\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 50, CIFAR-10 Batch 2:  training cost 0.0105076  validation cost 1.86894\n",
      "training accuracy 1  validation accuracy 0.6192\n",
      "Epoch 50, CIFAR-10 Batch 3:  training cost 0.0495605  validation cost 1.96646\n",
      "training accuracy 1  validation accuracy 0.6012\n",
      "Epoch 50, CIFAR-10 Batch 4:  training cost 0.0137986  validation cost 1.77064\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 50, CIFAR-10 Batch 5:  training cost 0.0636111  validation cost 1.82299\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 51, CIFAR-10 Batch 1:  training cost 0.0240743  validation cost 1.95637\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 51, CIFAR-10 Batch 2:  training cost 0.00434605  validation cost 1.74974\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 51, CIFAR-10 Batch 3:  training cost 0.0339086  validation cost 2.03519\n",
      "training accuracy 1  validation accuracy 0.6156\n",
      "Epoch 51, CIFAR-10 Batch 4:  training cost 0.0099111  validation cost 1.75632\n",
      "training accuracy 1  validation accuracy 0.6406\n",
      "Epoch 51, CIFAR-10 Batch 5:  training cost 0.0357031  validation cost 1.81711\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 52, CIFAR-10 Batch 1:  training cost 0.0182129  validation cost 2.02373\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 52, CIFAR-10 Batch 2:  training cost 0.0139138  validation cost 1.9226\n",
      "training accuracy 1  validation accuracy 0.6142\n",
      "Epoch 52, CIFAR-10 Batch 3:  training cost 0.0675233  validation cost 2.09245\n",
      "training accuracy 1  validation accuracy 0.591\n",
      "Epoch 52, CIFAR-10 Batch 4:  training cost 0.0207182  validation cost 1.74623\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 52, CIFAR-10 Batch 5:  training cost 0.0545801  validation cost 1.74466\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 53, CIFAR-10 Batch 1:  training cost 0.0164097  validation cost 1.86226\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 53, CIFAR-10 Batch 2:  training cost 0.0141273  validation cost 1.84345\n",
      "training accuracy 1  validation accuracy 0.6196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, CIFAR-10 Batch 3:  training cost 0.0770133  validation cost 2.11216\n",
      "training accuracy 1  validation accuracy 0.593\n",
      "Epoch 53, CIFAR-10 Batch 4:  training cost 0.0238785  validation cost 1.75502\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 53, CIFAR-10 Batch 5:  training cost 0.0494699  validation cost 1.84337\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 54, CIFAR-10 Batch 1:  training cost 0.0203628  validation cost 2.00226\n",
      "training accuracy 1  validation accuracy 0.617\n",
      "Epoch 54, CIFAR-10 Batch 2:  training cost 0.00161989  validation cost 1.87688\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 54, CIFAR-10 Batch 3:  training cost 0.0262873  validation cost 2.03789\n",
      "training accuracy 1  validation accuracy 0.6096\n",
      "Epoch 54, CIFAR-10 Batch 4:  training cost 0.0139012  validation cost 1.85046\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 54, CIFAR-10 Batch 5:  training cost 0.0414487  validation cost 1.8434\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 55, CIFAR-10 Batch 1:  training cost 0.0131553  validation cost 1.98239\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 55, CIFAR-10 Batch 2:  training cost 0.00142561  validation cost 1.8325\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 55, CIFAR-10 Batch 3:  training cost 0.0258063  validation cost 2.03277\n",
      "training accuracy 1  validation accuracy 0.6042\n",
      "Epoch 55, CIFAR-10 Batch 4:  training cost 0.014628  validation cost 1.76492\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 55, CIFAR-10 Batch 5:  training cost 0.0309303  validation cost 1.86771\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 56, CIFAR-10 Batch 1:  training cost 0.00962023  validation cost 1.93408\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 56, CIFAR-10 Batch 2:  training cost 0.0091322  validation cost 1.83649\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 56, CIFAR-10 Batch 3:  training cost 0.0771246  validation cost 2.20672\n",
      "training accuracy 1  validation accuracy 0.5994\n",
      "Epoch 56, CIFAR-10 Batch 4:  training cost 0.0473611  validation cost 1.86694\n",
      "training accuracy 1  validation accuracy 0.616\n",
      "Epoch 56, CIFAR-10 Batch 5:  training cost 0.0299816  validation cost 1.83132\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 57, CIFAR-10 Batch 1:  training cost 0.011278  validation cost 1.92747\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 57, CIFAR-10 Batch 2:  training cost 0.00102859  validation cost 1.97675\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 57, CIFAR-10 Batch 3:  training cost 0.0322301  validation cost 2.25455\n",
      "training accuracy 1  validation accuracy 0.6012\n",
      "Epoch 57, CIFAR-10 Batch 4:  training cost 0.0257152  validation cost 1.81196\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 57, CIFAR-10 Batch 5:  training cost 0.0607001  validation cost 1.84878\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 58, CIFAR-10 Batch 1:  training cost 0.00394288  validation cost 2.00449\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 58, CIFAR-10 Batch 2:  training cost 0.00400863  validation cost 1.89235\n",
      "training accuracy 1  validation accuracy 0.619\n",
      "Epoch 58, CIFAR-10 Batch 3:  training cost 0.0167998  validation cost 2.09849\n",
      "training accuracy 1  validation accuracy 0.604\n",
      "Epoch 58, CIFAR-10 Batch 4:  training cost 0.0237362  validation cost 1.85167\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 58, CIFAR-10 Batch 5:  training cost 0.0415583  validation cost 1.8879\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 59, CIFAR-10 Batch 1:  training cost 0.00482688  validation cost 2.07826\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 59, CIFAR-10 Batch 2:  training cost 0.0194008  validation cost 1.95981\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 59, CIFAR-10 Batch 3:  training cost 0.0526277  validation cost 2.27126\n",
      "training accuracy 1  validation accuracy 0.5878\n",
      "Epoch 59, CIFAR-10 Batch 4:  training cost 0.00885067  validation cost 1.88525\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 59, CIFAR-10 Batch 5:  training cost 0.0215528  validation cost 1.90948\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 60, CIFAR-10 Batch 1:  training cost 0.00476409  validation cost 2.05838\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 60, CIFAR-10 Batch 2:  training cost 0.00285262  validation cost 1.79976\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 60, CIFAR-10 Batch 3:  training cost 0.0730626  validation cost 2.08404\n",
      "training accuracy 1  validation accuracy 0.6186\n",
      "Epoch 60, CIFAR-10 Batch 4:  training cost 0.0223465  validation cost 1.78486\n",
      "training accuracy 1  validation accuracy 0.6402\n",
      "Epoch 60, CIFAR-10 Batch 5:  training cost 0.0646  validation cost 1.98592\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 61, CIFAR-10 Batch 1:  training cost 0.0133545  validation cost 2.03483\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 61, CIFAR-10 Batch 2:  training cost 0.0145524  validation cost 1.86947\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 61, CIFAR-10 Batch 3:  training cost 0.010596  validation cost 2.10392\n",
      "training accuracy 1  validation accuracy 0.6178\n",
      "Epoch 61, CIFAR-10 Batch 4:  training cost 0.00837238  validation cost 1.93774\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 61, CIFAR-10 Batch 5:  training cost 0.040591  validation cost 2.01765\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 62, CIFAR-10 Batch 1:  training cost 0.00298451  validation cost 2.16298\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 62, CIFAR-10 Batch 2:  training cost 0.00633401  validation cost 1.95944\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 62, CIFAR-10 Batch 3:  training cost 0.0312067  validation cost 2.17832\n",
      "training accuracy 1  validation accuracy 0.617\n",
      "Epoch 62, CIFAR-10 Batch 4:  training cost 0.00931065  validation cost 1.94437\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 62, CIFAR-10 Batch 5:  training cost 0.0206894  validation cost 2.03941\n",
      "training accuracy 1  validation accuracy 0.6212\n",
      "Epoch 63, CIFAR-10 Batch 1:  training cost 0.0148873  validation cost 2.02121\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 63, CIFAR-10 Batch 2:  training cost 0.0105731  validation cost 1.97222\n",
      "training accuracy 1  validation accuracy 0.6164\n",
      "Epoch 63, CIFAR-10 Batch 3:  training cost 0.0302489  validation cost 2.12111\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 63, CIFAR-10 Batch 4:  training cost 0.0160478  validation cost 1.98356\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 63, CIFAR-10 Batch 5:  training cost 0.0151794  validation cost 2.10451\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 64, CIFAR-10 Batch 1:  training cost 0.010764  validation cost 2.16065\n",
      "training accuracy 1  validation accuracy 0.6172\n",
      "Epoch 64, CIFAR-10 Batch 2:  training cost 0.00250141  validation cost 1.94458\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 64, CIFAR-10 Batch 3:  training cost 0.0102062  validation cost 2.2021\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 64, CIFAR-10 Batch 4:  training cost 0.00296304  validation cost 1.99087\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 64, CIFAR-10 Batch 5:  training cost 0.0196161  validation cost 2.057\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 65, CIFAR-10 Batch 1:  training cost 0.00751109  validation cost 2.09284\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 65, CIFAR-10 Batch 2:  training cost 0.00149779  validation cost 2.03183\n",
      "training accuracy 1  validation accuracy 0.6206\n",
      "Epoch 65, CIFAR-10 Batch 3:  training cost 0.0425888  validation cost 1.98776\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 65, CIFAR-10 Batch 4:  training cost 0.0136017  validation cost 1.87862\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 65, CIFAR-10 Batch 5:  training cost 0.0251952  validation cost 2.10148\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 66, CIFAR-10 Batch 1:  training cost 0.00473717  validation cost 2.21437\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 66, CIFAR-10 Batch 2:  training cost 0.00729272  validation cost 2.0695\n",
      "training accuracy 1  validation accuracy 0.634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, CIFAR-10 Batch 3:  training cost 0.0462559  validation cost 2.11007\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 66, CIFAR-10 Batch 4:  training cost 0.0385452  validation cost 1.97811\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 66, CIFAR-10 Batch 5:  training cost 0.0469329  validation cost 2.0554\n",
      "training accuracy 1  validation accuracy 0.6364\n",
      "Epoch 67, CIFAR-10 Batch 1:  training cost 0.0129707  validation cost 2.13843\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 67, CIFAR-10 Batch 2:  training cost 0.000738605  validation cost 2.12398\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 67, CIFAR-10 Batch 3:  training cost 0.0196228  validation cost 2.02628\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 67, CIFAR-10 Batch 4:  training cost 0.0274134  validation cost 2.01926\n",
      "training accuracy 1  validation accuracy 0.64\n",
      "Epoch 67, CIFAR-10 Batch 5:  training cost 0.00833489  validation cost 2.23952\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 68, CIFAR-10 Batch 1:  training cost 0.0239965  validation cost 2.31853\n",
      "training accuracy 1  validation accuracy 0.6156\n",
      "Epoch 68, CIFAR-10 Batch 2:  training cost 0.00294404  validation cost 2.15963\n",
      "training accuracy 1  validation accuracy 0.6188\n",
      "Epoch 68, CIFAR-10 Batch 3:  training cost 0.00809447  validation cost 2.31902\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 68, CIFAR-10 Batch 4:  training cost 0.0027336  validation cost 2.02594\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 68, CIFAR-10 Batch 5:  training cost 0.0215471  validation cost 2.15665\n",
      "training accuracy 1  validation accuracy 0.6192\n",
      "Epoch 69, CIFAR-10 Batch 1:  training cost 0.0263065  validation cost 2.25348\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 69, CIFAR-10 Batch 2:  training cost 0.00619626  validation cost 2.07\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 69, CIFAR-10 Batch 3:  training cost 0.0439241  validation cost 2.1279\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 69, CIFAR-10 Batch 4:  training cost 0.0138729  validation cost 1.9948\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 69, CIFAR-10 Batch 5:  training cost 0.0284107  validation cost 2.16776\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 70, CIFAR-10 Batch 1:  training cost 0.00535766  validation cost 2.17949\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 70, CIFAR-10 Batch 2:  training cost 0.000504081  validation cost 2.18705\n",
      "training accuracy 1  validation accuracy 0.6138\n",
      "Epoch 70, CIFAR-10 Batch 3:  training cost 0.00669361  validation cost 2.11182\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 70, CIFAR-10 Batch 4:  training cost 0.00353139  validation cost 2.01981\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 70, CIFAR-10 Batch 5:  training cost 0.0431179  validation cost 2.1135\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 71, CIFAR-10 Batch 1:  training cost 0.00918808  validation cost 2.31859\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 71, CIFAR-10 Batch 2:  training cost 0.00222383  validation cost 2.2793\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 71, CIFAR-10 Batch 3:  training cost 0.00471152  validation cost 2.19358\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 71, CIFAR-10 Batch 4:  training cost 0.0408382  validation cost 1.89704\n",
      "training accuracy 1  validation accuracy 0.6388\n",
      "Epoch 71, CIFAR-10 Batch 5:  training cost 0.0214758  validation cost 2.10731\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 72, CIFAR-10 Batch 1:  training cost 0.00255002  validation cost 2.46888\n",
      "training accuracy 1  validation accuracy 0.6168\n",
      "Epoch 72, CIFAR-10 Batch 2:  training cost 0.00112904  validation cost 2.22626\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 72, CIFAR-10 Batch 3:  training cost 0.00759878  validation cost 2.20983\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 72, CIFAR-10 Batch 4:  training cost 0.00562385  validation cost 2.15812\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 72, CIFAR-10 Batch 5:  training cost 0.0113879  validation cost 2.28543\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 73, CIFAR-10 Batch 1:  training cost 0.00916498  validation cost 2.16857\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 73, CIFAR-10 Batch 2:  training cost 0.00840471  validation cost 2.16137\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 73, CIFAR-10 Batch 3:  training cost 0.00820489  validation cost 2.25172\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 73, CIFAR-10 Batch 4:  training cost 0.0034559  validation cost 2.04306\n",
      "training accuracy 1  validation accuracy 0.6428\n",
      "Epoch 73, CIFAR-10 Batch 5:  training cost 0.0285311  validation cost 2.17821\n",
      "training accuracy 1  validation accuracy 0.6198\n",
      "Epoch 74, CIFAR-10 Batch 1:  training cost 0.0016303  validation cost 2.34961\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 74, CIFAR-10 Batch 2:  training cost 0.00701769  validation cost 2.16089\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 74, CIFAR-10 Batch 3:  training cost 0.0048662  validation cost 2.23869\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 74, CIFAR-10 Batch 4:  training cost 0.0111559  validation cost 2.15752\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 74, CIFAR-10 Batch 5:  training cost 0.00356301  validation cost 2.24589\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 75, CIFAR-10 Batch 1:  training cost 0.0140138  validation cost 2.27289\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 75, CIFAR-10 Batch 2:  training cost 0.00432622  validation cost 2.15102\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 75, CIFAR-10 Batch 3:  training cost 0.00299777  validation cost 2.3155\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 75, CIFAR-10 Batch 4:  training cost 0.00652521  validation cost 2.20055\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 75, CIFAR-10 Batch 5:  training cost 0.0102072  validation cost 2.2651\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 76, CIFAR-10 Batch 1:  training cost 0.022442  validation cost 2.37307\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 76, CIFAR-10 Batch 2:  training cost 0.00600844  validation cost 2.21898\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 76, CIFAR-10 Batch 3:  training cost 0.0092767  validation cost 2.30576\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 76, CIFAR-10 Batch 4:  training cost 0.00772867  validation cost 2.21506\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 76, CIFAR-10 Batch 5:  training cost 0.0107924  validation cost 2.24422\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 77, CIFAR-10 Batch 1:  training cost 0.0134472  validation cost 2.21532\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 77, CIFAR-10 Batch 2:  training cost 0.0111367  validation cost 2.1318\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 77, CIFAR-10 Batch 3:  training cost 0.00422097  validation cost 2.15945\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 77, CIFAR-10 Batch 4:  training cost 0.00869479  validation cost 2.08025\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 77, CIFAR-10 Batch 5:  training cost 0.0191506  validation cost 2.20207\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 78, CIFAR-10 Batch 1:  training cost 0.00258357  validation cost 2.25527\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 78, CIFAR-10 Batch 2:  training cost 0.0199261  validation cost 2.24102\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 78, CIFAR-10 Batch 3:  training cost 0.0108606  validation cost 2.25941\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 78, CIFAR-10 Batch 4:  training cost 0.0220305  validation cost 2.22294\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 78, CIFAR-10 Batch 5:  training cost 0.04112  validation cost 2.3662\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 79, CIFAR-10 Batch 1:  training cost 0.0301331  validation cost 2.40176\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 79, CIFAR-10 Batch 2:  training cost 0.0008826  validation cost 2.25452\n",
      "training accuracy 1  validation accuracy 0.6204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, CIFAR-10 Batch 3:  training cost 0.00320969  validation cost 2.20904\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 79, CIFAR-10 Batch 4:  training cost 0.00963849  validation cost 2.10641\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 79, CIFAR-10 Batch 5:  training cost 0.0249102  validation cost 2.28356\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 80, CIFAR-10 Batch 1:  training cost 0.0143695  validation cost 2.49539\n",
      "training accuracy 1  validation accuracy 0.6202\n",
      "Epoch 80, CIFAR-10 Batch 2:  training cost 0.00109984  validation cost 2.22719\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 80, CIFAR-10 Batch 3:  training cost 0.00443076  validation cost 2.38262\n",
      "training accuracy 1  validation accuracy 0.6192\n",
      "Epoch 80, CIFAR-10 Batch 4:  training cost 0.00408091  validation cost 2.28309\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 80, CIFAR-10 Batch 5:  training cost 0.0148284  validation cost 2.42036\n",
      "training accuracy 1  validation accuracy 0.6176\n",
      "Epoch 81, CIFAR-10 Batch 1:  training cost 0.0102426  validation cost 2.30943\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 81, CIFAR-10 Batch 2:  training cost 0.000510271  validation cost 2.28207\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 81, CIFAR-10 Batch 3:  training cost 0.00381628  validation cost 2.3009\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 81, CIFAR-10 Batch 4:  training cost 0.000873932  validation cost 2.22905\n",
      "training accuracy 1  validation accuracy 0.6434\n",
      "Epoch 81, CIFAR-10 Batch 5:  training cost 0.00623455  validation cost 2.27253\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 82, CIFAR-10 Batch 1:  training cost 0.0211473  validation cost 2.3939\n",
      "training accuracy 1  validation accuracy 0.6194\n",
      "Epoch 82, CIFAR-10 Batch 2:  training cost 0.0111939  validation cost 2.37044\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 82, CIFAR-10 Batch 3:  training cost 0.00568611  validation cost 2.36731\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 82, CIFAR-10 Batch 4:  training cost 0.0020533  validation cost 2.26143\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 82, CIFAR-10 Batch 5:  training cost 0.00962882  validation cost 2.26454\n",
      "training accuracy 1  validation accuracy 0.617\n",
      "Epoch 83, CIFAR-10 Batch 1:  training cost 0.0103844  validation cost 2.41702\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 83, CIFAR-10 Batch 2:  training cost 0.00200817  validation cost 2.30119\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 83, CIFAR-10 Batch 3:  training cost 0.00666692  validation cost 2.27122\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 83, CIFAR-10 Batch 4:  training cost 0.00724747  validation cost 2.27647\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 83, CIFAR-10 Batch 5:  training cost 0.00900188  validation cost 2.3637\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 84, CIFAR-10 Batch 1:  training cost 0.0104771  validation cost 2.43982\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 84, CIFAR-10 Batch 2:  training cost 0.00482737  validation cost 2.2677\n",
      "training accuracy 1  validation accuracy 0.619\n",
      "Epoch 84, CIFAR-10 Batch 3:  training cost 0.0030735  validation cost 2.27466\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 84, CIFAR-10 Batch 4:  training cost 0.00500936  validation cost 2.30265\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 84, CIFAR-10 Batch 5:  training cost 0.0216179  validation cost 2.33797\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 85, CIFAR-10 Batch 1:  training cost 0.00136059  validation cost 2.32782\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 85, CIFAR-10 Batch 2:  training cost 0.00190885  validation cost 2.22958\n",
      "training accuracy 1  validation accuracy 0.6204\n",
      "Epoch 85, CIFAR-10 Batch 3:  training cost 0.00372137  validation cost 2.24135\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 85, CIFAR-10 Batch 4:  training cost 0.00650896  validation cost 2.40557\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 85, CIFAR-10 Batch 5:  training cost 0.00680445  validation cost 2.48624\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 86, CIFAR-10 Batch 1:  training cost 0.0099479  validation cost 2.48322\n",
      "training accuracy 1  validation accuracy 0.6206\n",
      "Epoch 86, CIFAR-10 Batch 2:  training cost 0.00070449  validation cost 2.262\n",
      "training accuracy 1  validation accuracy 0.6208\n",
      "Epoch 86, CIFAR-10 Batch 3:  training cost 0.0074389  validation cost 2.4153\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 86, CIFAR-10 Batch 4:  training cost 0.00759551  validation cost 2.28287\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 86, CIFAR-10 Batch 5:  training cost 0.0108841  validation cost 2.38888\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 87, CIFAR-10 Batch 1:  training cost 0.0146925  validation cost 2.4665\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 87, CIFAR-10 Batch 2:  training cost 0.00194288  validation cost 2.31063\n",
      "training accuracy 1  validation accuracy 0.6208\n",
      "Epoch 87, CIFAR-10 Batch 3:  training cost 0.00627337  validation cost 2.45634\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 87, CIFAR-10 Batch 4:  training cost 0.0192939  validation cost 2.32874\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 87, CIFAR-10 Batch 5:  training cost 0.021614  validation cost 2.40904\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 88, CIFAR-10 Batch 1:  training cost 0.0089216  validation cost 2.45905\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 88, CIFAR-10 Batch 2:  training cost 0.0198884  validation cost 2.37955\n",
      "training accuracy 1  validation accuracy 0.6172\n",
      "Epoch 88, CIFAR-10 Batch 3:  training cost 0.0317415  validation cost 2.32817\n",
      "training accuracy 1  validation accuracy 0.612\n",
      "Epoch 88, CIFAR-10 Batch 4:  training cost 0.00248789  validation cost 2.30088\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 88, CIFAR-10 Batch 5:  training cost 0.017583  validation cost 2.34037\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 89, CIFAR-10 Batch 1:  training cost 0.00760366  validation cost 2.56533\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 89, CIFAR-10 Batch 2:  training cost 0.00650969  validation cost 2.4185\n",
      "training accuracy 1  validation accuracy 0.61\n",
      "Epoch 89, CIFAR-10 Batch 3:  training cost 0.00796133  validation cost 2.28273\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 89, CIFAR-10 Batch 4:  training cost 0.00157858  validation cost 2.30189\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 89, CIFAR-10 Batch 5:  training cost 0.022545  validation cost 2.45782\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 90, CIFAR-10 Batch 1:  training cost 0.00232515  validation cost 2.57493\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 90, CIFAR-10 Batch 2:  training cost 0.0110419  validation cost 2.39428\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 90, CIFAR-10 Batch 3:  training cost 0.00805507  validation cost 2.42845\n",
      "training accuracy 1  validation accuracy 0.618\n",
      "Epoch 90, CIFAR-10 Batch 4:  training cost 0.00701612  validation cost 2.51125\n",
      "training accuracy 1  validation accuracy 0.621\n",
      "Epoch 90, CIFAR-10 Batch 5:  training cost 0.00750512  validation cost 2.42962\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 91, CIFAR-10 Batch 1:  training cost 0.003762  validation cost 2.53615\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 91, CIFAR-10 Batch 2:  training cost 0.00109731  validation cost 2.50903\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 91, CIFAR-10 Batch 3:  training cost 0.00564314  validation cost 2.49933\n",
      "training accuracy 1  validation accuracy 0.617\n",
      "Epoch 91, CIFAR-10 Batch 4:  training cost 0.00444145  validation cost 2.35686\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 91, CIFAR-10 Batch 5:  training cost 0.00838069  validation cost 2.56034\n",
      "training accuracy 1  validation accuracy 0.6192\n",
      "Epoch 92, CIFAR-10 Batch 1:  training cost 0.00683633  validation cost 2.60827\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 92, CIFAR-10 Batch 2:  training cost 0.0262369  validation cost 2.53187\n",
      "training accuracy 1  validation accuracy 0.6122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, CIFAR-10 Batch 3:  training cost 0.0243928  validation cost 2.5052\n",
      "training accuracy 1  validation accuracy 0.6208\n",
      "Epoch 92, CIFAR-10 Batch 4:  training cost 0.0199708  validation cost 2.41907\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 92, CIFAR-10 Batch 5:  training cost 0.0308019  validation cost 2.46489\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 93, CIFAR-10 Batch 1:  training cost 0.00836999  validation cost 2.64492\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 93, CIFAR-10 Batch 2:  training cost 0.0133702  validation cost 2.43657\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 93, CIFAR-10 Batch 3:  training cost 0.00270488  validation cost 2.45573\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 93, CIFAR-10 Batch 4:  training cost 0.00314716  validation cost 2.48963\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 93, CIFAR-10 Batch 5:  training cost 0.00928394  validation cost 2.55211\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 94, CIFAR-10 Batch 1:  training cost 0.000968181  validation cost 2.55307\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 94, CIFAR-10 Batch 2:  training cost 0.00292043  validation cost 2.51821\n",
      "training accuracy 1  validation accuracy 0.6186\n",
      "Epoch 94, CIFAR-10 Batch 3:  training cost 0.0117146  validation cost 2.43366\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 94, CIFAR-10 Batch 4:  training cost 0.00701528  validation cost 2.42738\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 94, CIFAR-10 Batch 5:  training cost 0.00814062  validation cost 2.50941\n",
      "training accuracy 1  validation accuracy 0.6186\n",
      "Epoch 95, CIFAR-10 Batch 1:  training cost 0.0171241  validation cost 2.51893\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 95, CIFAR-10 Batch 2:  training cost 0.0122656  validation cost 2.45711\n",
      "training accuracy 1  validation accuracy 0.6178\n",
      "Epoch 95, CIFAR-10 Batch 3:  training cost 0.00195757  validation cost 2.43499\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 95, CIFAR-10 Batch 4:  training cost 0.0090958  validation cost 2.36789\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 95, CIFAR-10 Batch 5:  training cost 0.012291  validation cost 2.5771\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 96, CIFAR-10 Batch 1:  training cost 0.0116265  validation cost 2.51657\n",
      "training accuracy 1  validation accuracy 0.6174\n",
      "Epoch 96, CIFAR-10 Batch 2:  training cost 0.0803388  validation cost 2.47833\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 96, CIFAR-10 Batch 3:  training cost 0.000628147  validation cost 2.50213\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 96, CIFAR-10 Batch 4:  training cost 0.021026  validation cost 2.36463\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 96, CIFAR-10 Batch 5:  training cost 0.00765563  validation cost 2.61535\n",
      "training accuracy 1  validation accuracy 0.6136\n",
      "Epoch 97, CIFAR-10 Batch 1:  training cost 0.00299941  validation cost 2.5847\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 97, CIFAR-10 Batch 2:  training cost 0.00969429  validation cost 2.49086\n",
      "training accuracy 1  validation accuracy 0.6158\n",
      "Epoch 97, CIFAR-10 Batch 3:  training cost 0.0129425  validation cost 2.4772\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 97, CIFAR-10 Batch 4:  training cost 0.00741991  validation cost 2.49968\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 97, CIFAR-10 Batch 5:  training cost 0.0134348  validation cost 2.56588\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 98, CIFAR-10 Batch 1:  training cost 0.00552545  validation cost 2.50324\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 98, CIFAR-10 Batch 2:  training cost 0.0270081  validation cost 2.41938\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 98, CIFAR-10 Batch 3:  training cost 0.033744  validation cost 2.49728\n",
      "training accuracy 1  validation accuracy 0.6114\n",
      "Epoch 98, CIFAR-10 Batch 4:  training cost 0.0193037  validation cost 2.35647\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 98, CIFAR-10 Batch 5:  training cost 0.0143412  validation cost 2.57677\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 99, CIFAR-10 Batch 1:  training cost 0.00555102  validation cost 2.51937\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 99, CIFAR-10 Batch 2:  training cost 0.000161774  validation cost 2.50402\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 99, CIFAR-10 Batch 3:  training cost 0.00192291  validation cost 2.39606\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 99, CIFAR-10 Batch 4:  training cost 0.00761556  validation cost 2.49359\n",
      "training accuracy 1  validation accuracy 0.6186\n",
      "Epoch 99, CIFAR-10 Batch 5:  training cost 0.00406301  validation cost 2.55311\n",
      "training accuracy 1  validation accuracy 0.6214\n",
      "Epoch 100, CIFAR-10 Batch 1:  training cost 0.00645983  validation cost 2.59653\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 100, CIFAR-10 Batch 2:  training cost 0.0141889  validation cost 2.49844\n",
      "training accuracy 1  validation accuracy 0.6206\n",
      "Epoch 100, CIFAR-10 Batch 3:  training cost 0.000586475  validation cost 2.55845\n",
      "training accuracy 1  validation accuracy 0.6134\n",
      "Epoch 100, CIFAR-10 Batch 4:  training cost 0.00746906  validation cost 2.4825\n",
      "training accuracy 1  validation accuracy 0.6174\n",
      "Epoch 100, CIFAR-10 Batch 5:  training cost 0.00213184  validation cost 2.70582\n",
      "training accuracy 1  validation accuracy 0.6186\n",
      "Epoch 101, CIFAR-10 Batch 1:  training cost 0.0341071  validation cost 2.73715\n",
      "training accuracy 1  validation accuracy 0.619\n",
      "Epoch 101, CIFAR-10 Batch 2:  training cost 0.0075368  validation cost 2.42471\n",
      "training accuracy 1  validation accuracy 0.6182\n",
      "Epoch 101, CIFAR-10 Batch 3:  training cost 0.000628184  validation cost 2.56553\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 101, CIFAR-10 Batch 4:  training cost 0.00627337  validation cost 2.45665\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 101, CIFAR-10 Batch 5:  training cost 0.0084266  validation cost 2.64461\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 102, CIFAR-10 Batch 1:  training cost 0.0133829  validation cost 2.51579\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 102, CIFAR-10 Batch 2:  training cost 0.0223675  validation cost 2.58187\n",
      "training accuracy 1  validation accuracy 0.6174\n",
      "Epoch 102, CIFAR-10 Batch 3:  training cost 0.001568  validation cost 2.5652\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 102, CIFAR-10 Batch 4:  training cost 0.00604726  validation cost 2.5431\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 102, CIFAR-10 Batch 5:  training cost 0.00758042  validation cost 2.53985\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 103, CIFAR-10 Batch 1:  training cost 0.00141355  validation cost 2.63105\n",
      "training accuracy 1  validation accuracy 0.6204\n",
      "Epoch 103, CIFAR-10 Batch 2:  training cost 0.00273304  validation cost 2.53783\n",
      "training accuracy 1  validation accuracy 0.6176\n",
      "Epoch 103, CIFAR-10 Batch 3:  training cost 0.00220404  validation cost 2.48835\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 103, CIFAR-10 Batch 4:  training cost 0.000890064  validation cost 2.61975\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 103, CIFAR-10 Batch 5:  training cost 0.0199414  validation cost 2.49377\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 104, CIFAR-10 Batch 1:  training cost 0.0102102  validation cost 2.63995\n",
      "training accuracy 1  validation accuracy 0.621\n",
      "Epoch 104, CIFAR-10 Batch 2:  training cost 0.00921385  validation cost 2.68123\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 104, CIFAR-10 Batch 3:  training cost 0.0116521  validation cost 2.7042\n",
      "training accuracy 1  validation accuracy 0.6182\n",
      "Epoch 104, CIFAR-10 Batch 4:  training cost 0.0149902  validation cost 2.50612\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 104, CIFAR-10 Batch 5:  training cost 0.00672171  validation cost 2.57137\n",
      "training accuracy 1  validation accuracy 0.6214\n",
      "Epoch 105, CIFAR-10 Batch 1:  training cost 0.00129904  validation cost 2.72968\n",
      "training accuracy 1  validation accuracy 0.6166\n",
      "Epoch 105, CIFAR-10 Batch 2:  training cost 0.00121428  validation cost 2.55189\n",
      "training accuracy 1  validation accuracy 0.6252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105, CIFAR-10 Batch 3:  training cost 0.00747714  validation cost 2.5653\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 105, CIFAR-10 Batch 4:  training cost 0.00274515  validation cost 2.51031\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 105, CIFAR-10 Batch 5:  training cost 0.00220621  validation cost 2.62199\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 106, CIFAR-10 Batch 1:  training cost 0.00236024  validation cost 2.75339\n",
      "training accuracy 1  validation accuracy 0.6204\n",
      "Epoch 106, CIFAR-10 Batch 2:  training cost 0.00361681  validation cost 2.58986\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 106, CIFAR-10 Batch 3:  training cost 0.00810164  validation cost 2.57278\n",
      "training accuracy 1  validation accuracy 0.6212\n",
      "Epoch 106, CIFAR-10 Batch 4:  training cost 0.00293208  validation cost 2.55543\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 106, CIFAR-10 Batch 5:  training cost 0.0164209  validation cost 2.5971\n",
      "training accuracy 1  validation accuracy 0.6208\n",
      "Epoch 107, CIFAR-10 Batch 1:  training cost 0.00717498  validation cost 2.75925\n",
      "training accuracy 1  validation accuracy 0.6212\n",
      "Epoch 107, CIFAR-10 Batch 2:  training cost 0.00673078  validation cost 2.73266\n",
      "training accuracy 1  validation accuracy 0.617\n",
      "Epoch 107, CIFAR-10 Batch 3:  training cost 0.00869827  validation cost 2.52467\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 107, CIFAR-10 Batch 4:  training cost 0.0111471  validation cost 2.5502\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 107, CIFAR-10 Batch 5:  training cost 0.00594786  validation cost 2.70749\n",
      "training accuracy 1  validation accuracy 0.62\n",
      "Epoch 108, CIFAR-10 Batch 1:  training cost 0.0119302  validation cost 2.48446\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 108, CIFAR-10 Batch 2:  training cost 0.000667979  validation cost 2.57183\n",
      "training accuracy 1  validation accuracy 0.6188\n",
      "Epoch 108, CIFAR-10 Batch 3:  training cost 0.00170105  validation cost 2.57767\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 108, CIFAR-10 Batch 4:  training cost 0.0132271  validation cost 2.56074\n",
      "training accuracy 1  validation accuracy 0.614\n",
      "Epoch 108, CIFAR-10 Batch 5:  training cost 0.00440716  validation cost 2.56744\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 109, CIFAR-10 Batch 1:  training cost 0.0028229  validation cost 2.575\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 109, CIFAR-10 Batch 2:  training cost 0.00295008  validation cost 2.71322\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 109, CIFAR-10 Batch 3:  training cost 0.000682552  validation cost 2.59809\n",
      "training accuracy 1  validation accuracy 0.6214\n",
      "Epoch 109, CIFAR-10 Batch 4:  training cost 0.00552668  validation cost 2.59607\n",
      "training accuracy 1  validation accuracy 0.6198\n",
      "Epoch 109, CIFAR-10 Batch 5:  training cost 0.00569924  validation cost 2.71443\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 110, CIFAR-10 Batch 1:  training cost 0.00688198  validation cost 2.72404\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 110, CIFAR-10 Batch 2:  training cost 0.00479355  validation cost 2.67976\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 110, CIFAR-10 Batch 3:  training cost 0.00085421  validation cost 2.75608\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 110, CIFAR-10 Batch 4:  training cost 0.00892816  validation cost 2.71248\n",
      "training accuracy 1  validation accuracy 0.6174\n",
      "Epoch 110, CIFAR-10 Batch 5:  training cost 0.0052781  validation cost 2.71635\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 111, CIFAR-10 Batch 1:  training cost 0.000767527  validation cost 2.80562\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 111, CIFAR-10 Batch 2:  training cost 0.0114726  validation cost 2.66271\n",
      "training accuracy 1  validation accuracy 0.6138\n",
      "Epoch 111, CIFAR-10 Batch 3:  training cost 0.00568921  validation cost 2.5497\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 111, CIFAR-10 Batch 4:  training cost 0.00728302  validation cost 2.58911\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 111, CIFAR-10 Batch 5:  training cost 0.0125292  validation cost 2.65724\n",
      "training accuracy 1  validation accuracy 0.6208\n",
      "Epoch 112, CIFAR-10 Batch 1:  training cost 0.000869925  validation cost 2.70052\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 112, CIFAR-10 Batch 2:  training cost 0.00242338  validation cost 2.73643\n",
      "training accuracy 1  validation accuracy 0.6168\n",
      "Epoch 112, CIFAR-10 Batch 3:  training cost 0.00129508  validation cost 2.67003\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 112, CIFAR-10 Batch 4:  training cost 0.00836463  validation cost 2.61959\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 112, CIFAR-10 Batch 5:  training cost 0.00194598  validation cost 2.75536\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 113, CIFAR-10 Batch 1:  training cost 0.013389  validation cost 2.732\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 113, CIFAR-10 Batch 2:  training cost 0.00535684  validation cost 2.61828\n",
      "training accuracy 1  validation accuracy 0.62\n",
      "Epoch 113, CIFAR-10 Batch 3:  training cost 0.00074364  validation cost 2.69582\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 113, CIFAR-10 Batch 4:  training cost 0.00350869  validation cost 2.63212\n",
      "training accuracy 1  validation accuracy 0.6114\n",
      "Epoch 113, CIFAR-10 Batch 5:  training cost 0.00263736  validation cost 2.70893\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 114, CIFAR-10 Batch 1:  training cost 0.00240193  validation cost 2.69564\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 114, CIFAR-10 Batch 2:  training cost 0.000975824  validation cost 2.75721\n",
      "training accuracy 1  validation accuracy 0.6196\n",
      "Epoch 114, CIFAR-10 Batch 3:  training cost 0.000333286  validation cost 2.6926\n",
      "training accuracy 1  validation accuracy 0.6196\n",
      "Epoch 114, CIFAR-10 Batch 4:  training cost 0.00512294  validation cost 2.54239\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 114, CIFAR-10 Batch 5:  training cost 0.000410651  validation cost 2.77084\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 115, CIFAR-10 Batch 1:  training cost 0.00744284  validation cost 2.68888\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 115, CIFAR-10 Batch 2:  training cost 0.00366766  validation cost 2.6088\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 115, CIFAR-10 Batch 3:  training cost 0.000735266  validation cost 2.67068\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 115, CIFAR-10 Batch 4:  training cost 0.00331935  validation cost 2.61173\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 115, CIFAR-10 Batch 5:  training cost 0.00111536  validation cost 2.65055\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 116, CIFAR-10 Batch 1:  training cost 0.0145764  validation cost 2.73972\n",
      "training accuracy 1  validation accuracy 0.621\n",
      "Epoch 116, CIFAR-10 Batch 2:  training cost 0.000613177  validation cost 2.75842\n",
      "training accuracy 1  validation accuracy 0.615\n",
      "Epoch 116, CIFAR-10 Batch 3:  training cost 0.0142264  validation cost 2.78296\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 116, CIFAR-10 Batch 4:  training cost 0.0101021  validation cost 2.68647\n",
      "training accuracy 1  validation accuracy 0.6176\n",
      "Epoch 116, CIFAR-10 Batch 5:  training cost 0.00183003  validation cost 2.62212\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 117, CIFAR-10 Batch 1:  training cost 0.00336522  validation cost 2.67182\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 117, CIFAR-10 Batch 2:  training cost 0.0190482  validation cost 2.6689\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 117, CIFAR-10 Batch 3:  training cost 0.000762156  validation cost 2.66965\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 117, CIFAR-10 Batch 4:  training cost 0.00299765  validation cost 2.6831\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 117, CIFAR-10 Batch 5:  training cost 0.00296348  validation cost 2.72127\n",
      "training accuracy 1  validation accuracy 0.619\n",
      "Epoch 118, CIFAR-10 Batch 1:  training cost 0.00344659  validation cost 2.70342\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 118, CIFAR-10 Batch 2:  training cost 0.000763069  validation cost 2.66015\n",
      "training accuracy 1  validation accuracy 0.6158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, CIFAR-10 Batch 3:  training cost 0.00151421  validation cost 2.65133\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 118, CIFAR-10 Batch 4:  training cost 0.00874715  validation cost 2.55344\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 118, CIFAR-10 Batch 5:  training cost 0.00153203  validation cost 2.80319\n",
      "training accuracy 1  validation accuracy 0.6194\n",
      "Epoch 119, CIFAR-10 Batch 1:  training cost 0.00132832  validation cost 2.74725\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 119, CIFAR-10 Batch 2:  training cost 0.00966594  validation cost 2.69458\n",
      "training accuracy 1  validation accuracy 0.6142\n",
      "Epoch 119, CIFAR-10 Batch 3:  training cost 0.000594874  validation cost 2.66072\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 119, CIFAR-10 Batch 4:  training cost 0.00319172  validation cost 2.64059\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 119, CIFAR-10 Batch 5:  training cost 0.00556003  validation cost 2.7206\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 120, CIFAR-10 Batch 1:  training cost 0.00103243  validation cost 2.69195\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 120, CIFAR-10 Batch 2:  training cost 0.00274973  validation cost 2.64163\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 120, CIFAR-10 Batch 3:  training cost 0.000861091  validation cost 2.77928\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 120, CIFAR-10 Batch 4:  training cost 0.0102198  validation cost 2.57091\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 120, CIFAR-10 Batch 5:  training cost 0.00654958  validation cost 2.79072\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 121, CIFAR-10 Batch 1:  training cost 0.00155512  validation cost 2.70601\n",
      "training accuracy 1  validation accuracy 0.6138\n",
      "Epoch 121, CIFAR-10 Batch 2:  training cost 0.00201216  validation cost 2.61699\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 121, CIFAR-10 Batch 3:  training cost 0.00015619  validation cost 2.75084\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 121, CIFAR-10 Batch 4:  training cost 0.00588588  validation cost 2.61996\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 121, CIFAR-10 Batch 5:  training cost 0.00203518  validation cost 2.75145\n",
      "training accuracy 1  validation accuracy 0.621\n",
      "Epoch 122, CIFAR-10 Batch 1:  training cost 0.00951164  validation cost 2.78329\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 122, CIFAR-10 Batch 2:  training cost 0.00142622  validation cost 2.67351\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 122, CIFAR-10 Batch 3:  training cost 0.0012605  validation cost 2.72972\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 122, CIFAR-10 Batch 4:  training cost 0.00419461  validation cost 2.57522\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 122, CIFAR-10 Batch 5:  training cost 0.00511146  validation cost 2.61927\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 123, CIFAR-10 Batch 1:  training cost 0.0428129  validation cost 2.69947\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 123, CIFAR-10 Batch 2:  training cost 0.000153192  validation cost 2.65149\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 123, CIFAR-10 Batch 3:  training cost 0.000528386  validation cost 2.7103\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 123, CIFAR-10 Batch 4:  training cost 0.00463536  validation cost 2.68426\n",
      "training accuracy 1  validation accuracy 0.6176\n",
      "Epoch 123, CIFAR-10 Batch 5:  training cost 0.0146839  validation cost 2.71794\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 124, CIFAR-10 Batch 1:  training cost 0.0017583  validation cost 2.86651\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 124, CIFAR-10 Batch 2:  training cost 0.000206998  validation cost 2.70826\n",
      "training accuracy 1  validation accuracy 0.6212\n",
      "Epoch 124, CIFAR-10 Batch 3:  training cost 0.00141741  validation cost 2.68056\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 124, CIFAR-10 Batch 4:  training cost 0.0113494  validation cost 2.57244\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 124, CIFAR-10 Batch 5:  training cost 0.00335265  validation cost 2.70387\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 125, CIFAR-10 Batch 1:  training cost 0.0142071  validation cost 2.58988\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 125, CIFAR-10 Batch 2:  training cost 3.76821e-05  validation cost 2.62082\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 125, CIFAR-10 Batch 3:  training cost 0.000850055  validation cost 2.77165\n",
      "training accuracy 1  validation accuracy 0.6206\n",
      "Epoch 125, CIFAR-10 Batch 4:  training cost 0.0182725  validation cost 2.68412\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 125, CIFAR-10 Batch 5:  training cost 0.00213055  validation cost 2.8923\n",
      "training accuracy 1  validation accuracy 0.62\n",
      "Epoch 126, CIFAR-10 Batch 1:  training cost 0.000823911  validation cost 2.82307\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 126, CIFAR-10 Batch 2:  training cost 4.65155e-05  validation cost 2.83177\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 126, CIFAR-10 Batch 3:  training cost 0.00117557  validation cost 2.66835\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 126, CIFAR-10 Batch 4:  training cost 0.00224608  validation cost 2.83854\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 126, CIFAR-10 Batch 5:  training cost 0.00471533  validation cost 2.85061\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 127, CIFAR-10 Batch 1:  training cost 0.0135584  validation cost 2.71286\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 127, CIFAR-10 Batch 2:  training cost 0.000395679  validation cost 2.66839\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 127, CIFAR-10 Batch 3:  training cost 0.000862732  validation cost 2.73229\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 127, CIFAR-10 Batch 4:  training cost 0.00654987  validation cost 2.70618\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 127, CIFAR-10 Batch 5:  training cost 0.00428935  validation cost 2.89537\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 128, CIFAR-10 Batch 1:  training cost 0.00882098  validation cost 2.59475\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 128, CIFAR-10 Batch 2:  training cost 5.87502e-05  validation cost 2.61605\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 128, CIFAR-10 Batch 3:  training cost 0.00824422  validation cost 2.67109\n",
      "training accuracy 1  validation accuracy 0.621\n",
      "Epoch 128, CIFAR-10 Batch 4:  training cost 0.00287784  validation cost 2.52781\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 128, CIFAR-10 Batch 5:  training cost 0.0039239  validation cost 2.78106\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 129, CIFAR-10 Batch 1:  training cost 0.0232342  validation cost 2.72258\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 129, CIFAR-10 Batch 2:  training cost 0.000626092  validation cost 2.67283\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 129, CIFAR-10 Batch 3:  training cost 0.000340673  validation cost 2.68142\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 129, CIFAR-10 Batch 4:  training cost 0.00157855  validation cost 2.64873\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 129, CIFAR-10 Batch 5:  training cost 0.0126737  validation cost 2.64728\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 130, CIFAR-10 Batch 1:  training cost 0.00163737  validation cost 2.79347\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 130, CIFAR-10 Batch 2:  training cost 0.00086578  validation cost 2.74766\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 130, CIFAR-10 Batch 3:  training cost 0.0106971  validation cost 2.69674\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 130, CIFAR-10 Batch 4:  training cost 0.00788452  validation cost 2.57533\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 130, CIFAR-10 Batch 5:  training cost 0.0026052  validation cost 2.80847\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 131, CIFAR-10 Batch 1:  training cost 0.00067954  validation cost 2.66035\n",
      "training accuracy 1  validation accuracy 0.632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, CIFAR-10 Batch 2:  training cost 0.000325776  validation cost 2.69629\n",
      "training accuracy 1  validation accuracy 0.6156\n",
      "Epoch 131, CIFAR-10 Batch 3:  training cost 0.00045117  validation cost 2.65819\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 131, CIFAR-10 Batch 4:  training cost 0.0108796  validation cost 2.5789\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 131, CIFAR-10 Batch 5:  training cost 0.000904623  validation cost 2.59743\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 132, CIFAR-10 Batch 1:  training cost 0.0090938  validation cost 2.73937\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 132, CIFAR-10 Batch 2:  training cost 0.025261  validation cost 2.80199\n",
      "training accuracy 1  validation accuracy 0.6206\n",
      "Epoch 132, CIFAR-10 Batch 3:  training cost 0.000601089  validation cost 2.73214\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 132, CIFAR-10 Batch 4:  training cost 0.00251369  validation cost 2.73359\n",
      "training accuracy 1  validation accuracy 0.621\n",
      "Epoch 132, CIFAR-10 Batch 5:  training cost 0.003051  validation cost 2.83723\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 133, CIFAR-10 Batch 1:  training cost 0.000445624  validation cost 2.77782\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 133, CIFAR-10 Batch 2:  training cost 0.00365448  validation cost 2.62996\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 133, CIFAR-10 Batch 3:  training cost 0.000296883  validation cost 2.82653\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 133, CIFAR-10 Batch 4:  training cost 0.00763554  validation cost 2.82141\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 133, CIFAR-10 Batch 5:  training cost 0.00999401  validation cost 2.7893\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 134, CIFAR-10 Batch 1:  training cost 0.00373504  validation cost 2.70572\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 134, CIFAR-10 Batch 2:  training cost 0.000684684  validation cost 2.87849\n",
      "training accuracy 1  validation accuracy 0.6068\n",
      "Epoch 134, CIFAR-10 Batch 3:  training cost 0.000129906  validation cost 2.66118\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 134, CIFAR-10 Batch 4:  training cost 0.0019559  validation cost 2.76289\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 134, CIFAR-10 Batch 5:  training cost 0.00110075  validation cost 2.92353\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 135, CIFAR-10 Batch 1:  training cost 0.000256123  validation cost 2.89828\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 135, CIFAR-10 Batch 2:  training cost 0.00653047  validation cost 2.81089\n",
      "training accuracy 1  validation accuracy 0.616\n",
      "Epoch 135, CIFAR-10 Batch 3:  training cost 0.000170359  validation cost 2.88252\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 135, CIFAR-10 Batch 4:  training cost 0.00142597  validation cost 2.70883\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 135, CIFAR-10 Batch 5:  training cost 0.00613469  validation cost 2.67218\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 136, CIFAR-10 Batch 1:  training cost 0.00512371  validation cost 2.69773\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 136, CIFAR-10 Batch 2:  training cost 0.00471923  validation cost 2.81028\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 136, CIFAR-10 Batch 3:  training cost 0.000563048  validation cost 2.73474\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 136, CIFAR-10 Batch 4:  training cost 0.00230049  validation cost 2.62242\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 136, CIFAR-10 Batch 5:  training cost 0.0198135  validation cost 2.75674\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 137, CIFAR-10 Batch 1:  training cost 0.00342009  validation cost 2.87338\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 137, CIFAR-10 Batch 2:  training cost 0.00417565  validation cost 2.78562\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 137, CIFAR-10 Batch 3:  training cost 0.00125609  validation cost 2.82977\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 137, CIFAR-10 Batch 4:  training cost 0.0188486  validation cost 2.61735\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 137, CIFAR-10 Batch 5:  training cost 0.0388087  validation cost 2.84283\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 138, CIFAR-10 Batch 1:  training cost 0.00250844  validation cost 2.8755\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 138, CIFAR-10 Batch 2:  training cost 0.00896936  validation cost 2.90272\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 138, CIFAR-10 Batch 3:  training cost 0.000497674  validation cost 2.819\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 138, CIFAR-10 Batch 4:  training cost 0.00991723  validation cost 2.76526\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 138, CIFAR-10 Batch 5:  training cost 0.00260273  validation cost 2.8731\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 139, CIFAR-10 Batch 1:  training cost 0.0063167  validation cost 3.01253\n",
      "training accuracy 1  validation accuracy 0.6154\n",
      "Epoch 139, CIFAR-10 Batch 2:  training cost 0.00129193  validation cost 2.84499\n",
      "training accuracy 1  validation accuracy 0.621\n",
      "Epoch 139, CIFAR-10 Batch 3:  training cost 0.000611621  validation cost 2.67292\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 139, CIFAR-10 Batch 4:  training cost 0.0154585  validation cost 2.66994\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 139, CIFAR-10 Batch 5:  training cost 0.00320949  validation cost 2.96561\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 140, CIFAR-10 Batch 1:  training cost 0.00692437  validation cost 2.99388\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 140, CIFAR-10 Batch 2:  training cost 0.000132811  validation cost 2.86997\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 140, CIFAR-10 Batch 3:  training cost 0.000232055  validation cost 2.95457\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 140, CIFAR-10 Batch 4:  training cost 0.0103038  validation cost 2.7186\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 140, CIFAR-10 Batch 5:  training cost 0.00123813  validation cost 2.86311\n",
      "training accuracy 1  validation accuracy 0.6202\n",
      "Epoch 141, CIFAR-10 Batch 1:  training cost 0.00262994  validation cost 2.74552\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 141, CIFAR-10 Batch 2:  training cost 0.00406639  validation cost 2.89135\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 141, CIFAR-10 Batch 3:  training cost 0.0123523  validation cost 2.84138\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 141, CIFAR-10 Batch 4:  training cost 4.79928e-05  validation cost 2.76556\n",
      "training accuracy 1  validation accuracy 0.6204\n",
      "Epoch 141, CIFAR-10 Batch 5:  training cost 0.00119185  validation cost 2.76953\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 142, CIFAR-10 Batch 1:  training cost 0.000806567  validation cost 2.96706\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 142, CIFAR-10 Batch 2:  training cost 0.000679314  validation cost 2.81235\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 142, CIFAR-10 Batch 3:  training cost 0.000741915  validation cost 2.80615\n",
      "training accuracy 1  validation accuracy 0.641\n",
      "Epoch 142, CIFAR-10 Batch 4:  training cost 0.00109598  validation cost 2.72338\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 142, CIFAR-10 Batch 5:  training cost 0.00294402  validation cost 2.91429\n",
      "training accuracy 1  validation accuracy 0.619\n",
      "Epoch 143, CIFAR-10 Batch 1:  training cost 0.0530423  validation cost 2.81934\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 143, CIFAR-10 Batch 2:  training cost 0.018474  validation cost 2.79175\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 143, CIFAR-10 Batch 3:  training cost 0.00914878  validation cost 2.8128\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 143, CIFAR-10 Batch 4:  training cost 0.00118481  validation cost 2.7528\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 143, CIFAR-10 Batch 5:  training cost 0.000408893  validation cost 2.90305\n",
      "training accuracy 1  validation accuracy 0.622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144, CIFAR-10 Batch 1:  training cost 0.000832995  validation cost 2.7963\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 144, CIFAR-10 Batch 2:  training cost 0.00220663  validation cost 2.89678\n",
      "training accuracy 1  validation accuracy 0.6208\n",
      "Epoch 144, CIFAR-10 Batch 3:  training cost 0.00464556  validation cost 2.93828\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 144, CIFAR-10 Batch 4:  training cost 0.00134584  validation cost 2.7728\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 144, CIFAR-10 Batch 5:  training cost 0.00374423  validation cost 2.87\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 145, CIFAR-10 Batch 1:  training cost 0.00671792  validation cost 2.85538\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 145, CIFAR-10 Batch 2:  training cost 0.00259579  validation cost 2.77018\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 145, CIFAR-10 Batch 3:  training cost 0.00148768  validation cost 3.00781\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 145, CIFAR-10 Batch 4:  training cost 2.53908e-05  validation cost 2.87227\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 145, CIFAR-10 Batch 5:  training cost 0.00118508  validation cost 2.95596\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 146, CIFAR-10 Batch 1:  training cost 0.00340036  validation cost 2.87885\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 146, CIFAR-10 Batch 2:  training cost 0.000818142  validation cost 2.77306\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 146, CIFAR-10 Batch 3:  training cost 0.000476686  validation cost 2.78885\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 146, CIFAR-10 Batch 4:  training cost 0.00201102  validation cost 2.83462\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 146, CIFAR-10 Batch 5:  training cost 0.000606526  validation cost 3.15432\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 147, CIFAR-10 Batch 1:  training cost 0.00534274  validation cost 2.86315\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 147, CIFAR-10 Batch 2:  training cost 0.00102281  validation cost 2.86982\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 147, CIFAR-10 Batch 3:  training cost 0.00159073  validation cost 2.82473\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 147, CIFAR-10 Batch 4:  training cost 0.000943313  validation cost 2.71151\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 147, CIFAR-10 Batch 5:  training cost 0.000201719  validation cost 2.88627\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 148, CIFAR-10 Batch 1:  training cost 0.000390211  validation cost 2.84262\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 148, CIFAR-10 Batch 2:  training cost 0.000586541  validation cost 2.93798\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 148, CIFAR-10 Batch 3:  training cost 0.00130149  validation cost 3.01238\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 148, CIFAR-10 Batch 4:  training cost 0.00235156  validation cost 2.87804\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 148, CIFAR-10 Batch 5:  training cost 0.00457403  validation cost 3.00738\n",
      "training accuracy 1  validation accuracy 0.6186\n",
      "Epoch 149, CIFAR-10 Batch 1:  training cost 0.00109576  validation cost 2.95975\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 149, CIFAR-10 Batch 2:  training cost 0.00720587  validation cost 2.85394\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 149, CIFAR-10 Batch 3:  training cost 0.000580358  validation cost 2.76279\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 149, CIFAR-10 Batch 4:  training cost 0.00391598  validation cost 2.78687\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 149, CIFAR-10 Batch 5:  training cost 0.000632759  validation cost 2.88691\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 150, CIFAR-10 Batch 1:  training cost 0.00115946  validation cost 2.73889\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 150, CIFAR-10 Batch 2:  training cost 0.000763282  validation cost 2.34738\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 150, CIFAR-10 Batch 3:  training cost 0.00205322  validation cost 2.67841\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 150, CIFAR-10 Batch 4:  training cost 0.00280312  validation cost 2.70948\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 150, CIFAR-10 Batch 5:  training cost 0.00277967  validation cost 2.84479\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 151, CIFAR-10 Batch 1:  training cost 0.00257098  validation cost 2.80748\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 151, CIFAR-10 Batch 2:  training cost 0.0124055  validation cost 2.75846\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 151, CIFAR-10 Batch 3:  training cost 0.000916835  validation cost 2.88705\n",
      "training accuracy 1  validation accuracy 0.6386\n",
      "Epoch 151, CIFAR-10 Batch 4:  training cost 0.000537979  validation cost 2.89785\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 151, CIFAR-10 Batch 5:  training cost 0.00269519  validation cost 2.69134\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 152, CIFAR-10 Batch 1:  training cost 0.00324194  validation cost 2.70887\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 152, CIFAR-10 Batch 2:  training cost 0.00468837  validation cost 2.81799\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 152, CIFAR-10 Batch 3:  training cost 0.00118198  validation cost 3.06252\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 152, CIFAR-10 Batch 4:  training cost 0.000451891  validation cost 2.9438\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 152, CIFAR-10 Batch 5:  training cost 0.00116382  validation cost 2.93341\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 153, CIFAR-10 Batch 1:  training cost 0.106269  validation cost 2.9423\n",
      "training accuracy 0.875  validation accuracy 0.626\n",
      "Epoch 153, CIFAR-10 Batch 2:  training cost 0.00212292  validation cost 2.76926\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 153, CIFAR-10 Batch 3:  training cost 9.1791e-05  validation cost 2.91801\n",
      "training accuracy 1  validation accuracy 0.6202\n",
      "Epoch 153, CIFAR-10 Batch 4:  training cost 0.00713662  validation cost 2.80856\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 153, CIFAR-10 Batch 5:  training cost 0.00309504  validation cost 2.86966\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 154, CIFAR-10 Batch 1:  training cost 0.000735204  validation cost 2.82115\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 154, CIFAR-10 Batch 2:  training cost 0.00725191  validation cost 2.93124\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 154, CIFAR-10 Batch 3:  training cost 6.4901e-05  validation cost 2.87781\n",
      "training accuracy 1  validation accuracy 0.6374\n",
      "Epoch 154, CIFAR-10 Batch 4:  training cost 0.000869927  validation cost 2.81249\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 154, CIFAR-10 Batch 5:  training cost 0.00974318  validation cost 2.8648\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 155, CIFAR-10 Batch 1:  training cost 0.00341197  validation cost 2.89547\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 155, CIFAR-10 Batch 2:  training cost 0.00688634  validation cost 2.87845\n",
      "training accuracy 1  validation accuracy 0.615\n",
      "Epoch 155, CIFAR-10 Batch 3:  training cost 0.000285694  validation cost 2.87929\n",
      "training accuracy 1  validation accuracy 0.6398\n",
      "Epoch 155, CIFAR-10 Batch 4:  training cost 0.000733303  validation cost 2.86287\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 155, CIFAR-10 Batch 5:  training cost 0.0613567  validation cost 2.93669\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 156, CIFAR-10 Batch 1:  training cost 0.00300288  validation cost 2.94557\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 156, CIFAR-10 Batch 2:  training cost 0.000171453  validation cost 2.87167\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 156, CIFAR-10 Batch 3:  training cost 0.000118137  validation cost 3.11705\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 156, CIFAR-10 Batch 4:  training cost 0.0026247  validation cost 2.68938\n",
      "training accuracy 1  validation accuracy 0.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156, CIFAR-10 Batch 5:  training cost 0.00251298  validation cost 2.76121\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 157, CIFAR-10 Batch 1:  training cost 0.0215535  validation cost 2.48278\n",
      "training accuracy 1  validation accuracy 0.6092\n",
      "Epoch 157, CIFAR-10 Batch 2:  training cost 0.00995123  validation cost 2.62512\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 157, CIFAR-10 Batch 3:  training cost 9.71604e-05  validation cost 2.853\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 157, CIFAR-10 Batch 4:  training cost 0.00459918  validation cost 2.8441\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 157, CIFAR-10 Batch 5:  training cost 0.00176796  validation cost 2.97889\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 158, CIFAR-10 Batch 1:  training cost 0.00221184  validation cost 2.9433\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 158, CIFAR-10 Batch 2:  training cost 0.00237757  validation cost 2.91851\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 158, CIFAR-10 Batch 3:  training cost 6.55709e-05  validation cost 2.91347\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 158, CIFAR-10 Batch 4:  training cost 0.00833485  validation cost 2.96939\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 158, CIFAR-10 Batch 5:  training cost 0.00525538  validation cost 3.04839\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 159, CIFAR-10 Batch 1:  training cost 0.00104745  validation cost 3.0504\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 159, CIFAR-10 Batch 2:  training cost 0.00615967  validation cost 2.97718\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 159, CIFAR-10 Batch 3:  training cost 0.000137084  validation cost 2.9523\n",
      "training accuracy 1  validation accuracy 0.638\n",
      "Epoch 159, CIFAR-10 Batch 4:  training cost 0.000256025  validation cost 2.9435\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 159, CIFAR-10 Batch 5:  training cost 0.0129184  validation cost 3.0518\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 160, CIFAR-10 Batch 1:  training cost 0.00289946  validation cost 2.89079\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 160, CIFAR-10 Batch 2:  training cost 0.000145944  validation cost 2.94698\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 160, CIFAR-10 Batch 3:  training cost 0.000372263  validation cost 2.73383\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 160, CIFAR-10 Batch 4:  training cost 0.000383372  validation cost 2.85444\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 160, CIFAR-10 Batch 5:  training cost 0.00235497  validation cost 3.09022\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 161, CIFAR-10 Batch 1:  training cost 0.00140736  validation cost 2.92238\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 161, CIFAR-10 Batch 2:  training cost 0.0351004  validation cost 2.69636\n",
      "training accuracy 1  validation accuracy 0.6154\n",
      "Epoch 161, CIFAR-10 Batch 3:  training cost 0.000743919  validation cost 2.79622\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 161, CIFAR-10 Batch 4:  training cost 0.00215893  validation cost 2.81083\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 161, CIFAR-10 Batch 5:  training cost 0.000378343  validation cost 2.7953\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 162, CIFAR-10 Batch 1:  training cost 0.00473581  validation cost 2.79582\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 162, CIFAR-10 Batch 2:  training cost 0.00923936  validation cost 3.03772\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 162, CIFAR-10 Batch 3:  training cost 0.0026031  validation cost 2.97354\n",
      "training accuracy 1  validation accuracy 0.6386\n",
      "Epoch 162, CIFAR-10 Batch 4:  training cost 0.00447177  validation cost 2.93277\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 162, CIFAR-10 Batch 5:  training cost 0.000146313  validation cost 3.04587\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 163, CIFAR-10 Batch 1:  training cost 0.0063607  validation cost 2.38251\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 163, CIFAR-10 Batch 2:  training cost 0.000155479  validation cost 2.63778\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 163, CIFAR-10 Batch 3:  training cost 7.10577e-05  validation cost 2.7731\n",
      "training accuracy 1  validation accuracy 0.6376\n",
      "Epoch 163, CIFAR-10 Batch 4:  training cost 0.00156306  validation cost 2.79148\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 163, CIFAR-10 Batch 5:  training cost 0.000766894  validation cost 2.85808\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 164, CIFAR-10 Batch 1:  training cost 0.00165441  validation cost 2.92155\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 164, CIFAR-10 Batch 2:  training cost 0.000508462  validation cost 2.82042\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 164, CIFAR-10 Batch 3:  training cost 0.0025292  validation cost 2.89511\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 164, CIFAR-10 Batch 4:  training cost 0.00480496  validation cost 2.8664\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 164, CIFAR-10 Batch 5:  training cost 0.000453762  validation cost 2.95036\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 165, CIFAR-10 Batch 1:  training cost 0.00825127  validation cost 2.93725\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 165, CIFAR-10 Batch 2:  training cost 5.45871e-05  validation cost 2.97204\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 165, CIFAR-10 Batch 3:  training cost 0.000542326  validation cost 2.92641\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 165, CIFAR-10 Batch 4:  training cost 0.0143615  validation cost 2.84738\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 165, CIFAR-10 Batch 5:  training cost 0.00233377  validation cost 3.04346\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 166, CIFAR-10 Batch 1:  training cost 0.00464744  validation cost 3.00221\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 166, CIFAR-10 Batch 2:  training cost 0.00167162  validation cost 3.04021\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 166, CIFAR-10 Batch 3:  training cost 0.0073748  validation cost 2.77182\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 166, CIFAR-10 Batch 4:  training cost 0.00155636  validation cost 2.81598\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 166, CIFAR-10 Batch 5:  training cost 0.000373983  validation cost 3.15367\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 167, CIFAR-10 Batch 1:  training cost 0.00438302  validation cost 2.9467\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 167, CIFAR-10 Batch 2:  training cost 0.000119538  validation cost 2.89408\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 167, CIFAR-10 Batch 3:  training cost 0.000169634  validation cost 2.99614\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 167, CIFAR-10 Batch 4:  training cost 0.00444715  validation cost 2.97046\n",
      "training accuracy 1  validation accuracy 0.6168\n",
      "Epoch 167, CIFAR-10 Batch 5:  training cost 0.0021813  validation cost 2.92719\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 168, CIFAR-10 Batch 1:  training cost 0.00594016  validation cost 3.02638\n",
      "training accuracy 1  validation accuracy 0.6214\n",
      "Epoch 168, CIFAR-10 Batch 2:  training cost 9.98449e-05  validation cost 2.77635\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 168, CIFAR-10 Batch 3:  training cost 0.00329621  validation cost 2.97379\n",
      "training accuracy 1  validation accuracy 0.639\n",
      "Epoch 168, CIFAR-10 Batch 4:  training cost 0.00351365  validation cost 2.87375\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 168, CIFAR-10 Batch 5:  training cost 0.00711775  validation cost 3.02976\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 169, CIFAR-10 Batch 1:  training cost 0.000176587  validation cost 2.96939\n",
      "training accuracy 1  validation accuracy 0.6202\n",
      "Epoch 169, CIFAR-10 Batch 2:  training cost 0.00264043  validation cost 2.86585\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 169, CIFAR-10 Batch 3:  training cost 0.000126017  validation cost 2.79939\n",
      "training accuracy 1  validation accuracy 0.634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169, CIFAR-10 Batch 4:  training cost 0.00151777  validation cost 2.84943\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 169, CIFAR-10 Batch 5:  training cost 0.0106994  validation cost 3.02916\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 170, CIFAR-10 Batch 1:  training cost 0.00617268  validation cost 2.77529\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 170, CIFAR-10 Batch 2:  training cost 0.00524555  validation cost 2.90366\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 170, CIFAR-10 Batch 3:  training cost 0.00171783  validation cost 3.04972\n",
      "training accuracy 1  validation accuracy 0.6378\n",
      "Epoch 170, CIFAR-10 Batch 4:  training cost 0.00607252  validation cost 2.89618\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 170, CIFAR-10 Batch 5:  training cost 0.00146672  validation cost 3.02021\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 171, CIFAR-10 Batch 1:  training cost 0.0109616  validation cost 3.10849\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 171, CIFAR-10 Batch 2:  training cost 0.00153867  validation cost 2.98458\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 171, CIFAR-10 Batch 3:  training cost 0.00149482  validation cost 2.9233\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 171, CIFAR-10 Batch 4:  training cost 0.0106753  validation cost 2.93958\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 171, CIFAR-10 Batch 5:  training cost 0.00135416  validation cost 3.05495\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 172, CIFAR-10 Batch 1:  training cost 0.000815868  validation cost 2.95247\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 172, CIFAR-10 Batch 2:  training cost 0.00714672  validation cost 2.9517\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 172, CIFAR-10 Batch 3:  training cost 3.30933e-05  validation cost 3.0266\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 172, CIFAR-10 Batch 4:  training cost 0.000269662  validation cost 2.97052\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 172, CIFAR-10 Batch 5:  training cost 0.00180894  validation cost 3.00527\n",
      "training accuracy 1  validation accuracy 0.6346\n",
      "Epoch 173, CIFAR-10 Batch 1:  training cost 0.000242181  validation cost 3.01521\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 173, CIFAR-10 Batch 2:  training cost 0.00139394  validation cost 3.01354\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 173, CIFAR-10 Batch 3:  training cost 0.0151625  validation cost 3.02529\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 173, CIFAR-10 Batch 4:  training cost 0.00302393  validation cost 2.82169\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 173, CIFAR-10 Batch 5:  training cost 0.00569633  validation cost 3.09248\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 174, CIFAR-10 Batch 1:  training cost 0.00302614  validation cost 3.01062\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 174, CIFAR-10 Batch 2:  training cost 0.00142265  validation cost 3.02688\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 174, CIFAR-10 Batch 3:  training cost 0.00103947  validation cost 2.96223\n",
      "training accuracy 1  validation accuracy 0.618\n",
      "Epoch 174, CIFAR-10 Batch 4:  training cost 0.000319623  validation cost 2.70229\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 174, CIFAR-10 Batch 5:  training cost 0.010854  validation cost 2.94486\n",
      "training accuracy 1  validation accuracy 0.6214\n",
      "Epoch 175, CIFAR-10 Batch 1:  training cost 0.000101011  validation cost 3.01153\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 175, CIFAR-10 Batch 2:  training cost 0.0106928  validation cost 3.02579\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 175, CIFAR-10 Batch 3:  training cost 6.60605e-05  validation cost 2.94032\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 175, CIFAR-10 Batch 4:  training cost 0.00152977  validation cost 2.86709\n",
      "training accuracy 1  validation accuracy 0.6172\n",
      "Epoch 175, CIFAR-10 Batch 5:  training cost 0.000209459  validation cost 3.08451\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 176, CIFAR-10 Batch 1:  training cost 0.00252866  validation cost 2.84408\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 176, CIFAR-10 Batch 2:  training cost 0.000802807  validation cost 2.88659\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 176, CIFAR-10 Batch 3:  training cost 3.67439e-05  validation cost 2.91496\n",
      "training accuracy 1  validation accuracy 0.6386\n",
      "Epoch 176, CIFAR-10 Batch 4:  training cost 0.000391399  validation cost 2.91214\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 176, CIFAR-10 Batch 5:  training cost 0.000523624  validation cost 3.03992\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 177, CIFAR-10 Batch 1:  training cost 0.00034993  validation cost 2.93753\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 177, CIFAR-10 Batch 2:  training cost 0.000357476  validation cost 3.04561\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 177, CIFAR-10 Batch 3:  training cost 0.00280372  validation cost 3.13886\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 177, CIFAR-10 Batch 4:  training cost 0.000850894  validation cost 2.85357\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 177, CIFAR-10 Batch 5:  training cost 0.000314735  validation cost 2.9804\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 178, CIFAR-10 Batch 1:  training cost 0.00156105  validation cost 3.00156\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 178, CIFAR-10 Batch 2:  training cost 0.000202978  validation cost 2.91883\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 178, CIFAR-10 Batch 3:  training cost 0.00220496  validation cost 2.9664\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 178, CIFAR-10 Batch 4:  training cost 0.00167602  validation cost 2.85306\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 178, CIFAR-10 Batch 5:  training cost 0.00144036  validation cost 3.06827\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 179, CIFAR-10 Batch 1:  training cost 0.0299289  validation cost 2.98434\n",
      "training accuracy 1  validation accuracy 0.6166\n",
      "Epoch 179, CIFAR-10 Batch 2:  training cost 0.00206725  validation cost 2.88221\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 179, CIFAR-10 Batch 3:  training cost 0.00203233  validation cost 3.22552\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 179, CIFAR-10 Batch 4:  training cost 0.000204183  validation cost 2.83891\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 179, CIFAR-10 Batch 5:  training cost 0.00220485  validation cost 3.25716\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 180, CIFAR-10 Batch 1:  training cost 0.0034735  validation cost 2.83834\n",
      "training accuracy 1  validation accuracy 0.619\n",
      "Epoch 180, CIFAR-10 Batch 2:  training cost 0.00267159  validation cost 2.63836\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 180, CIFAR-10 Batch 3:  training cost 0.0068625  validation cost 2.79944\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 180, CIFAR-10 Batch 4:  training cost 0.00544192  validation cost 2.78192\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 180, CIFAR-10 Batch 5:  training cost 0.00132209  validation cost 3.07356\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 181, CIFAR-10 Batch 1:  training cost 0.00757613  validation cost 2.94075\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 181, CIFAR-10 Batch 2:  training cost 0.0129486  validation cost 3.11969\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 181, CIFAR-10 Batch 3:  training cost 0.000754824  validation cost 3.03724\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 181, CIFAR-10 Batch 4:  training cost 0.00213654  validation cost 3.00085\n",
      "training accuracy 1  validation accuracy 0.616\n",
      "Epoch 181, CIFAR-10 Batch 5:  training cost 0.00172177  validation cost 3.20788\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 182, CIFAR-10 Batch 1:  training cost 0.000551887  validation cost 3.10365\n",
      "training accuracy 1  validation accuracy 0.62\n",
      "Epoch 182, CIFAR-10 Batch 2:  training cost 0.00641761  validation cost 3.07668\n",
      "training accuracy 1  validation accuracy 0.6296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182, CIFAR-10 Batch 3:  training cost 0.00029743  validation cost 2.89388\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 182, CIFAR-10 Batch 4:  training cost 0.00635958  validation cost 2.86135\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 182, CIFAR-10 Batch 5:  training cost 0.00094046  validation cost 3.01827\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 183, CIFAR-10 Batch 1:  training cost 0.00079784  validation cost 3.08808\n",
      "training accuracy 1  validation accuracy 0.6144\n",
      "Epoch 183, CIFAR-10 Batch 2:  training cost 0.0123404  validation cost 3.02851\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 183, CIFAR-10 Batch 3:  training cost 6.22049e-05  validation cost 2.96841\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 183, CIFAR-10 Batch 4:  training cost 0.0135069  validation cost 2.61278\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 183, CIFAR-10 Batch 5:  training cost 0.000293604  validation cost 3.05455\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 184, CIFAR-10 Batch 1:  training cost 0.00523091  validation cost 3.08262\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 184, CIFAR-10 Batch 2:  training cost 0.00540835  validation cost 2.9006\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 184, CIFAR-10 Batch 3:  training cost 0.00316456  validation cost 3.01217\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 184, CIFAR-10 Batch 4:  training cost 0.000837851  validation cost 3.08035\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 184, CIFAR-10 Batch 5:  training cost 0.00156321  validation cost 3.06807\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 185, CIFAR-10 Batch 1:  training cost 0.000801708  validation cost 3.03024\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 185, CIFAR-10 Batch 2:  training cost 0.00189466  validation cost 2.90991\n",
      "training accuracy 1  validation accuracy 0.6222\n",
      "Epoch 185, CIFAR-10 Batch 3:  training cost 0.000334489  validation cost 2.79551\n",
      "training accuracy 1  validation accuracy 0.6414\n",
      "Epoch 185, CIFAR-10 Batch 4:  training cost 0.000500902  validation cost 2.9061\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 185, CIFAR-10 Batch 5:  training cost 0.000998045  validation cost 3.19105\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 186, CIFAR-10 Batch 1:  training cost 0.00838487  validation cost 2.93879\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 186, CIFAR-10 Batch 2:  training cost 0.00407195  validation cost 2.92914\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 186, CIFAR-10 Batch 3:  training cost 9.22421e-05  validation cost 3.00344\n",
      "training accuracy 1  validation accuracy 0.6396\n",
      "Epoch 186, CIFAR-10 Batch 4:  training cost 0.00192674  validation cost 3.07279\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 186, CIFAR-10 Batch 5:  training cost 0.00138815  validation cost 2.96217\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 187, CIFAR-10 Batch 1:  training cost 0.000715907  validation cost 2.75445\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 187, CIFAR-10 Batch 2:  training cost 7.78866e-05  validation cost 2.96196\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 187, CIFAR-10 Batch 3:  training cost 0.000492911  validation cost 3.29411\n",
      "training accuracy 1  validation accuracy 0.6378\n",
      "Epoch 187, CIFAR-10 Batch 4:  training cost 0.00291313  validation cost 2.91596\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 187, CIFAR-10 Batch 5:  training cost 0.00782548  validation cost 2.81927\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 188, CIFAR-10 Batch 1:  training cost 0.00239113  validation cost 3.14057\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 188, CIFAR-10 Batch 2:  training cost 7.70752e-05  validation cost 2.98627\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 188, CIFAR-10 Batch 3:  training cost 2.92349e-05  validation cost 3.19157\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 188, CIFAR-10 Batch 4:  training cost 0.000528902  validation cost 2.91201\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 188, CIFAR-10 Batch 5:  training cost 0.0143282  validation cost 3.07784\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 189, CIFAR-10 Batch 1:  training cost 5.45199e-05  validation cost 3.10795\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 189, CIFAR-10 Batch 2:  training cost 0.00120743  validation cost 2.83262\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 189, CIFAR-10 Batch 3:  training cost 0.00135866  validation cost 3.10048\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 189, CIFAR-10 Batch 4:  training cost 0.000263798  validation cost 2.82612\n",
      "training accuracy 1  validation accuracy 0.6376\n",
      "Epoch 189, CIFAR-10 Batch 5:  training cost 0.00484116  validation cost 3.05859\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 190, CIFAR-10 Batch 1:  training cost 0.00566838  validation cost 2.80387\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 190, CIFAR-10 Batch 2:  training cost 0.0138665  validation cost 2.955\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 190, CIFAR-10 Batch 3:  training cost 0.000183904  validation cost 2.9703\n",
      "training accuracy 1  validation accuracy 0.641\n",
      "Epoch 190, CIFAR-10 Batch 4:  training cost 0.00189007  validation cost 3.0911\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 190, CIFAR-10 Batch 5:  training cost 0.000156794  validation cost 3.11865\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 191, CIFAR-10 Batch 1:  training cost 0.00503003  validation cost 2.90686\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 191, CIFAR-10 Batch 2:  training cost 0.008354  validation cost 2.97309\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 191, CIFAR-10 Batch 3:  training cost 0.000198524  validation cost 2.99745\n",
      "training accuracy 1  validation accuracy 0.6464\n",
      "Epoch 191, CIFAR-10 Batch 4:  training cost 0.000120568  validation cost 3.08928\n",
      "training accuracy 1  validation accuracy 0.6388\n",
      "Epoch 191, CIFAR-10 Batch 5:  training cost 0.00354848  validation cost 3.17607\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 192, CIFAR-10 Batch 1:  training cost 0.000694052  validation cost 2.86353\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 192, CIFAR-10 Batch 2:  training cost 0.000780308  validation cost 3.06524\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 192, CIFAR-10 Batch 3:  training cost 0.000309725  validation cost 2.736\n",
      "training accuracy 1  validation accuracy 0.6438\n",
      "Epoch 192, CIFAR-10 Batch 4:  training cost 0.00378384  validation cost 2.90692\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 192, CIFAR-10 Batch 5:  training cost 0.00439412  validation cost 2.98832\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 193, CIFAR-10 Batch 1:  training cost 0.000713691  validation cost 2.88742\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 193, CIFAR-10 Batch 2:  training cost 0.000474743  validation cost 3.06163\n",
      "training accuracy 1  validation accuracy 0.6272\n",
      "Epoch 193, CIFAR-10 Batch 3:  training cost 0.000156598  validation cost 2.97401\n",
      "training accuracy 1  validation accuracy 0.6402\n",
      "Epoch 193, CIFAR-10 Batch 4:  training cost 0.000796631  validation cost 3.12405\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 193, CIFAR-10 Batch 5:  training cost 0.00508931  validation cost 2.96089\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 194, CIFAR-10 Batch 1:  training cost 0.000715583  validation cost 2.80937\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 194, CIFAR-10 Batch 2:  training cost 0.000266715  validation cost 2.9801\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 194, CIFAR-10 Batch 3:  training cost 0.000240904  validation cost 3.18931\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 194, CIFAR-10 Batch 4:  training cost 0.000478811  validation cost 3.06407\n",
      "training accuracy 1  validation accuracy 0.6198\n",
      "Epoch 194, CIFAR-10 Batch 5:  training cost 0.000794444  validation cost 3.09294\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 195, CIFAR-10 Batch 1:  training cost 0.0015138  validation cost 2.87049\n",
      "training accuracy 1  validation accuracy 0.6328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, CIFAR-10 Batch 2:  training cost 6.82211e-05  validation cost 2.99052\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 195, CIFAR-10 Batch 3:  training cost 2.85044e-05  validation cost 2.93736\n",
      "training accuracy 1  validation accuracy 0.6456\n",
      "Epoch 195, CIFAR-10 Batch 4:  training cost 0.000201048  validation cost 2.9335\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 195, CIFAR-10 Batch 5:  training cost 0.00479775  validation cost 3.04156\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 196, CIFAR-10 Batch 1:  training cost 0.000377132  validation cost 3.06891\n",
      "training accuracy 1  validation accuracy 0.6178\n",
      "Epoch 196, CIFAR-10 Batch 2:  training cost 0.00635813  validation cost 3.19419\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 196, CIFAR-10 Batch 3:  training cost 0.000179798  validation cost 2.77791\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 196, CIFAR-10 Batch 4:  training cost 0.000343235  validation cost 2.97859\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 196, CIFAR-10 Batch 5:  training cost 0.000832942  validation cost 3.10514\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 197, CIFAR-10 Batch 1:  training cost 0.00346087  validation cost 2.93431\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 197, CIFAR-10 Batch 2:  training cost 0.000537835  validation cost 3.08365\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 197, CIFAR-10 Batch 3:  training cost 0.000648799  validation cost 3.03439\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 197, CIFAR-10 Batch 4:  training cost 8.54294e-05  validation cost 2.86097\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 197, CIFAR-10 Batch 5:  training cost 0.0048408  validation cost 2.91039\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 198, CIFAR-10 Batch 1:  training cost 0.00215081  validation cost 3.11293\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 198, CIFAR-10 Batch 2:  training cost 0.000189786  validation cost 2.71679\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 198, CIFAR-10 Batch 3:  training cost 0.000863826  validation cost 2.89054\n",
      "training accuracy 1  validation accuracy 0.6378\n",
      "Epoch 198, CIFAR-10 Batch 4:  training cost 4.19145e-05  validation cost 3.02646\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 198, CIFAR-10 Batch 5:  training cost 0.00981131  validation cost 2.89884\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 199, CIFAR-10 Batch 1:  training cost 0.00146351  validation cost 2.95748\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 199, CIFAR-10 Batch 2:  training cost 0.00101765  validation cost 3.04616\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 199, CIFAR-10 Batch 3:  training cost 0.000419366  validation cost 3.11846\n",
      "training accuracy 1  validation accuracy 0.637\n",
      "Epoch 199, CIFAR-10 Batch 4:  training cost 0.00509244  validation cost 2.8181\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 199, CIFAR-10 Batch 5:  training cost 0.000279369  validation cost 2.75553\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 200, CIFAR-10 Batch 1:  training cost 0.00302745  validation cost 3.1359\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 200, CIFAR-10 Batch 2:  training cost 0.000494862  validation cost 3.11815\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 200, CIFAR-10 Batch 3:  training cost 7.01899e-05  validation cost 2.95898\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 200, CIFAR-10 Batch 4:  training cost 0.00355563  validation cost 3.13752\n",
      "training accuracy 1  validation accuracy 0.6216\n",
      "Epoch 200, CIFAR-10 Batch 5:  training cost 0.000755721  validation cost 3.29736\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 201, CIFAR-10 Batch 1:  training cost 0.00200837  validation cost 2.98376\n",
      "training accuracy 1  validation accuracy 0.641\n",
      "Epoch 201, CIFAR-10 Batch 2:  training cost 0.000467424  validation cost 3.0498\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 201, CIFAR-10 Batch 3:  training cost 0.000769968  validation cost 2.85801\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 201, CIFAR-10 Batch 4:  training cost 0.000373991  validation cost 3.07764\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 201, CIFAR-10 Batch 5:  training cost 0.00155029  validation cost 3.14339\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 202, CIFAR-10 Batch 1:  training cost 0.00241798  validation cost 2.9815\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 202, CIFAR-10 Batch 2:  training cost 0.00123732  validation cost 3.05717\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 202, CIFAR-10 Batch 3:  training cost 0.000149847  validation cost 2.93055\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 202, CIFAR-10 Batch 4:  training cost 0.00150298  validation cost 2.98759\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 202, CIFAR-10 Batch 5:  training cost 0.000221911  validation cost 2.86958\n",
      "training accuracy 1  validation accuracy 0.638\n",
      "Epoch 203, CIFAR-10 Batch 1:  training cost 0.00253274  validation cost 2.9385\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 203, CIFAR-10 Batch 2:  training cost 0.000512228  validation cost 2.92479\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 203, CIFAR-10 Batch 3:  training cost 0.00458043  validation cost 2.95085\n",
      "training accuracy 1  validation accuracy 0.643\n",
      "Epoch 203, CIFAR-10 Batch 4:  training cost 0.00531727  validation cost 2.83323\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 203, CIFAR-10 Batch 5:  training cost 0.000872648  validation cost 2.94291\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 204, CIFAR-10 Batch 1:  training cost 0.00934646  validation cost 2.9137\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 204, CIFAR-10 Batch 2:  training cost 8.01816e-05  validation cost 3.11746\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 204, CIFAR-10 Batch 3:  training cost 4.86917e-05  validation cost 3.12663\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 204, CIFAR-10 Batch 4:  training cost 9.22461e-05  validation cost 3.20229\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 204, CIFAR-10 Batch 5:  training cost 0.000942654  validation cost 3.01415\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 205, CIFAR-10 Batch 1:  training cost 0.00202274  validation cost 3.17652\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 205, CIFAR-10 Batch 2:  training cost 0.000165917  validation cost 2.92484\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 205, CIFAR-10 Batch 3:  training cost 9.75898e-05  validation cost 2.99137\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 205, CIFAR-10 Batch 4:  training cost 0.000344309  validation cost 2.94513\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 205, CIFAR-10 Batch 5:  training cost 0.00122382  validation cost 3.12205\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 206, CIFAR-10 Batch 1:  training cost 0.000302778  validation cost 3.26738\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 206, CIFAR-10 Batch 2:  training cost 0.000131767  validation cost 2.986\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 206, CIFAR-10 Batch 3:  training cost 8.07472e-05  validation cost 3.22099\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 206, CIFAR-10 Batch 4:  training cost 0.000859737  validation cost 3.10893\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 206, CIFAR-10 Batch 5:  training cost 0.00015398  validation cost 3.2525\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 207, CIFAR-10 Batch 1:  training cost 0.00116083  validation cost 2.96654\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 207, CIFAR-10 Batch 2:  training cost 0.00118618  validation cost 3.09454\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 207, CIFAR-10 Batch 3:  training cost 0.000747775  validation cost 3.04464\n",
      "training accuracy 1  validation accuracy 0.6392\n",
      "Epoch 207, CIFAR-10 Batch 4:  training cost 0.00146825  validation cost 3.09147\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 207, CIFAR-10 Batch 5:  training cost 0.000507502  validation cost 2.76311\n",
      "training accuracy 1  validation accuracy 0.6332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208, CIFAR-10 Batch 1:  training cost 0.000585686  validation cost 2.94405\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 208, CIFAR-10 Batch 2:  training cost 0.00495731  validation cost 3.21661\n",
      "training accuracy 1  validation accuracy 0.6166\n",
      "Epoch 208, CIFAR-10 Batch 3:  training cost 0.000524051  validation cost 2.73671\n",
      "training accuracy 1  validation accuracy 0.638\n",
      "Epoch 208, CIFAR-10 Batch 4:  training cost 0.00354771  validation cost 2.95861\n",
      "training accuracy 1  validation accuracy 0.64\n",
      "Epoch 208, CIFAR-10 Batch 5:  training cost 0.000878641  validation cost 3.09352\n",
      "training accuracy 1  validation accuracy 0.64\n",
      "Epoch 209, CIFAR-10 Batch 1:  training cost 0.000616932  validation cost 3.20278\n",
      "training accuracy 1  validation accuracy 0.6366\n",
      "Epoch 209, CIFAR-10 Batch 2:  training cost 0.000665237  validation cost 3.07429\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 209, CIFAR-10 Batch 3:  training cost 0.000161358  validation cost 3.08042\n",
      "training accuracy 1  validation accuracy 0.6434\n",
      "Epoch 209, CIFAR-10 Batch 4:  training cost 0.0043099  validation cost 3.03634\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 209, CIFAR-10 Batch 5:  training cost 0.00458763  validation cost 2.84707\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 210, CIFAR-10 Batch 1:  training cost 0.000338182  validation cost 2.87858\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 210, CIFAR-10 Batch 2:  training cost 0.00103656  validation cost 3.06733\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 210, CIFAR-10 Batch 3:  training cost 0.000409059  validation cost 3.017\n",
      "training accuracy 1  validation accuracy 0.6402\n",
      "Epoch 210, CIFAR-10 Batch 4:  training cost 0.0110194  validation cost 2.96167\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 210, CIFAR-10 Batch 5:  training cost 0.00222761  validation cost 3.01997\n",
      "training accuracy 1  validation accuracy 0.637\n",
      "Epoch 211, CIFAR-10 Batch 1:  training cost 0.00102171  validation cost 2.93565\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 211, CIFAR-10 Batch 2:  training cost 0.0147936  validation cost 2.95097\n",
      "training accuracy 1  validation accuracy 0.6394\n",
      "Epoch 211, CIFAR-10 Batch 3:  training cost 1.88495e-05  validation cost 2.97689\n",
      "training accuracy 1  validation accuracy 0.6386\n",
      "Epoch 211, CIFAR-10 Batch 4:  training cost 0.000411438  validation cost 3.00492\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 211, CIFAR-10 Batch 5:  training cost 0.000815897  validation cost 3.10134\n",
      "training accuracy 1  validation accuracy 0.6456\n",
      "Epoch 212, CIFAR-10 Batch 1:  training cost 0.0010106  validation cost 3.04737\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 212, CIFAR-10 Batch 2:  training cost 0.00127341  validation cost 3.10612\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 212, CIFAR-10 Batch 3:  training cost 0.0014041  validation cost 3.01673\n",
      "training accuracy 1  validation accuracy 0.6426\n",
      "Epoch 212, CIFAR-10 Batch 4:  training cost 0.00178103  validation cost 2.40907\n",
      "training accuracy 1  validation accuracy 0.6206\n",
      "Epoch 212, CIFAR-10 Batch 5:  training cost 0.00421186  validation cost 2.85103\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 213, CIFAR-10 Batch 1:  training cost 0.00773524  validation cost 2.98213\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 213, CIFAR-10 Batch 2:  training cost 0.00140282  validation cost 3.22897\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 213, CIFAR-10 Batch 3:  training cost 0.00135986  validation cost 3.04769\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 213, CIFAR-10 Batch 4:  training cost 0.00126191  validation cost 3.20276\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 213, CIFAR-10 Batch 5:  training cost 0.00457518  validation cost 2.89468\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 214, CIFAR-10 Batch 1:  training cost 0.00162444  validation cost 2.91482\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 214, CIFAR-10 Batch 2:  training cost 0.000391202  validation cost 3.08615\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 214, CIFAR-10 Batch 3:  training cost 2.55986e-05  validation cost 2.97914\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 214, CIFAR-10 Batch 4:  training cost 0.000756685  validation cost 3.04762\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 214, CIFAR-10 Batch 5:  training cost 0.000424799  validation cost 3.11983\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 215, CIFAR-10 Batch 1:  training cost 0.00106018  validation cost 2.95431\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 215, CIFAR-10 Batch 2:  training cost 0.000509567  validation cost 2.98882\n",
      "training accuracy 1  validation accuracy 0.6426\n",
      "Epoch 215, CIFAR-10 Batch 3:  training cost 7.88678e-05  validation cost 3.04512\n",
      "training accuracy 1  validation accuracy 0.639\n",
      "Epoch 215, CIFAR-10 Batch 4:  training cost 3.41224e-05  validation cost 3.08973\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 215, CIFAR-10 Batch 5:  training cost 0.00111425  validation cost 2.99373\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 216, CIFAR-10 Batch 1:  training cost 0.0156818  validation cost 2.83692\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 216, CIFAR-10 Batch 2:  training cost 0.00281591  validation cost 2.90839\n",
      "training accuracy 1  validation accuracy 0.6412\n",
      "Epoch 216, CIFAR-10 Batch 3:  training cost 0.00384262  validation cost 3.08628\n",
      "training accuracy 1  validation accuracy 0.6374\n",
      "Epoch 216, CIFAR-10 Batch 4:  training cost 0.0021587  validation cost 3.02947\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 216, CIFAR-10 Batch 5:  training cost 0.00028929  validation cost 3.16384\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 217, CIFAR-10 Batch 1:  training cost 0.000374412  validation cost 3.0891\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 217, CIFAR-10 Batch 2:  training cost 0.000171534  validation cost 3.20942\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 217, CIFAR-10 Batch 3:  training cost 0.000447593  validation cost 3.12403\n",
      "training accuracy 1  validation accuracy 0.637\n",
      "Epoch 217, CIFAR-10 Batch 4:  training cost 0.000614875  validation cost 3.06662\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 217, CIFAR-10 Batch 5:  training cost 0.00155027  validation cost 2.74961\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 218, CIFAR-10 Batch 1:  training cost 0.486945  validation cost 3.03064\n",
      "training accuracy 0.875  validation accuracy 0.6232\n",
      "Epoch 218, CIFAR-10 Batch 2:  training cost 0.000904379  validation cost 2.86504\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 218, CIFAR-10 Batch 3:  training cost 0.00124121  validation cost 3.04445\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 218, CIFAR-10 Batch 4:  training cost 0.0025212  validation cost 2.83297\n",
      "training accuracy 1  validation accuracy 0.6214\n",
      "Epoch 218, CIFAR-10 Batch 5:  training cost 0.00736521  validation cost 3.07823\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 219, CIFAR-10 Batch 1:  training cost 0.00355138  validation cost 2.89618\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 219, CIFAR-10 Batch 2:  training cost 0.000755919  validation cost 2.72289\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 219, CIFAR-10 Batch 3:  training cost 0.0166079  validation cost 1.78938\n",
      "training accuracy 1  validation accuracy 0.6092\n",
      "Epoch 219, CIFAR-10 Batch 4:  training cost 0.00802691  validation cost 2.28406\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 219, CIFAR-10 Batch 5:  training cost 0.00664391  validation cost 2.57307\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 220, CIFAR-10 Batch 1:  training cost 0.00123958  validation cost 2.70793\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 220, CIFAR-10 Batch 2:  training cost 0.0212032  validation cost 2.78497\n",
      "training accuracy 1  validation accuracy 0.6346\n",
      "Epoch 220, CIFAR-10 Batch 3:  training cost 0.000281407  validation cost 2.88134\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 220, CIFAR-10 Batch 4:  training cost 0.000356847  validation cost 2.95362\n",
      "training accuracy 1  validation accuracy 0.6358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220, CIFAR-10 Batch 5:  training cost 0.00184348  validation cost 3.25944\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 221, CIFAR-10 Batch 1:  training cost 0.000859831  validation cost 3.08157\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 221, CIFAR-10 Batch 2:  training cost 0.00149322  validation cost 3.1067\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 221, CIFAR-10 Batch 3:  training cost 8.07632e-06  validation cost 3.149\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 221, CIFAR-10 Batch 4:  training cost 0.000350688  validation cost 3.06644\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 221, CIFAR-10 Batch 5:  training cost 0.0018504  validation cost 3.13738\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 222, CIFAR-10 Batch 1:  training cost 0.000522806  validation cost 3.18693\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 222, CIFAR-10 Batch 2:  training cost 0.0105764  validation cost 3.19052\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 222, CIFAR-10 Batch 3:  training cost 0.00166925  validation cost 3.02436\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 222, CIFAR-10 Batch 4:  training cost 5.42396e-06  validation cost 2.95644\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 222, CIFAR-10 Batch 5:  training cost 0.00438458  validation cost 3.05111\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 223, CIFAR-10 Batch 1:  training cost 0.00155851  validation cost 2.89758\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 223, CIFAR-10 Batch 2:  training cost 0.00226439  validation cost 3.09722\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 223, CIFAR-10 Batch 3:  training cost 0.000844491  validation cost 3.21533\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 223, CIFAR-10 Batch 4:  training cost 2.74463e-05  validation cost 3.03276\n",
      "training accuracy 1  validation accuracy 0.6428\n",
      "Epoch 223, CIFAR-10 Batch 5:  training cost 0.000498698  validation cost 2.97679\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 224, CIFAR-10 Batch 1:  training cost 0.000332579  validation cost 2.93371\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 224, CIFAR-10 Batch 2:  training cost 1.71358e-05  validation cost 3.06617\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 224, CIFAR-10 Batch 3:  training cost 0.00231687  validation cost 3.13151\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 224, CIFAR-10 Batch 4:  training cost 0.000764538  validation cost 3.03262\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 224, CIFAR-10 Batch 5:  training cost 8.85346e-05  validation cost 3.15082\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 225, CIFAR-10 Batch 1:  training cost 0.00246967  validation cost 3.20604\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 225, CIFAR-10 Batch 2:  training cost 0.000487799  validation cost 2.94235\n",
      "training accuracy 1  validation accuracy 0.6388\n",
      "Epoch 225, CIFAR-10 Batch 3:  training cost 0.000194567  validation cost 3.02016\n",
      "training accuracy 1  validation accuracy 0.6414\n",
      "Epoch 225, CIFAR-10 Batch 4:  training cost 0.00947892  validation cost 2.98395\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 225, CIFAR-10 Batch 5:  training cost 0.00212367  validation cost 3.17875\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 226, CIFAR-10 Batch 1:  training cost 0.02409  validation cost 2.60726\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 226, CIFAR-10 Batch 2:  training cost 0.000160701  validation cost 2.98584\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 226, CIFAR-10 Batch 3:  training cost 0.00560985  validation cost 3.08172\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 226, CIFAR-10 Batch 4:  training cost 2.55088e-05  validation cost 3.05346\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 226, CIFAR-10 Batch 5:  training cost 0.00136362  validation cost 3.08764\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 227, CIFAR-10 Batch 1:  training cost 0.000334634  validation cost 2.95394\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 227, CIFAR-10 Batch 2:  training cost 0.000545656  validation cost 3.146\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 227, CIFAR-10 Batch 3:  training cost 0.00169408  validation cost 2.42662\n",
      "training accuracy 1  validation accuracy 0.6364\n",
      "Epoch 227, CIFAR-10 Batch 4:  training cost 0.00107687  validation cost 2.29458\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 227, CIFAR-10 Batch 5:  training cost 0.000138014  validation cost 2.71833\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 228, CIFAR-10 Batch 1:  training cost 0.00842792  validation cost 2.96338\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 228, CIFAR-10 Batch 2:  training cost 0.00350701  validation cost 2.99459\n",
      "training accuracy 1  validation accuracy 0.6448\n",
      "Epoch 228, CIFAR-10 Batch 3:  training cost 0.000145735  validation cost 2.95116\n",
      "training accuracy 1  validation accuracy 0.6422\n",
      "Epoch 228, CIFAR-10 Batch 4:  training cost 1.59883e-05  validation cost 3.08124\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 228, CIFAR-10 Batch 5:  training cost 0.00165355  validation cost 3.14926\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 229, CIFAR-10 Batch 1:  training cost 0.000878578  validation cost 3.09254\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 229, CIFAR-10 Batch 2:  training cost 0.0101216  validation cost 3.11504\n",
      "training accuracy 1  validation accuracy 0.6382\n",
      "Epoch 229, CIFAR-10 Batch 3:  training cost 0.00100644  validation cost 2.41828\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 229, CIFAR-10 Batch 4:  training cost 0.00063868  validation cost 2.86153\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 229, CIFAR-10 Batch 5:  training cost 0.00059093  validation cost 3.06725\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 230, CIFAR-10 Batch 1:  training cost 0.00143275  validation cost 3.05953\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 230, CIFAR-10 Batch 2:  training cost 0.000626583  validation cost 3.2136\n",
      "training accuracy 1  validation accuracy 0.628\n",
      "Epoch 230, CIFAR-10 Batch 3:  training cost 0.000389322  validation cost 3.15051\n",
      "training accuracy 1  validation accuracy 0.6424\n",
      "Epoch 230, CIFAR-10 Batch 4:  training cost 0.00292598  validation cost 3.00463\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 230, CIFAR-10 Batch 5:  training cost 0.000485037  validation cost 3.07265\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 231, CIFAR-10 Batch 1:  training cost 0.000768239  validation cost 3.21872\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 231, CIFAR-10 Batch 2:  training cost 0.00114945  validation cost 3.07801\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 231, CIFAR-10 Batch 3:  training cost 0.000142322  validation cost 3.02018\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 231, CIFAR-10 Batch 4:  training cost 0.000736025  validation cost 3.11885\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 231, CIFAR-10 Batch 5:  training cost 4.70224e-05  validation cost 3.31835\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 232, CIFAR-10 Batch 1:  training cost 0.000324773  validation cost 3.18322\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 232, CIFAR-10 Batch 2:  training cost 0.000690227  validation cost 3.16745\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 232, CIFAR-10 Batch 3:  training cost 9.267e-05  validation cost 3.20832\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 232, CIFAR-10 Batch 4:  training cost 0.000231046  validation cost 2.98108\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 232, CIFAR-10 Batch 5:  training cost 0.0101032  validation cost 2.65683\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 233, CIFAR-10 Batch 1:  training cost 0.00737296  validation cost 3.29754\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 233, CIFAR-10 Batch 2:  training cost 0.00192649  validation cost 2.97646\n",
      "training accuracy 1  validation accuracy 0.6382\n",
      "Epoch 233, CIFAR-10 Batch 3:  training cost 0.000107715  validation cost 3.07018\n",
      "training accuracy 1  validation accuracy 0.6382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233, CIFAR-10 Batch 4:  training cost 0.00195201  validation cost 3.24819\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 233, CIFAR-10 Batch 5:  training cost 0.00338649  validation cost 3.10925\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 234, CIFAR-10 Batch 1:  training cost 0.00254961  validation cost 3.18494\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 234, CIFAR-10 Batch 2:  training cost 0.000216508  validation cost 3.16497\n",
      "training accuracy 1  validation accuracy 0.6264\n",
      "Epoch 234, CIFAR-10 Batch 3:  training cost 0.0114824  validation cost 3.20497\n",
      "training accuracy 1  validation accuracy 0.6364\n",
      "Epoch 234, CIFAR-10 Batch 4:  training cost 0.000510241  validation cost 3.0927\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 234, CIFAR-10 Batch 5:  training cost 0.00243689  validation cost 3.13821\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 235, CIFAR-10 Batch 1:  training cost 0.00442616  validation cost 2.85586\n",
      "training accuracy 1  validation accuracy 0.618\n",
      "Epoch 235, CIFAR-10 Batch 2:  training cost 0.00146617  validation cost 3.32604\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 235, CIFAR-10 Batch 3:  training cost 7.86773e-06  validation cost 3.29577\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 235, CIFAR-10 Batch 4:  training cost 0.00125022  validation cost 3.21444\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 235, CIFAR-10 Batch 5:  training cost 0.000206534  validation cost 3.33401\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 236, CIFAR-10 Batch 1:  training cost 0.0025615  validation cost 3.25807\n",
      "training accuracy 1  validation accuracy 0.6234\n",
      "Epoch 236, CIFAR-10 Batch 2:  training cost 3.86924e-05  validation cost 3.19351\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 236, CIFAR-10 Batch 3:  training cost 0.000564692  validation cost 3.11334\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 236, CIFAR-10 Batch 4:  training cost 0.000270858  validation cost 3.00983\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 236, CIFAR-10 Batch 5:  training cost 0.00249176  validation cost 3.322\n",
      "training accuracy 1  validation accuracy 0.6248\n",
      "Epoch 237, CIFAR-10 Batch 1:  training cost 0.00256055  validation cost 3.25157\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 237, CIFAR-10 Batch 2:  training cost 0.00180265  validation cost 3.27749\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 237, CIFAR-10 Batch 3:  training cost 5.33716e-05  validation cost 3.03949\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 237, CIFAR-10 Batch 4:  training cost 0.00130756  validation cost 3.00578\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 237, CIFAR-10 Batch 5:  training cost 0.000943054  validation cost 3.23902\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 238, CIFAR-10 Batch 1:  training cost 0.0029537  validation cost 3.28514\n",
      "training accuracy 1  validation accuracy 0.6214\n",
      "Epoch 238, CIFAR-10 Batch 2:  training cost 0.000464282  validation cost 2.93774\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 238, CIFAR-10 Batch 3:  training cost 0.00142772  validation cost 2.58805\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 238, CIFAR-10 Batch 4:  training cost 7.58153e-05  validation cost 3.04179\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 238, CIFAR-10 Batch 5:  training cost 0.0034055  validation cost 3.07523\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 239, CIFAR-10 Batch 1:  training cost 0.00167117  validation cost 3.03727\n",
      "training accuracy 1  validation accuracy 0.6232\n",
      "Epoch 239, CIFAR-10 Batch 2:  training cost 0.000111058  validation cost 3.01496\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 239, CIFAR-10 Batch 3:  training cost 0.00427917  validation cost 3.21661\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 239, CIFAR-10 Batch 4:  training cost 0.000472162  validation cost 2.98367\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 239, CIFAR-10 Batch 5:  training cost 5.93294e-05  validation cost 3.33287\n",
      "training accuracy 1  validation accuracy 0.6236\n",
      "Epoch 240, CIFAR-10 Batch 1:  training cost 0.00283726  validation cost 3.13119\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 240, CIFAR-10 Batch 2:  training cost 5.82839e-05  validation cost 3.08046\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 240, CIFAR-10 Batch 3:  training cost 4.7322e-05  validation cost 3.21433\n",
      "training accuracy 1  validation accuracy 0.6374\n",
      "Epoch 240, CIFAR-10 Batch 4:  training cost 0.000538759  validation cost 3.05592\n",
      "training accuracy 1  validation accuracy 0.637\n",
      "Epoch 240, CIFAR-10 Batch 5:  training cost 0.00418615  validation cost 3.22205\n",
      "training accuracy 1  validation accuracy 0.6258\n",
      "Epoch 241, CIFAR-10 Batch 1:  training cost 0.00138652  validation cost 3.02823\n",
      "training accuracy 1  validation accuracy 0.617\n",
      "Epoch 241, CIFAR-10 Batch 2:  training cost 3.88878e-05  validation cost 3.32191\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 241, CIFAR-10 Batch 3:  training cost 0.00247906  validation cost 3.23174\n",
      "training accuracy 1  validation accuracy 0.6388\n",
      "Epoch 241, CIFAR-10 Batch 4:  training cost 0.00303009  validation cost 3.04117\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 241, CIFAR-10 Batch 5:  training cost 3.51497e-05  validation cost 3.24507\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 242, CIFAR-10 Batch 1:  training cost 0.000697765  validation cost 2.95883\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 242, CIFAR-10 Batch 2:  training cost 0.00159688  validation cost 3.11903\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 242, CIFAR-10 Batch 3:  training cost 2.64922e-05  validation cost 3.23349\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 242, CIFAR-10 Batch 4:  training cost 0.00041865  validation cost 2.79727\n",
      "training accuracy 1  validation accuracy 0.6374\n",
      "Epoch 242, CIFAR-10 Batch 5:  training cost 0.000338331  validation cost 2.99679\n",
      "training accuracy 1  validation accuracy 0.6308\n",
      "Epoch 243, CIFAR-10 Batch 1:  training cost 0.00143537  validation cost 3.18066\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 243, CIFAR-10 Batch 2:  training cost 0.0026065  validation cost 3.01638\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 243, CIFAR-10 Batch 3:  training cost 0.0033939  validation cost 3.31754\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 243, CIFAR-10 Batch 4:  training cost 0.000329382  validation cost 3.24294\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 243, CIFAR-10 Batch 5:  training cost 0.000125772  validation cost 3.25555\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 244, CIFAR-10 Batch 1:  training cost 0.00212213  validation cost 3.30379\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 244, CIFAR-10 Batch 2:  training cost 2.14566e-05  validation cost 3.47017\n",
      "training accuracy 1  validation accuracy 0.6196\n",
      "Epoch 244, CIFAR-10 Batch 3:  training cost 4.03634e-05  validation cost 3.06463\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 244, CIFAR-10 Batch 4:  training cost 0.00623073  validation cost 3.05543\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 244, CIFAR-10 Batch 5:  training cost 0.00301977  validation cost 3.08699\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 245, CIFAR-10 Batch 1:  training cost 0.000652683  validation cost 3.06914\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 245, CIFAR-10 Batch 2:  training cost 0.00093247  validation cost 3.18245\n",
      "training accuracy 1  validation accuracy 0.637\n",
      "Epoch 245, CIFAR-10 Batch 3:  training cost 0.00490486  validation cost 3.26005\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 245, CIFAR-10 Batch 4:  training cost 0.000916765  validation cost 3.30457\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 245, CIFAR-10 Batch 5:  training cost 0.00137851  validation cost 2.827\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 246, CIFAR-10 Batch 1:  training cost 0.00390541  validation cost 3.15087\n",
      "training accuracy 1  validation accuracy 0.62\n",
      "Epoch 246, CIFAR-10 Batch 2:  training cost 0.000267859  validation cost 3.15601\n",
      "training accuracy 1  validation accuracy 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246, CIFAR-10 Batch 3:  training cost 0.000310844  validation cost 3.16911\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 246, CIFAR-10 Batch 4:  training cost 0.00295213  validation cost 2.95791\n",
      "training accuracy 1  validation accuracy 0.623\n",
      "Epoch 246, CIFAR-10 Batch 5:  training cost 0.000520006  validation cost 3.21457\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 247, CIFAR-10 Batch 1:  training cost 0.000340065  validation cost 3.06269\n",
      "training accuracy 1  validation accuracy 0.6338\n",
      "Epoch 247, CIFAR-10 Batch 2:  training cost 0.000110086  validation cost 3.06778\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 247, CIFAR-10 Batch 3:  training cost 0.00194597  validation cost 2.72418\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 247, CIFAR-10 Batch 4:  training cost 0.00265814  validation cost 3.13926\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 247, CIFAR-10 Batch 5:  training cost 0.000129139  validation cost 3.18593\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 248, CIFAR-10 Batch 1:  training cost 0.0326411  validation cost 3.07462\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 248, CIFAR-10 Batch 2:  training cost 3.74886e-05  validation cost 3.08902\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 248, CIFAR-10 Batch 3:  training cost 0.000606625  validation cost 3.25382\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 248, CIFAR-10 Batch 4:  training cost 0.000119471  validation cost 3.15433\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 248, CIFAR-10 Batch 5:  training cost 0.000356521  validation cost 3.25482\n",
      "training accuracy 1  validation accuracy 0.6394\n",
      "Epoch 249, CIFAR-10 Batch 1:  training cost 0.0022064  validation cost 3.17492\n",
      "training accuracy 1  validation accuracy 0.6172\n",
      "Epoch 249, CIFAR-10 Batch 2:  training cost 0.00980593  validation cost 3.29138\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 249, CIFAR-10 Batch 3:  training cost 7.91715e-05  validation cost 3.24656\n",
      "training accuracy 1  validation accuracy 0.6376\n",
      "Epoch 249, CIFAR-10 Batch 4:  training cost 0.000562075  validation cost 2.86592\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 249, CIFAR-10 Batch 5:  training cost 0.000584483  validation cost 2.97048\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 250, CIFAR-10 Batch 1:  training cost 0.000698647  validation cost 3.1892\n",
      "training accuracy 1  validation accuracy 0.625\n",
      "Epoch 250, CIFAR-10 Batch 2:  training cost 9.83443e-06  validation cost 3.32289\n",
      "training accuracy 1  validation accuracy 0.6366\n",
      "Epoch 250, CIFAR-10 Batch 3:  training cost 1.17418e-05  validation cost 3.33191\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 250, CIFAR-10 Batch 4:  training cost 0.000205349  validation cost 3.03304\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 250, CIFAR-10 Batch 5:  training cost 0.000971122  validation cost 3.20106\n",
      "training accuracy 1  validation accuracy 0.639\n",
      "Epoch 251, CIFAR-10 Batch 1:  training cost 0.00429237  validation cost 3.283\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 251, CIFAR-10 Batch 2:  training cost 0.000752251  validation cost 3.14298\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 251, CIFAR-10 Batch 3:  training cost 9.62607e-06  validation cost 3.24413\n",
      "training accuracy 1  validation accuracy 0.6396\n",
      "Epoch 251, CIFAR-10 Batch 4:  training cost 0.000231185  validation cost 3.31591\n",
      "training accuracy 1  validation accuracy 0.6282\n",
      "Epoch 251, CIFAR-10 Batch 5:  training cost 0.00830313  validation cost 3.11209\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 252, CIFAR-10 Batch 1:  training cost 0.000811074  validation cost 3.03368\n",
      "training accuracy 1  validation accuracy 0.6284\n",
      "Epoch 252, CIFAR-10 Batch 2:  training cost 0.00747579  validation cost 3.30355\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 252, CIFAR-10 Batch 3:  training cost 0.000146733  validation cost 3.10231\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 252, CIFAR-10 Batch 4:  training cost 0.000688607  validation cost 3.20373\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 252, CIFAR-10 Batch 5:  training cost 0.0075201  validation cost 3.43229\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 253, CIFAR-10 Batch 1:  training cost 0.000665526  validation cost 3.21976\n",
      "training accuracy 1  validation accuracy 0.6206\n",
      "Epoch 253, CIFAR-10 Batch 2:  training cost 0.000316043  validation cost 3.09704\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 253, CIFAR-10 Batch 3:  training cost 7.99675e-05  validation cost 3.23226\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 253, CIFAR-10 Batch 4:  training cost 0.00104808  validation cost 3.15106\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 253, CIFAR-10 Batch 5:  training cost 0.00286716  validation cost 3.12461\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 254, CIFAR-10 Batch 1:  training cost 0.00645804  validation cost 3.27581\n",
      "training accuracy 1  validation accuracy 0.6224\n",
      "Epoch 254, CIFAR-10 Batch 2:  training cost 0.00238851  validation cost 3.39608\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 254, CIFAR-10 Batch 3:  training cost 0.000158847  validation cost 3.1784\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 254, CIFAR-10 Batch 4:  training cost 0.00339056  validation cost 3.26175\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 254, CIFAR-10 Batch 5:  training cost 0.00890356  validation cost 3.31875\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 255, CIFAR-10 Batch 1:  training cost 0.00269034  validation cost 3.25087\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 255, CIFAR-10 Batch 2:  training cost 0.000571495  validation cost 3.17208\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 255, CIFAR-10 Batch 3:  training cost 0.000332977  validation cost 3.08447\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 255, CIFAR-10 Batch 4:  training cost 3.13654e-05  validation cost 3.20882\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 255, CIFAR-10 Batch 5:  training cost 0.00756317  validation cost 2.87904\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 256, CIFAR-10 Batch 1:  training cost 0.00119602  validation cost 2.86021\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 256, CIFAR-10 Batch 2:  training cost 0.000603164  validation cost 3.0406\n",
      "training accuracy 1  validation accuracy 0.638\n",
      "Epoch 256, CIFAR-10 Batch 3:  training cost 0.000876187  validation cost 3.19766\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 256, CIFAR-10 Batch 4:  training cost 0.000762544  validation cost 2.91498\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 256, CIFAR-10 Batch 5:  training cost 0.00172934  validation cost 3.07459\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 257, CIFAR-10 Batch 1:  training cost 0.0129858  validation cost 3.14904\n",
      "training accuracy 1  validation accuracy 0.6162\n",
      "Epoch 257, CIFAR-10 Batch 2:  training cost 8.31253e-05  validation cost 2.92232\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 257, CIFAR-10 Batch 3:  training cost 0.000790449  validation cost 2.96436\n",
      "training accuracy 1  validation accuracy 0.6418\n",
      "Epoch 257, CIFAR-10 Batch 4:  training cost 8.29153e-05  validation cost 3.2107\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 257, CIFAR-10 Batch 5:  training cost 0.00124245  validation cost 3.30967\n",
      "training accuracy 1  validation accuracy 0.6364\n",
      "Epoch 258, CIFAR-10 Batch 1:  training cost 0.00613982  validation cost 3.26829\n",
      "training accuracy 1  validation accuracy 0.6252\n",
      "Epoch 258, CIFAR-10 Batch 2:  training cost 0.00192715  validation cost 3.46993\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 258, CIFAR-10 Batch 3:  training cost 5.15822e-05  validation cost 2.96068\n",
      "training accuracy 1  validation accuracy 0.6408\n",
      "Epoch 258, CIFAR-10 Batch 4:  training cost 0.00019203  validation cost 3.05416\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 258, CIFAR-10 Batch 5:  training cost 0.00415505  validation cost 3.19914\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 259, CIFAR-10 Batch 1:  training cost 2.58959e-05  validation cost 3.19326\n",
      "training accuracy 1  validation accuracy 0.6308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259, CIFAR-10 Batch 2:  training cost 0.00117008  validation cost 3.07648\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 259, CIFAR-10 Batch 3:  training cost 0.000299907  validation cost 3.2554\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 259, CIFAR-10 Batch 4:  training cost 8.25005e-05  validation cost 2.92965\n",
      "training accuracy 1  validation accuracy 0.6422\n",
      "Epoch 259, CIFAR-10 Batch 5:  training cost 1.46622e-05  validation cost 3.04438\n",
      "training accuracy 1  validation accuracy 0.6388\n",
      "Epoch 260, CIFAR-10 Batch 1:  training cost 0.0040318  validation cost 3.32405\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 260, CIFAR-10 Batch 2:  training cost 0.00137383  validation cost 3.28629\n",
      "training accuracy 1  validation accuracy 0.6374\n",
      "Epoch 260, CIFAR-10 Batch 3:  training cost 0.000530859  validation cost 3.14649\n",
      "training accuracy 1  validation accuracy 0.6418\n",
      "Epoch 260, CIFAR-10 Batch 4:  training cost 0.000356216  validation cost 3.11502\n",
      "training accuracy 1  validation accuracy 0.6242\n",
      "Epoch 260, CIFAR-10 Batch 5:  training cost 1.23679e-06  validation cost 3.24133\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 261, CIFAR-10 Batch 1:  training cost 0.00189492  validation cost 3.26925\n",
      "training accuracy 1  validation accuracy 0.6228\n",
      "Epoch 261, CIFAR-10 Batch 2:  training cost 0.000205729  validation cost 2.93025\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 261, CIFAR-10 Batch 3:  training cost 0.000131558  validation cost 3.00069\n",
      "training accuracy 1  validation accuracy 0.639\n",
      "Epoch 261, CIFAR-10 Batch 4:  training cost 0.000188903  validation cost 3.1252\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 261, CIFAR-10 Batch 5:  training cost 0.00029127  validation cost 3.03352\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 262, CIFAR-10 Batch 1:  training cost 0.000449987  validation cost 3.14177\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 262, CIFAR-10 Batch 2:  training cost 0.00166455  validation cost 2.75996\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 262, CIFAR-10 Batch 3:  training cost 9.63101e-05  validation cost 3.07027\n",
      "training accuracy 1  validation accuracy 0.6346\n",
      "Epoch 262, CIFAR-10 Batch 4:  training cost 6.82969e-05  validation cost 3.31851\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 262, CIFAR-10 Batch 5:  training cost 0.00010095  validation cost 3.02924\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 263, CIFAR-10 Batch 1:  training cost 0.000996265  validation cost 3.33848\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 263, CIFAR-10 Batch 2:  training cost 2.7269e-06  validation cost 3.33345\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 263, CIFAR-10 Batch 3:  training cost 2.19042e-05  validation cost 3.23011\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 263, CIFAR-10 Batch 4:  training cost 0.000693468  validation cost 3.18478\n",
      "training accuracy 1  validation accuracy 0.6276\n",
      "Epoch 263, CIFAR-10 Batch 5:  training cost 0.000114699  validation cost 3.15033\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 264, CIFAR-10 Batch 1:  training cost 0.00305859  validation cost 3.09889\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 264, CIFAR-10 Batch 2:  training cost 0.000263367  validation cost 3.06804\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 264, CIFAR-10 Batch 3:  training cost 0.000127267  validation cost 3.37924\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 264, CIFAR-10 Batch 4:  training cost 4.52048e-05  validation cost 3.34387\n",
      "training accuracy 1  validation accuracy 0.6262\n",
      "Epoch 264, CIFAR-10 Batch 5:  training cost 4.14343e-05  validation cost 3.12544\n",
      "training accuracy 1  validation accuracy 0.6218\n",
      "Epoch 265, CIFAR-10 Batch 1:  training cost 0.00155891  validation cost 3.58783\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 265, CIFAR-10 Batch 2:  training cost 2.67457e-05  validation cost 3.20315\n",
      "training accuracy 1  validation accuracy 0.6332\n",
      "Epoch 265, CIFAR-10 Batch 3:  training cost 0.00218497  validation cost 2.96631\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 265, CIFAR-10 Batch 4:  training cost 0.00287263  validation cost 2.33202\n",
      "training accuracy 1  validation accuracy 0.6396\n",
      "Epoch 265, CIFAR-10 Batch 5:  training cost 0.000728164  validation cost 2.56541\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 266, CIFAR-10 Batch 1:  training cost 0.000744194  validation cost 2.87533\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 266, CIFAR-10 Batch 2:  training cost 0.000507119  validation cost 3.1957\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 266, CIFAR-10 Batch 3:  training cost 0.000185376  validation cost 3.21327\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 266, CIFAR-10 Batch 4:  training cost 0.000115607  validation cost 3.12913\n",
      "training accuracy 1  validation accuracy 0.6386\n",
      "Epoch 266, CIFAR-10 Batch 5:  training cost 6.58227e-05  validation cost 3.03806\n",
      "training accuracy 1  validation accuracy 0.6394\n",
      "Epoch 267, CIFAR-10 Batch 1:  training cost 6.36315e-05  validation cost 3.17937\n",
      "training accuracy 1  validation accuracy 0.642\n",
      "Epoch 267, CIFAR-10 Batch 2:  training cost 0.000149414  validation cost 3.40588\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 267, CIFAR-10 Batch 3:  training cost 5.27316e-05  validation cost 3.23783\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 267, CIFAR-10 Batch 4:  training cost 0.000570252  validation cost 3.04631\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 267, CIFAR-10 Batch 5:  training cost 0.000125389  validation cost 3.1826\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 268, CIFAR-10 Batch 1:  training cost 0.000862763  validation cost 3.22221\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 268, CIFAR-10 Batch 2:  training cost 0.00217807  validation cost 3.36666\n",
      "training accuracy 1  validation accuracy 0.6274\n",
      "Epoch 268, CIFAR-10 Batch 3:  training cost 0.00383265  validation cost 3.40896\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 268, CIFAR-10 Batch 4:  training cost 0.0206107  validation cost 2.9509\n",
      "training accuracy 1  validation accuracy 0.6238\n",
      "Epoch 268, CIFAR-10 Batch 5:  training cost 5.93443e-05  validation cost 3.14563\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 269, CIFAR-10 Batch 1:  training cost 0.00232362  validation cost 3.21462\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 269, CIFAR-10 Batch 2:  training cost 0.00022516  validation cost 3.2065\n",
      "training accuracy 1  validation accuracy 0.639\n",
      "Epoch 269, CIFAR-10 Batch 3:  training cost 2.79387e-05  validation cost 2.97258\n",
      "training accuracy 1  validation accuracy 0.6398\n",
      "Epoch 269, CIFAR-10 Batch 4:  training cost 5.51724e-05  validation cost 3.06438\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 269, CIFAR-10 Batch 5:  training cost 0.000459894  validation cost 3.22424\n",
      "training accuracy 1  validation accuracy 0.6382\n",
      "Epoch 270, CIFAR-10 Batch 1:  training cost 0.000591885  validation cost 2.83332\n",
      "training accuracy 1  validation accuracy 0.6386\n",
      "Epoch 270, CIFAR-10 Batch 2:  training cost 0.00151344  validation cost 3.17867\n",
      "training accuracy 1  validation accuracy 0.635\n",
      "Epoch 270, CIFAR-10 Batch 3:  training cost 7.72108e-05  validation cost 3.16042\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 270, CIFAR-10 Batch 4:  training cost 6.18928e-05  validation cost 2.92732\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 270, CIFAR-10 Batch 5:  training cost 3.34664e-05  validation cost 3.03008\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 271, CIFAR-10 Batch 1:  training cost 0.000626049  validation cost 3.10957\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 271, CIFAR-10 Batch 2:  training cost 1.24272e-05  validation cost 3.44544\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 271, CIFAR-10 Batch 3:  training cost 4.51501e-06  validation cost 3.15432\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 271, CIFAR-10 Batch 4:  training cost 2.57175e-05  validation cost 3.06354\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 271, CIFAR-10 Batch 5:  training cost 0.00536544  validation cost 3.24963\n",
      "training accuracy 1  validation accuracy 0.6368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 272, CIFAR-10 Batch 1:  training cost 0.000291087  validation cost 3.2266\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 272, CIFAR-10 Batch 2:  training cost 0.0025368  validation cost 3.03446\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 272, CIFAR-10 Batch 3:  training cost 0.00016423  validation cost 3.04149\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 272, CIFAR-10 Batch 4:  training cost 2.3856e-05  validation cost 3.05126\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 272, CIFAR-10 Batch 5:  training cost 0.00354199  validation cost 3.247\n",
      "training accuracy 1  validation accuracy 0.6324\n",
      "Epoch 273, CIFAR-10 Batch 1:  training cost 0.00101964  validation cost 3.11707\n",
      "training accuracy 1  validation accuracy 0.6256\n",
      "Epoch 273, CIFAR-10 Batch 2:  training cost 5.63512e-05  validation cost 3.32685\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 273, CIFAR-10 Batch 3:  training cost 3.73545e-05  validation cost 3.07726\n",
      "training accuracy 1  validation accuracy 0.6346\n",
      "Epoch 273, CIFAR-10 Batch 4:  training cost 2.35277e-05  validation cost 2.78728\n",
      "training accuracy 1  validation accuracy 0.6352\n",
      "Epoch 273, CIFAR-10 Batch 5:  training cost 0.016272  validation cost 2.883\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 274, CIFAR-10 Batch 1:  training cost 5.9508e-05  validation cost 3.05529\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 274, CIFAR-10 Batch 2:  training cost 0.000509149  validation cost 3.26084\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 274, CIFAR-10 Batch 3:  training cost 0.000213426  validation cost 3.07141\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 274, CIFAR-10 Batch 4:  training cost 0.00123895  validation cost 3.16091\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 274, CIFAR-10 Batch 5:  training cost 0.0029615  validation cost 3.02786\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 275, CIFAR-10 Batch 1:  training cost 0.000763946  validation cost 3.1857\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 275, CIFAR-10 Batch 2:  training cost 0.00137712  validation cost 3.01177\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 275, CIFAR-10 Batch 3:  training cost 0.000203049  validation cost 3.04518\n",
      "training accuracy 1  validation accuracy 0.6416\n",
      "Epoch 275, CIFAR-10 Batch 4:  training cost 5.54505e-05  validation cost 3.14871\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 275, CIFAR-10 Batch 5:  training cost 0.000385072  validation cost 3.15003\n",
      "training accuracy 1  validation accuracy 0.626\n",
      "Epoch 276, CIFAR-10 Batch 1:  training cost 0.00346626  validation cost 2.77206\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 276, CIFAR-10 Batch 2:  training cost 0.000467515  validation cost 3.00223\n",
      "training accuracy 1  validation accuracy 0.6374\n",
      "Epoch 276, CIFAR-10 Batch 3:  training cost 0.000722446  validation cost 3.3554\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 276, CIFAR-10 Batch 4:  training cost 7.59538e-05  validation cost 3.07079\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 276, CIFAR-10 Batch 5:  training cost 0.00232583  validation cost 3.33822\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 277, CIFAR-10 Batch 1:  training cost 0.000711057  validation cost 3.11413\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 277, CIFAR-10 Batch 2:  training cost 0.000456994  validation cost 3.31534\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 277, CIFAR-10 Batch 3:  training cost 0.000161577  validation cost 3.20345\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 277, CIFAR-10 Batch 4:  training cost 1.10117e-05  validation cost 3.10251\n",
      "training accuracy 1  validation accuracy 0.6326\n",
      "Epoch 277, CIFAR-10 Batch 5:  training cost 0.000137043  validation cost 3.22557\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 278, CIFAR-10 Batch 1:  training cost 0.00498381  validation cost 2.90279\n",
      "training accuracy 1  validation accuracy 0.6288\n",
      "Epoch 278, CIFAR-10 Batch 2:  training cost 6.89789e-05  validation cost 3.19834\n",
      "training accuracy 1  validation accuracy 0.644\n",
      "Epoch 278, CIFAR-10 Batch 3:  training cost 3.16928e-05  validation cost 3.17127\n",
      "training accuracy 1  validation accuracy 0.6442\n",
      "Epoch 278, CIFAR-10 Batch 4:  training cost 2.61944e-05  validation cost 2.89679\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 278, CIFAR-10 Batch 5:  training cost 0.00011046  validation cost 3.13295\n",
      "training accuracy 1  validation accuracy 0.6382\n",
      "Epoch 279, CIFAR-10 Batch 1:  training cost 0.000161618  validation cost 3.26471\n",
      "training accuracy 1  validation accuracy 0.6268\n",
      "Epoch 279, CIFAR-10 Batch 2:  training cost 0.00108897  validation cost 3.26686\n",
      "training accuracy 1  validation accuracy 0.6294\n",
      "Epoch 279, CIFAR-10 Batch 3:  training cost 4.21538e-05  validation cost 3.11665\n",
      "training accuracy 1  validation accuracy 0.6476\n",
      "Epoch 279, CIFAR-10 Batch 4:  training cost 8.10611e-06  validation cost 3.30046\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 279, CIFAR-10 Batch 5:  training cost 0.000457962  validation cost 3.07956\n",
      "training accuracy 1  validation accuracy 0.6382\n",
      "Epoch 280, CIFAR-10 Batch 1:  training cost 0.00556311  validation cost 3.32667\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 280, CIFAR-10 Batch 2:  training cost 0.00156026  validation cost 3.18074\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 280, CIFAR-10 Batch 3:  training cost 0.000449712  validation cost 3.01185\n",
      "training accuracy 1  validation accuracy 0.6402\n",
      "Epoch 280, CIFAR-10 Batch 4:  training cost 0.00707692  validation cost 3.06684\n",
      "training accuracy 1  validation accuracy 0.6298\n",
      "Epoch 280, CIFAR-10 Batch 5:  training cost 0.000374921  validation cost 3.10886\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 281, CIFAR-10 Batch 1:  training cost 0.00329078  validation cost 3.26735\n",
      "training accuracy 1  validation accuracy 0.6286\n",
      "Epoch 281, CIFAR-10 Batch 2:  training cost 0.00043331  validation cost 3.32634\n",
      "training accuracy 1  validation accuracy 0.638\n",
      "Epoch 281, CIFAR-10 Batch 3:  training cost 0.00031343  validation cost 3.13293\n",
      "training accuracy 1  validation accuracy 0.6266\n",
      "Epoch 281, CIFAR-10 Batch 4:  training cost 9.47686e-06  validation cost 3.06266\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 281, CIFAR-10 Batch 5:  training cost 0.00018402  validation cost 3.36009\n",
      "training accuracy 1  validation accuracy 0.6244\n",
      "Epoch 282, CIFAR-10 Batch 1:  training cost 0.00460958  validation cost 3.16858\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 282, CIFAR-10 Batch 2:  training cost 0.00236139  validation cost 3.12164\n",
      "training accuracy 1  validation accuracy 0.6414\n",
      "Epoch 282, CIFAR-10 Batch 3:  training cost 0.000222779  validation cost 3.20676\n",
      "training accuracy 1  validation accuracy 0.6374\n",
      "Epoch 282, CIFAR-10 Batch 4:  training cost 0.0068594  validation cost 3.10094\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 282, CIFAR-10 Batch 5:  training cost 0.00595242  validation cost 3.18584\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 283, CIFAR-10 Batch 1:  training cost 0.000270469  validation cost 3.06999\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 283, CIFAR-10 Batch 2:  training cost 0.00635813  validation cost 2.17788\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 283, CIFAR-10 Batch 3:  training cost 0.000378805  validation cost 2.75278\n",
      "training accuracy 1  validation accuracy 0.6452\n",
      "Epoch 283, CIFAR-10 Batch 4:  training cost 3.30804e-06  validation cost 2.99087\n",
      "training accuracy 1  validation accuracy 0.637\n",
      "Epoch 283, CIFAR-10 Batch 5:  training cost 0.000903969  validation cost 3.09986\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 284, CIFAR-10 Batch 1:  training cost 8.8165e-05  validation cost 3.30679\n",
      "training accuracy 1  validation accuracy 0.6246\n",
      "Epoch 284, CIFAR-10 Batch 2:  training cost 8.28965e-05  validation cost 2.93527\n",
      "training accuracy 1  validation accuracy 0.641\n",
      "Epoch 284, CIFAR-10 Batch 3:  training cost 0.00285529  validation cost 2.7517\n",
      "training accuracy 1  validation accuracy 0.6402\n",
      "Epoch 284, CIFAR-10 Batch 4:  training cost 2.33947e-06  validation cost 2.92342\n",
      "training accuracy 1  validation accuracy 0.633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284, CIFAR-10 Batch 5:  training cost 0.00236864  validation cost 3.09403\n",
      "training accuracy 1  validation accuracy 0.6392\n",
      "Epoch 285, CIFAR-10 Batch 1:  training cost 0.000275183  validation cost 3.49494\n",
      "training accuracy 1  validation accuracy 0.622\n",
      "Epoch 285, CIFAR-10 Batch 2:  training cost 0.000106945  validation cost 3.24674\n",
      "training accuracy 1  validation accuracy 0.6396\n",
      "Epoch 285, CIFAR-10 Batch 3:  training cost 2.84605e-05  validation cost 3.087\n",
      "training accuracy 1  validation accuracy 0.6392\n",
      "Epoch 285, CIFAR-10 Batch 4:  training cost 0.000341085  validation cost 2.73726\n",
      "training accuracy 1  validation accuracy 0.6346\n",
      "Epoch 285, CIFAR-10 Batch 5:  training cost 0.000420792  validation cost 2.9681\n",
      "training accuracy 1  validation accuracy 0.6452\n",
      "Epoch 286, CIFAR-10 Batch 1:  training cost 3.03373e-05  validation cost 3.38279\n",
      "training accuracy 1  validation accuracy 0.6184\n",
      "Epoch 286, CIFAR-10 Batch 2:  training cost 0.000797679  validation cost 2.83242\n",
      "training accuracy 1  validation accuracy 0.64\n",
      "Epoch 286, CIFAR-10 Batch 3:  training cost 3.80409e-05  validation cost 3.14269\n",
      "training accuracy 1  validation accuracy 0.6424\n",
      "Epoch 286, CIFAR-10 Batch 4:  training cost 0.000246869  validation cost 3.06685\n",
      "training accuracy 1  validation accuracy 0.634\n",
      "Epoch 286, CIFAR-10 Batch 5:  training cost 0.00516299  validation cost 2.53772\n",
      "training accuracy 1  validation accuracy 0.6312\n",
      "Epoch 287, CIFAR-10 Batch 1:  training cost 0.00653552  validation cost 2.91633\n",
      "training accuracy 1  validation accuracy 0.6304\n",
      "Epoch 287, CIFAR-10 Batch 2:  training cost 0.000167259  validation cost 3.12392\n",
      "training accuracy 1  validation accuracy 0.6406\n",
      "Epoch 287, CIFAR-10 Batch 3:  training cost 0.00236985  validation cost 3.36797\n",
      "training accuracy 1  validation accuracy 0.639\n",
      "Epoch 287, CIFAR-10 Batch 4:  training cost 3.35685e-05  validation cost 3.02744\n",
      "training accuracy 1  validation accuracy 0.6296\n",
      "Epoch 287, CIFAR-10 Batch 5:  training cost 0.00285513  validation cost 3.28865\n",
      "training accuracy 1  validation accuracy 0.6354\n",
      "Epoch 288, CIFAR-10 Batch 1:  training cost 0.000324396  validation cost 3.24412\n",
      "training accuracy 1  validation accuracy 0.6302\n",
      "Epoch 288, CIFAR-10 Batch 2:  training cost 3.07379e-05  validation cost 3.16362\n",
      "training accuracy 1  validation accuracy 0.629\n",
      "Epoch 288, CIFAR-10 Batch 3:  training cost 0.000948113  validation cost 2.64528\n",
      "training accuracy 1  validation accuracy 0.6418\n",
      "Epoch 288, CIFAR-10 Batch 4:  training cost 6.67696e-05  validation cost 2.96653\n",
      "training accuracy 1  validation accuracy 0.6466\n",
      "Epoch 288, CIFAR-10 Batch 5:  training cost 0.00356259  validation cost 3.27152\n",
      "training accuracy 1  validation accuracy 0.6396\n",
      "Epoch 289, CIFAR-10 Batch 1:  training cost 0.00512794  validation cost 3.22656\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 289, CIFAR-10 Batch 2:  training cost 0.00121973  validation cost 3.12241\n",
      "training accuracy 1  validation accuracy 0.6328\n",
      "Epoch 289, CIFAR-10 Batch 3:  training cost 0.00829414  validation cost 3.21819\n",
      "training accuracy 1  validation accuracy 0.6198\n",
      "Epoch 289, CIFAR-10 Batch 4:  training cost 3.3598e-05  validation cost 3.00381\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 289, CIFAR-10 Batch 5:  training cost 0.0013992  validation cost 2.99123\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 290, CIFAR-10 Batch 1:  training cost 0.00204411  validation cost 3.30714\n",
      "training accuracy 1  validation accuracy 0.6316\n",
      "Epoch 290, CIFAR-10 Batch 2:  training cost 0.000675828  validation cost 3.15541\n",
      "training accuracy 1  validation accuracy 0.6334\n",
      "Epoch 290, CIFAR-10 Batch 3:  training cost 0.000126415  validation cost 3.104\n",
      "training accuracy 1  validation accuracy 0.64\n",
      "Epoch 290, CIFAR-10 Batch 4:  training cost 0.000112684  validation cost 2.84511\n",
      "training accuracy 1  validation accuracy 0.6376\n",
      "Epoch 290, CIFAR-10 Batch 5:  training cost 0.00293404  validation cost 3.19791\n",
      "training accuracy 1  validation accuracy 0.636\n",
      "Epoch 291, CIFAR-10 Batch 1:  training cost 0.00204403  validation cost 3.22404\n",
      "training accuracy 1  validation accuracy 0.633\n",
      "Epoch 291, CIFAR-10 Batch 2:  training cost 0.000858238  validation cost 3.23805\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 291, CIFAR-10 Batch 3:  training cost 8.54207e-05  validation cost 3.08184\n",
      "training accuracy 1  validation accuracy 0.6426\n",
      "Epoch 291, CIFAR-10 Batch 4:  training cost 0.000461499  validation cost 3.04886\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 291, CIFAR-10 Batch 5:  training cost 0.000245909  validation cost 3.09036\n",
      "training accuracy 1  validation accuracy 0.6344\n",
      "Epoch 292, CIFAR-10 Batch 1:  training cost 0.00178306  validation cost 3.11653\n",
      "training accuracy 1  validation accuracy 0.6254\n",
      "Epoch 292, CIFAR-10 Batch 2:  training cost 0.00157019  validation cost 3.40577\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 292, CIFAR-10 Batch 3:  training cost 7.56314e-05  validation cost 3.22089\n",
      "training accuracy 1  validation accuracy 0.6366\n",
      "Epoch 292, CIFAR-10 Batch 4:  training cost 0.000185262  validation cost 3.12422\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 292, CIFAR-10 Batch 5:  training cost 0.000384592  validation cost 3.08508\n",
      "training accuracy 1  validation accuracy 0.6384\n",
      "Epoch 293, CIFAR-10 Batch 1:  training cost 0.000721303  validation cost 3.05647\n",
      "training accuracy 1  validation accuracy 0.6226\n",
      "Epoch 293, CIFAR-10 Batch 2:  training cost 1.67778e-05  validation cost 3.33113\n",
      "training accuracy 1  validation accuracy 0.6358\n",
      "Epoch 293, CIFAR-10 Batch 3:  training cost 0.00157799  validation cost 3.06417\n",
      "training accuracy 1  validation accuracy 0.643\n",
      "Epoch 293, CIFAR-10 Batch 4:  training cost 0.000301272  validation cost 2.8118\n",
      "training accuracy 1  validation accuracy 0.6176\n",
      "Epoch 293, CIFAR-10 Batch 5:  training cost 0.00333598  validation cost 2.97529\n",
      "training accuracy 1  validation accuracy 0.6336\n",
      "Epoch 294, CIFAR-10 Batch 1:  training cost 0.000709499  validation cost 3.13446\n",
      "training accuracy 1  validation accuracy 0.6392\n",
      "Epoch 294, CIFAR-10 Batch 2:  training cost 2.95456e-05  validation cost 3.20162\n",
      "training accuracy 1  validation accuracy 0.6366\n",
      "Epoch 294, CIFAR-10 Batch 3:  training cost 0.00511958  validation cost 2.9119\n",
      "training accuracy 1  validation accuracy 0.649\n",
      "Epoch 294, CIFAR-10 Batch 4:  training cost 0.000264347  validation cost 2.96281\n",
      "training accuracy 1  validation accuracy 0.6306\n",
      "Epoch 294, CIFAR-10 Batch 5:  training cost 0.000552439  validation cost 3.27209\n",
      "training accuracy 1  validation accuracy 0.6398\n",
      "Epoch 295, CIFAR-10 Batch 1:  training cost 0.000203604  validation cost 3.31275\n",
      "training accuracy 1  validation accuracy 0.6318\n",
      "Epoch 295, CIFAR-10 Batch 2:  training cost 0.000446872  validation cost 3.09906\n",
      "training accuracy 1  validation accuracy 0.639\n",
      "Epoch 295, CIFAR-10 Batch 3:  training cost 2.22026e-06  validation cost 3.27666\n",
      "training accuracy 1  validation accuracy 0.6458\n",
      "Epoch 295, CIFAR-10 Batch 4:  training cost 0.00269197  validation cost 3.23464\n",
      "training accuracy 1  validation accuracy 0.6292\n",
      "Epoch 295, CIFAR-10 Batch 5:  training cost 4.03647e-05  validation cost 3.31958\n",
      "training accuracy 1  validation accuracy 0.6396\n",
      "Epoch 296, CIFAR-10 Batch 1:  training cost 7.82076e-05  validation cost 3.24599\n",
      "training accuracy 1  validation accuracy 0.6372\n",
      "Epoch 296, CIFAR-10 Batch 2:  training cost 0.000102795  validation cost 2.80889\n",
      "training accuracy 1  validation accuracy 0.6362\n",
      "Epoch 296, CIFAR-10 Batch 3:  training cost 7.61394e-05  validation cost 3.07546\n",
      "training accuracy 1  validation accuracy 0.6456\n",
      "Epoch 296, CIFAR-10 Batch 4:  training cost 0.000162637  validation cost 3.21805\n",
      "training accuracy 1  validation accuracy 0.6348\n",
      "Epoch 296, CIFAR-10 Batch 5:  training cost 0.000867878  validation cost 3.33547\n",
      "training accuracy 1  validation accuracy 0.6322\n",
      "Epoch 297, CIFAR-10 Batch 1:  training cost 0.00138019  validation cost 3.36052\n",
      "training accuracy 1  validation accuracy 0.624\n",
      "Epoch 297, CIFAR-10 Batch 2:  training cost 4.22084e-05  validation cost 3.1977\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 297, CIFAR-10 Batch 3:  training cost 2.88913e-05  validation cost 3.23779\n",
      "training accuracy 1  validation accuracy 0.6458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297, CIFAR-10 Batch 4:  training cost 0.00163259  validation cost 2.91592\n",
      "training accuracy 1  validation accuracy 0.6368\n",
      "Epoch 297, CIFAR-10 Batch 5:  training cost 0.00138556  validation cost 2.84835\n",
      "training accuracy 1  validation accuracy 0.6356\n",
      "Epoch 298, CIFAR-10 Batch 1:  training cost 0.000451524  validation cost 3.27\n",
      "training accuracy 1  validation accuracy 0.6314\n",
      "Epoch 298, CIFAR-10 Batch 2:  training cost 3.23761e-05  validation cost 3.53861\n",
      "training accuracy 1  validation accuracy 0.627\n",
      "Epoch 298, CIFAR-10 Batch 3:  training cost 0.000133885  validation cost 3.51127\n",
      "training accuracy 1  validation accuracy 0.632\n",
      "Epoch 298, CIFAR-10 Batch 4:  training cost 0.000107825  validation cost 3.19733\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 298, CIFAR-10 Batch 5:  training cost 0.0010219  validation cost 3.26597\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 299, CIFAR-10 Batch 1:  training cost 0.0028843  validation cost 3.27859\n",
      "training accuracy 1  validation accuracy 0.6278\n",
      "Epoch 299, CIFAR-10 Batch 2:  training cost 5.31414e-05  validation cost 3.17079\n",
      "training accuracy 1  validation accuracy 0.64\n",
      "Epoch 299, CIFAR-10 Batch 3:  training cost 1.40217e-05  validation cost 3.25715\n",
      "training accuracy 1  validation accuracy 0.6454\n",
      "Epoch 299, CIFAR-10 Batch 4:  training cost 0.00141121  validation cost 3.04336\n",
      "training accuracy 1  validation accuracy 0.6404\n",
      "Epoch 299, CIFAR-10 Batch 5:  training cost 0.00638687  validation cost 3.30817\n",
      "training accuracy 1  validation accuracy 0.6342\n",
      "Epoch 300, CIFAR-10 Batch 1:  training cost 0.000381453  validation cost 3.31795\n",
      "training accuracy 1  validation accuracy 0.63\n",
      "Epoch 300, CIFAR-10 Batch 2:  training cost 0.000649058  validation cost 3.15584\n",
      "training accuracy 1  validation accuracy 0.631\n",
      "Epoch 300, CIFAR-10 Batch 3:  training cost 0.000113664  validation cost 2.94901\n",
      "training accuracy 1  validation accuracy 0.642\n",
      "Epoch 300, CIFAR-10 Batch 4:  training cost 5.38715e-05  validation cost 3.26579\n",
      "training accuracy 1  validation accuracy 0.6346\n",
      "Epoch 300, CIFAR-10 Batch 5:  training cost 0.00023086  validation cost 3.23586\n",
      "training accuracy 1  validation accuracy 0.6332\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6335862619808307\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f266ec417b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
